{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/13883\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2020-04-16 16:07:15.738187\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import cProfile\n",
    "import hashlib\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pstats\n",
    "import re\n",
    "import shutil\n",
    "import signal\n",
    "import statistics\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torchaudio\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from torch import nn, topk\n",
    "from torch.optim import SGD, Adadelta, Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH, SPEECHCOMMANDS\n",
    "from torchaudio.transforms import MFCC, Resample\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "print(\"start time: {}\".format(str(datetime.now())), flush=True)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    in_notebook = True\n",
    "except NameError:\n",
    "    matplotlib.use(\"Agg\")\n",
    "    in_notebook = False\n",
    "    \n",
    "# Empty CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Profiling performance\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='wav2letter', batch_size=64, dataset='librispeech', dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', epochs=200, eps=1e-08, gradient=False, jit=False, learning_rate=1.0, print_freq=10, resume='', rho=0.95, start_epoch=0, weight_decay=1e-05, workers=0, world_size=1)\n"
     ]
    }
   ],
   "source": [
    "# Create argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--workers', default=0, type=int,\n",
    "                    metavar='N', help='number of data loading workers')\n",
    "parser.add_argument('--resume', default='', type=str,\n",
    "                    metavar='PATH', help='path to latest checkpoint')\n",
    "\n",
    "parser.add_argument('--epochs', default=200, type=int,\n",
    "                    metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int,\n",
    "                    metavar='N', help='manual epoch number')\n",
    "parser.add_argument('--print-freq', default=10, type=int,\n",
    "                    metavar='N', help='print frequency in epochs')\n",
    "\n",
    "parser.add_argument('--arch', metavar='ARCH', default='wav2letter',\n",
    "                    choices=[\"wav2letter\", \"lstm\"], help='model architecture')\n",
    "parser.add_argument('--batch-size', default=64, type=int,\n",
    "                    metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--learning-rate', default=1., type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "# parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "parser.add_argument('--weight-decay', default=1e-5,\n",
    "                    type=float, metavar='W', help='weight decay')\n",
    "parser.add_argument(\"--eps\", metavar='EPS', type=float, default=1e-8)\n",
    "parser.add_argument(\"--rho\", metavar='RHO', type=float, default=.95)\n",
    "\n",
    "parser.add_argument('--world-size', default=1, type=int,\n",
    "                    help='number of distributed processes')\n",
    "parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456',\n",
    "                    type=str, help='url used to set up distributed training')\n",
    "parser.add_argument('--dist-backend', default='nccl',\n",
    "                    type=str, help='distributed backend')\n",
    "parser.add_argument('--dataset', default='librispeech', type=str)\n",
    "parser.add_argument('--gradient', action=\"store_true\")\n",
    "parser.add_argument('--jit', action=\"store_true\")\n",
    "\n",
    "if in_notebook:\n",
    "    args, _ = parser.parse_known_args()\n",
    "else:\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.arch = \"lstm\"\n",
    "args.learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal handler installed\n"
     ]
    }
   ],
   "source": [
    "MAIN_PID = os.getpid()\n",
    "CHECKPOINT_filename = args.resume if args.resume else 'checkpoint.pth.tar'\n",
    "CHECKPOINT_tempfile = CHECKPOINT_filename + '.temp'\n",
    "HALT_filename = CHECKPOINT_filename + '.HALT'\n",
    "SIGNAL_RECEIVED = False\n",
    "\n",
    "# HALT file is used as a sign of job completion.\n",
    "# Make sure no HALT file left from previous runs.\n",
    "if os.path.isfile(HALT_filename):\n",
    "    os.remove(HALT_filename)\n",
    "\n",
    "# Remove CHECKPOINT_tempfile, in case the signal arrives in the\n",
    "# middle of copying from CHECKPOINT_tempfile to CHECKPOINT_filename\n",
    "if os.path.isfile(CHECKPOINT_tempfile):\n",
    "    os.remove(CHECKPOINT_tempfile)\n",
    "\n",
    "\n",
    "def SIGTERM_handler(a, b):\n",
    "    print('received sigterm')\n",
    "    pass\n",
    "\n",
    "\n",
    "def signal_handler(a, b):\n",
    "    global SIGNAL_RECEIVED\n",
    "    print('Signal received', a, datetime.now().strftime(\n",
    "        \"%y%m%d.%H%M%S\"), flush=True)\n",
    "    SIGNAL_RECEIVED = True\n",
    "\n",
    "    # If HALT file exists, which means the job is done, exit peacefully.\n",
    "    if os.path.isfile(HALT_filename):\n",
    "        print('Job is done, exiting')\n",
    "        exit(0)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def trigger_job_requeue():\n",
    "    # Submit a new job to resume from checkpoint.\n",
    "    if os.path.isfile(CHECKPOINT_filename) and \\\n",
    "       os.environ['SLURM_PROCID'] == '0' and \\\n",
    "       os.getpid() == MAIN_PID:\n",
    "        print('pid: ', os.getpid(), ' ppid: ', os.getppid(), flush=True)\n",
    "        print('time is up, back to slurm queue', flush=True)\n",
    "        command = 'scontrol requeue ' + os.environ['SLURM_JOB_ID']\n",
    "        print(command)\n",
    "        if os.system(command):\n",
    "            raise RuntimeError('requeue failed')\n",
    "        print('New job submitted to the queue', flush=True)\n",
    "    exit(0)\n",
    "\n",
    "\n",
    "# Install signal handler\n",
    "signal.signal(signal.SIGUSR1, signal_handler)\n",
    "signal.signal(signal.SIGTERM, SIGTERM_handler)\n",
    "print('Signal handler installed', flush=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=CHECKPOINT_filename):\n",
    "    \"\"\"\n",
    "    Save the model to a temporary file first,\n",
    "    then copy it to filename, in case the signal interrupts\n",
    "    the torch.save() process.\n",
    "    \"\"\"\n",
    "    if not args.distributed or os.environ['SLURM_PROCID'] == '0':\n",
    "        torch.save(state, CHECKPOINT_tempfile)\n",
    "        if os.path.isfile(CHECKPOINT_tempfile):\n",
    "            os.rename(CHECKPOINT_tempfile, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        print(\"Checkpoint: saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use #nodes as world_size\n",
    "if 'SLURM_NNODES' in os.environ:\n",
    "    args.world_size = int(os.environ['SLURM_NNODES'])\n",
    "\n",
    "args.distributed = args.world_size > 1\n",
    "\n",
    "if args.distributed:\n",
    "    os.environ['RANK'] = os.environ['SLURM_PROCID']\n",
    "    os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "    print('in distributed', os.environ['RANK'],\n",
    "          os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'], flush=True)\n",
    "    dist.init_process_group(backend=args.dist_backend,\n",
    "                            init_method=args.dist_url, world_size=args.world_size)\n",
    "\n",
    "    print('init process', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.distributed or os.environ['SLURM_PROCID'] == '0':\n",
    "    print(args, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GPUs\n"
     ]
    }
   ],
   "source": [
    "audio_backend = \"soundfile\"\n",
    "torchaudio.set_audio_backend(audio_backend)\n",
    "\n",
    "root = \"/datasets01/\"\n",
    "folder_in_archive = \"librispeech/062419/\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_devices = torch.cuda.device_count()\n",
    "# num_devices = 1\n",
    "print(num_devices, \"GPUs\", flush=True)\n",
    "\n",
    "# max number of sentences per batch\n",
    "batch_size = args.batch_size\n",
    "# batch_size = 2048\n",
    "# batch_size = 512\n",
    "# batch_size = 256\n",
    "# batch_size = 64\n",
    "# batch_size = 1\n",
    "\n",
    "training_percentage = 90.\n",
    "validation_percentage = 5.\n",
    "\n",
    "data_loader_training_params = {\n",
    "    \"num_workers\": args.workers,\n",
    "    \"pin_memory\": True,\n",
    "    \"shuffle\": True,\n",
    "    \"drop_last\": True,\n",
    "}\n",
    "data_loader_validation_params = data_loader_training_params.copy()\n",
    "data_loader_validation_params[\"shuffle\"] = False\n",
    "\n",
    "non_blocking = True\n",
    "\n",
    "\n",
    "# text preprocessing\n",
    "\n",
    "char_null = \"-\"\n",
    "char_space = \" \"\n",
    "char_pad = \"*\"\n",
    "char_apostrophe = \"'\"\n",
    "\n",
    "labels = [char_null + char_pad + char_apostrophe + string.ascii_lowercase]\n",
    "\n",
    "# excluded_dir = [\"_background_noise_\"]\n",
    "# folder_speechcommands = './SpeechCommands/speech_commands_v0.02'\n",
    "# labels = [char_null, char_pad] + [d for d in next(os.walk(folder_speechcommands))[1] if d not in excluded_dir]\n",
    "\n",
    "\n",
    "# audio\n",
    "\n",
    "sample_rate_original = 16000\n",
    "sample_rate_new = 8000\n",
    "# resample = Resample(sample_rate_original, sample_rate_new).to(device)\n",
    "resample = None\n",
    "\n",
    "n_mfcc = 13\n",
    "melkwargs = {\n",
    "    'n_fft': 512,\n",
    "    'n_mels': 20,\n",
    "    'hop_length': 80,  # (160, 80)\n",
    "}\n",
    "mfcc = MFCC(sample_rate=sample_rate_original,\n",
    "            n_mfcc=n_mfcc, melkwargs=melkwargs).to(device)\n",
    "# mfcc = None\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "optimizer_params_adadelta = {\n",
    "    \"lr\": args.learning_rate,\n",
    "    \"eps\": args.eps,\n",
    "    \"rho\": args.rho,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "}\n",
    "\n",
    "optimizer_params_adam = {\n",
    "    \"lr\": args.learning_rate,\n",
    "    \"eps\": args.eps,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "}\n",
    "\n",
    "optimizer_params_sgd = {\n",
    "    \"lr\": args.learning_rate,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "}\n",
    "\n",
    "optimizer_params_adadelta = {\n",
    "    \"lr\": args.learning_rate,\n",
    "    \"eps\": args.eps,\n",
    "    \"rho\": args.rho,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "}\n",
    "\n",
    "Optimizer = Adadelta\n",
    "optimizer_params = optimizer_params_sgd\n",
    "\n",
    "gamma = 0.96\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "num_features = n_mfcc if n_mfcc else 1\n",
    "\n",
    "lstm_params = {\n",
    "    \"hidden_size\": 600,\n",
    "    \"num_layers\": 2,\n",
    "    \"batch_first\": False,\n",
    "    \"bidirectional\": False,\n",
    "    \"dropout\": 0.,\n",
    "}\n",
    "\n",
    "clip_norm = 0.  # 10.\n",
    "\n",
    "zero_infinity = False\n",
    "\n",
    "start_epoch = args.start_epoch\n",
    "max_epoch = args.epochs\n",
    "mod_epoch = args.print_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 29\n"
     ]
    }
   ],
   "source": [
    "class Coder:\n",
    "    def __init__(self, labels):\n",
    "        labels = list(collections.OrderedDict.fromkeys(list(\"\".join(labels))))\n",
    "        self.length = len(labels)\n",
    "        enumerated = list(enumerate(labels))\n",
    "        flipped = [(sub[1], sub[0]) for sub in enumerated]\n",
    "\n",
    "        d1 = collections.OrderedDict(enumerated)\n",
    "        d2 = collections.OrderedDict(flipped)\n",
    "        self.mapping = {**d1, **d2}\n",
    "        self.mapping[char_space] = self.mapping[char_pad]\n",
    "\n",
    "    def _map(self, iterable):\n",
    "        # iterable to iterable\n",
    "        return [self.mapping[i] for i in iterable]\n",
    "\n",
    "    def encode(self, iterable):\n",
    "        if isinstance(iterable[0], list):\n",
    "            return [self.encode(i) for i in iterable]\n",
    "        else:\n",
    "            return self._map(iterable)\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        if isinstance(tensor[0], list):\n",
    "            return [self.decode(t) for t in tensor]\n",
    "        else:\n",
    "            # not idempotent, since clean string\n",
    "            return \"\".join(self._map(tensor)).replace(char_null, \"\").replace(char_pad, char_space).strip()\n",
    "\n",
    "\n",
    "coder = Coder(labels)\n",
    "encode = coder.encode\n",
    "decode = coder.decode\n",
    "vocab_size = coder.length\n",
    "print(\"vocab_size\", vocab_size, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMemoryCache:\n",
    "\n",
    "    def __init__(self, iterable):\n",
    "        self.iterable = iterable\n",
    "        self._iter = iter(iterable)\n",
    "        self._done = False\n",
    "        self._values = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self._done:\n",
    "            return iter(self._values)\n",
    "        return itertools.chain(self._values, self._gen_iter())\n",
    "\n",
    "    def _gen_iter(self):\n",
    "        for new_value in self._iter:\n",
    "            self._values.append(new_value)\n",
    "            yield new_value\n",
    "        self._done = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._iterable)\n",
    "\n",
    "\n",
    "class MapMemoryCache(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a dataset so that, whenever a new item is returned, it is saved to memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self._cache = [None] * len(dataset)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        if self._cache[n]:\n",
    "            return self._cache[n]\n",
    "\n",
    "        item = self.dataset[n]\n",
    "        self._cache[n] = item\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "class Processed(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, process_datapoint, dataset):\n",
    "        self.process_datapoint = process_datapoint\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        try:\n",
    "            item = self.dataset[n]\n",
    "            return self.process_datapoint(item)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return None\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            item = next(self.dataset)\n",
    "            return self.process_datapoint(item)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return self.__next__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "def process_datapoint(item):\n",
    "    transformed = item[0].to(device, non_blocking=non_blocking)\n",
    "    target = item[2].lower().replace(char_space, char_pad)\n",
    "\n",
    "    # apply mfcc, tranpose for pad_sequence\n",
    "    if resample is not None:\n",
    "        transformed = resample(transformed)\n",
    "\n",
    "    if mfcc is not None:\n",
    "        transformed = mfcc(transformed)\n",
    "    else:\n",
    "        transformed = transformed.unsqueeze(1)\n",
    "\n",
    "    transformed = transformed[0, ...].transpose(0, -1)\n",
    "\n",
    "    target = \" \" + target + \" \"\n",
    "    target = encode(target)\n",
    "    target = torch.tensor(target, dtype=torch.long, device=transformed.device)\n",
    "\n",
    "    transformed = transformed.to(\"cpu\")\n",
    "    target = target.to(\"cpu\")\n",
    "    return transformed, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gives_error(d, i):\n",
    "    try:\n",
    "        d[i]\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "\n",
    "if False:\n",
    "    a = LIBRISPEECH(root, \"dev-clean\",\n",
    "                    folder_in_archive=folder_in_archive, download=False)\n",
    "    la = [i for i in range(len(a)) if gives_error(a, i)]\n",
    "    print(la)\n",
    "\n",
    "    b = LIBRISPEECH(root, \"train-clean-100\",\n",
    "                    folder_in_archive=folder_in_archive, download=False)\n",
    "    lb = [i for i in range(len(b)) if gives_error(b, i)]\n",
    "    print(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_librispeech():\n",
    "\n",
    "    def create(tag):\n",
    "        data = LIBRISPEECH(\n",
    "            root, tag, folder_in_archive=folder_in_archive, download=False)\n",
    "        data = Processed(process_datapoint, data)\n",
    "        data = MapMemoryCache(data)\n",
    "        return data\n",
    "\n",
    "    return create(\"train-clean-100\"), create(\"dev-clean\"), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "\n",
    "    We want to keep files in the same training, validation, or testing sets even\n",
    "    if new ones are added over time. This makes it less likely that testing\n",
    "    samples will accidentally be reused in training when long runs are restarted\n",
    "    for example. To keep this stability, a hash of the filename is taken and used\n",
    "    to determine which set it should belong to. This determination only depends on\n",
    "    the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "    It's also useful to associate particular files as related (for example words\n",
    "    spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "    Args:\n",
    "        filename: File path of the data sample.\n",
    "        validation_percentage: How much of the data set to use for validation.\n",
    "        testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "\n",
    "    MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "    base_name = os.path.basename(filename)\n",
    "\n",
    "    # We want to ignore anything after '_nohash_' in the file name when\n",
    "    # deciding which set to put a wav in, so the data set creator has a way of\n",
    "    # grouping wavs that are close variations of each other.\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode(\"utf-8\")\n",
    "\n",
    "    # This looks a bit magical, but we need to decide whether this file should\n",
    "    # go into the training, testing, or validation sets, and we want to keep\n",
    "    # existing files in the same set even if more files are subsequently\n",
    "    # added.\n",
    "    # To do that, we need a stable way of deciding based on just the file name\n",
    "    # itself, so we do a hash of that and then use that to generate a\n",
    "    # probability value that we use to assign it.\n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) % (\n",
    "        MAX_NUM_WAVS_PER_CLASS + 1)) * (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_speechcommands(tag, training_percentage, data):\n",
    "    if training_percentage < 100.:\n",
    "        testing_percentage = (\n",
    "            100. - training_percentage - validation_percentage)\n",
    "\n",
    "        def which_set_filter(x): return which_set(\n",
    "            x, validation_percentage, testing_percentage) == tag\n",
    "        data._walker = list(filter(which_set_filter, data._walker))\n",
    "    return data\n",
    "\n",
    "\n",
    "def datasets_speechcommands():\n",
    "\n",
    "    root = \"./\"\n",
    "\n",
    "    def create(tag):\n",
    "        data = SPEECHCOMMANDS(root, download=True)\n",
    "        data = filter_speechcommands(tag, training_percentage, data)\n",
    "        data = Processed(process_datapoint, data)\n",
    "        data = MapMemoryCache(data)\n",
    "        return data\n",
    "\n",
    "    return create(\"training\"), create(\"validation\"), create(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"librispeech\":\n",
    "    training, validation, _ = datasets_librispeech()\n",
    "elif args.dataset == \"speechcommand\":\n",
    "    training, validation, _ = datasets_speechcommands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    training_unprocessed = SPEECHCOMMANDS(\"./\", download=True)\n",
    "    training_unprocessed = filter_speechcommands(\n",
    "        training_percentage, training_unprocessed)\n",
    "\n",
    "    counter = Counter([t[2] for t in training_unprocessed])\n",
    "    counter = OrderedDict(counter.most_common())\n",
    "\n",
    "    plt.bar(counter.keys(), counter.values(), align='center')\n",
    "\n",
    "    if resample is not None:\n",
    "        waveform, sample_rate = training_unprocessed[0][0], training_unprocessed[0][1]\n",
    "\n",
    "        fn = \"sound.wav\"\n",
    "        torchaudio.save(fn, waveform, sample_rate_new)\n",
    "        ipd.Audio(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "[Wav2Letter](https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        size = m.weight.size()\n",
    "        fan_out = size[0]  # number of rows\n",
    "        fan_in = size[1]  # number of columns\n",
    "        variance = math.sqrt(2.0/(fan_in + fan_out))\n",
    "        m.weight.data.normal_(0.0, variance)\n",
    "\n",
    "\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x, flush=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Letter(nn.Module):\n",
    "    \"\"\"Wav2Letter Speech Recognition model\n",
    "        https://arxiv.org/pdf/1609.03193.pdf\n",
    "        This specific architecture accepts mfcc or power spectrums speech signals\n",
    "\n",
    "        Args:\n",
    "            num_features (int): number of mfcc features\n",
    "            num_classes (int): number of unique grapheme class labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_features, out_channels=250,\n",
    "                      kernel_size=48, stride=2, padding=23),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=2000,\n",
    "                      kernel_size=32, stride=1, padding=16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=2000, out_channels=2000,\n",
    "                      kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=2000, out_channels=num_classes,\n",
    "                      kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass through Wav2Letter network than\n",
    "            takes log probability of output\n",
    "        Args:\n",
    "            batch (int): mini batch of data\n",
    "            shape (batch, num_features, frame_len)\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, num_classes, output_len)\n",
    "        \"\"\"\n",
    "        # batch: (batch_size, num_features, seq_len)\n",
    "        y_pred = self.layers(batch)\n",
    "        # y_pred: (batch_size, num_classes, output_len)\n",
    "        y_pred = y_pred.transpose(-1, -2)\n",
    "        # y_pred: (batch_size, output_len, num_classes)\n",
    "        return nn.functional.log_softmax(y_pred, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes, hidden_size, num_layers, bidirectional, dropout, batch_first):\n",
    "        super().__init__()\n",
    "\n",
    "        directions = bidirectional + 1\n",
    "\n",
    "        self.layer = nn.LSTM(\n",
    "            num_features, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=batch_first\n",
    "        )\n",
    "        # self.activation = nn.ReLU(inplace=True)\n",
    "        self.hidden2class = nn.Linear(directions*hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        self.layer.flatten_parameters()\n",
    "        # print(\"forward\", flush=True)\n",
    "        # batch: batch, num_features, seq_len\n",
    "        # print(batch.shape, flush=True)\n",
    "        batch = batch.transpose(-1, -2).contiguous()\n",
    "        # batch: batch, seq_len, num_features\n",
    "        # print(batch.shape, flush=True)\n",
    "        outputs, _ = self.layer(batch)\n",
    "        # outputs = self.activation(outputs)\n",
    "        # outputs: batch, seq_len, directions*num_features\n",
    "        outputs = self.hidden2class(outputs)\n",
    "        # outputs: batch, seq_len, num_features\n",
    "        # print(outputs.shape, flush=True)\n",
    "        return nn.functional.log_softmax(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(outputs):\n",
    "    \"\"\"Greedy Decoder. Returns highest probability of class labels for each timestep\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): shape (input length, batch size, number of classes (including blank))\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: class labels per time step.\n",
    "    \"\"\"\n",
    "    _, indices = topk(outputs, k=1, dim=-1)\n",
    "    return indices[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_transitions():\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    c = None\n",
    "\n",
    "    for _, label in training:\n",
    "        # Count bigrams\n",
    "        count = [((a.item(), b.item())) for (a, b) in zip(label, label[1:])]\n",
    "        count = Counter(count)\n",
    "        if c is None:\n",
    "            c = count\n",
    "        else:\n",
    "            c = c + count\n",
    "\n",
    "    # Encode as transition matrix\n",
    "\n",
    "    ind = torch.tensor(list(zip(*[a for (a, b) in c.items()])))\n",
    "    val = torch.tensor([b for (a, b) in c.items()], dtype=torch.float)\n",
    "\n",
    "    transitions = torch.sparse_coo_tensor(indices=ind, values=val, size=[\n",
    "                                          vocab_size, vocab_size]).coalesce().to_dense()\n",
    "    transitions = (transitions/torch.max(torch.tensor(1.),\n",
    "                                         transitions.max(dim=1)[0]).unsqueeze(1))\n",
    "\n",
    "    return transitions\n",
    "\n",
    "\n",
    "# transitions = build_transitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/PetrochukM/afaa3613a99a8e7213d2efdd02ae4762\n",
    "# https://github.com/napsternxg/pytorch-practice/blob/master/Viterbi%20decoding%20and%20CRF.ipynb\n",
    "\n",
    "\n",
    "def viterbi_decode(tag_sequence: torch.Tensor, transition_matrix: torch.Tensor, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform Viterbi decoding in log space over a sequence given a transition matrix\n",
    "    specifying pairwise (transition) potentials between tags and a matrix of shape\n",
    "    (sequence_length, num_tags) specifying unary potentials for possible tags per\n",
    "    timestep.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag_sequence : torch.Tensor, required.\n",
    "        A tensor of shape (sequence_length, num_tags) representing scores for\n",
    "        a set of tags over a given sequence.\n",
    "    transition_matrix : torch.Tensor, required.\n",
    "        A tensor of shape (num_tags, num_tags) representing the binary potentials\n",
    "        for transitioning between a given pair of tags.\n",
    "    top_k : int, required.\n",
    "        Integer defining the top number of paths to decode.\n",
    "    Returns\n",
    "    -------\n",
    "    viterbi_path : List[int]\n",
    "        The tag indices of the maximum likelihood tag sequence.\n",
    "    viterbi_score : float\n",
    "        The score of the viterbi path.\n",
    "    \"\"\"\n",
    "    sequence_length, num_tags = tag_sequence.size()\n",
    "\n",
    "    path_scores = []\n",
    "    path_indices = []\n",
    "    # At the beginning, the maximum number of permutations is 1; therefore, we unsqueeze(0)\n",
    "    # to allow for 1 permutation.\n",
    "    path_scores.append(tag_sequence[0, :].unsqueeze(0))\n",
    "    # assert path_scores[0].size() == (n_permutations, num_tags)\n",
    "\n",
    "    # Evaluate the scores for all possible paths.\n",
    "    for timestep in range(1, sequence_length):\n",
    "        # Add pairwise potentials to current scores.\n",
    "        # assert path_scores[timestep - 1].size() == (n_permutations, num_tags)\n",
    "        summed_potentials = path_scores[timestep -\n",
    "                                        1].unsqueeze(2) + transition_matrix\n",
    "        summed_potentials = summed_potentials.view(-1, num_tags)\n",
    "\n",
    "        # Best pairwise potential path score from the previous timestep.\n",
    "        max_k = min(summed_potentials.size()[0], top_k)\n",
    "        scores, paths = torch.topk(summed_potentials, k=max_k, dim=0)\n",
    "        # assert scores.size() == (n_permutations, num_tags)\n",
    "        # assert paths.size() == (n_permutations, num_tags)\n",
    "\n",
    "        scores = tag_sequence[timestep, :] + scores\n",
    "        # assert scores.size() == (n_permutations, num_tags)\n",
    "        path_scores.append(scores)\n",
    "        path_indices.append(paths.squeeze())\n",
    "\n",
    "    # Construct the most likely sequence backwards.\n",
    "    path_scores = path_scores[-1].view(-1)\n",
    "    max_k = min(path_scores.size()[0], top_k)\n",
    "    viterbi_scores, best_paths = torch.topk(path_scores, k=max_k, dim=0)\n",
    "\n",
    "    viterbi_paths = []\n",
    "    for i in range(max_k):\n",
    "\n",
    "        viterbi_path = [best_paths[i].item()]\n",
    "        for backward_timestep in reversed(path_indices):\n",
    "            viterbi_path.append(\n",
    "                int(backward_timestep.view(-1)[viterbi_path[-1]]))\n",
    "\n",
    "        # Reverse the backward path.\n",
    "        viterbi_path.reverse()\n",
    "\n",
    "        # Viterbi paths uses (num_tags * n_permutations) nodes; therefore, we need to modulo.\n",
    "        viterbi_path = [j % num_tags for j in viterbi_path]\n",
    "        viterbi_paths.append(viterbi_path)\n",
    "\n",
    "    return viterbi_paths, viterbi_scores\n",
    "\n",
    "\n",
    "def batch_viterbi_decode(tag_sequence: torch.Tensor, transition_matrix: torch.Tensor, top_k: int = 5):\n",
    "\n",
    "    outputs = []\n",
    "    scores = []\n",
    "    for i in range(tag_sequence.shape[1]):\n",
    "        paths, score = viterbi_decode(tag_sequence[:, i, :], transitions)\n",
    "        outputs.append(paths)\n",
    "        scores.append(score)\n",
    "\n",
    "    return torch.tensor(outputs).transpose(0, -1), torch.cat(scores)\n",
    "\n",
    "\n",
    "def top_batch_viterbi_decode(tag_sequence: torch.Tensor):\n",
    "    output, _ = batch_viterbi_decode(tag_sequence, transitions, top_k=1)\n",
    "    return output[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://martin-thoma.com/word-error-rate-calculation/\n",
    "\n",
    "\n",
    "def levenshtein_distance(r, h, device=None):\n",
    "\n",
    "    # initialisation\n",
    "    d = torch.zeros((len(r)+1, len(h)+1), dtype=torch.long, device=device)\n",
    "    d[0, :] = torch.arange(0, len(h)+1, dtype=torch.long, device=device)\n",
    "    d[:, 0] = torch.arange(0, len(r)+1, dtype=torch.long, device=device)\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i, j] = d[i-1, j-1]\n",
    "            else:\n",
    "                substitution = d[i-1, j-1] + 1\n",
    "                insertion = d[i, j-1] + 1\n",
    "                deletion = d[i-1, j] + 1\n",
    "                d[i, j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    dist = d[len(r)][len(h)].item()\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.arch == \"wav2letter\":\n",
    "    model = Wav2Letter(num_features, vocab_size)\n",
    "\n",
    "    def model_length_function(tensor):\n",
    "        return int(tensor.shape[0])//2 + 1\n",
    "\n",
    "elif args.arch == \"lstm\":\n",
    "    model = LSTMModel(num_features, vocab_size, **lstm_params)\n",
    "\n",
    "    def model_length_function(tensor):\n",
    "        return int(tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_after_model = {}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    tensors = [b[0] for b in batch if b]\n",
    "\n",
    "    if False:\n",
    "        for tensor in tensors:\n",
    "            shape = int(tensor.shape[0])\n",
    "            if shape not in shape_after_model:\n",
    "                tensor = tensor.t().unsqueeze(0)\n",
    "\n",
    "                training = model.training\n",
    "                model.eval()\n",
    "                output = model(tensor)\n",
    "                model.train(training)\n",
    "\n",
    "                shape_after_model[shape] = int(output.shape[1])\n",
    "\n",
    "        tensors_lengths = torch.tensor(\n",
    "            [shape_after_model[int(t.shape[0])] for t in tensors], dtype=torch.long, device=tensors[0].device\n",
    "        )\n",
    "        # print(tensors_lengths)\n",
    "\n",
    "    if True:\n",
    "        tensors_lengths = torch.tensor(\n",
    "            [model_length_function(t) for t in tensors], dtype=torch.long, device=tensors[0].device\n",
    "        )\n",
    "        # print(tensors_lengths)\n",
    "\n",
    "    if False:\n",
    "        # (batch, seq_len, num_directions * hidden_size)\n",
    "        tensors_lengths = torch.tensor(\n",
    "            [int(t.shape[-1]) for t in tensors], dtype=torch.long, device=tensors[0].device\n",
    "        )\n",
    "\n",
    "    # print([int(t.shape[0]) for t in tensors])\n",
    "    # print([shape_after_model[int(t.shape[0])] for t in tensors])\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "\n",
    "    targets = [b[1] for b in batch if b]\n",
    "    target_lengths = torch.tensor(\n",
    "        [target.shape[0] for target in targets], dtype=torch.long, device=tensors.device\n",
    "    )\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "    # print(targets.shape, flush=True)\n",
    "    # print(decode(targets.tolist()), flush=True)\n",
    "\n",
    "    return tensors, targets, tensors_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model cuda\n"
     ]
    }
   ],
   "source": [
    "if args.jit:\n",
    "    model = torch.jit.script(model)\n",
    "\n",
    "if not args.distributed:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "else:\n",
    "    model.cuda()\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    # model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)\n",
    "\n",
    "model = model.to(device, non_blocking=non_blocking)\n",
    "print('model cuda', flush=True)\n",
    "# model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "if printing_machine:\n",
    "    n = count_parameters(model)\n",
    "    print(\"Number of parameters: \", n, flush=True)\n",
    "    print(\"Approximate space taken: \", n * 4 / (10 ** 6), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(model.parameters(), **optimizer_params)\n",
    "scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "# scheduler = ReduceLROnPlateau(optimizer)\n",
    "\n",
    "criterion = torch.nn.CTCLoss(zero_infinity=zero_infinity)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = torch.nn.NLLLoss()\n",
    "\n",
    "best_loss = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 42\n"
     ]
    }
   ],
   "source": [
    "loader_training = DataLoader(\n",
    "    training, batch_size=batch_size, collate_fn=collate_fn, **data_loader_training_params\n",
    ")\n",
    "\n",
    "loader_validation = DataLoader(\n",
    "    validation, batch_size=batch_size, collate_fn=collate_fn, **data_loader_validation_params\n",
    ")\n",
    "\n",
    "print(len(loader_training), len(loader_validation), flush=True)\n",
    "\n",
    "# num_features = next(iter(loader_training))[0].shape[1]\n",
    "# print(num_features, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(inputs, targets, tensors_lengths, target_lengths):\n",
    "\n",
    "    inputs = inputs.to(device, non_blocking=non_blocking)\n",
    "    targets = targets.to(device, non_blocking=non_blocking)\n",
    "\n",
    "    # keep batch first for data parallel\n",
    "    outputs = model(inputs).transpose(0, 1)\n",
    "\n",
    "    this_batch_size = outputs.shape[1]\n",
    "    seq_len = outputs.shape[0]\n",
    "    # input_lengths = torch.full((this_batch_size,), seq_len, dtype=torch.long, device=outputs.device)\n",
    "    input_lengths = tensors_lengths\n",
    "\n",
    "    # CTC\n",
    "    # outputs: input length, batch size, number of classes (including blank)\n",
    "    # targets: batch size, max target length\n",
    "    # input_lengths: batch size\n",
    "    # target_lengths: batch size\n",
    "\n",
    "    return criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "\n",
    "def forward_decode(output, targets, decoder):\n",
    "\n",
    "    output = model(inputs).to(\"cpu\")\n",
    "    output = decoder(output)\n",
    "\n",
    "    output = decode(output.tolist())\n",
    "    target = decode(targets.tolist())\n",
    "\n",
    "    print_length = 20\n",
    "    output_print = output[0].ljust(print_length)[:print_length]\n",
    "    target_print = target[0].ljust(print_length)[:print_length]\n",
    "    print(\n",
    "        f\"Epoch: {epoch:4}   Target: {target_print}   Output: {output_print}\", flush=True)\n",
    "\n",
    "    cers = [levenshtein_distance(a, b) for a, b in zip(target, output)]\n",
    "    cers_normalized = [d/len(a) for a, d in zip(target, cers)]\n",
    "    cers = statistics.mean(cers)\n",
    "    cers_normalized = statistics.mean(cers_normalized)\n",
    "\n",
    "    output = [o.split(char_space) for o in output]\n",
    "    target = [o.split(char_space) for o in target]\n",
    "\n",
    "    wers = [levenshtein_distance(a, b) for a, b in zip(target, output)]\n",
    "    wers_normalized = [d/len(a) for a, d in zip(target, wers)]\n",
    "    wers = statistics.mean(wers)\n",
    "    wers_normalized = statistics.mean(wers_normalized)\n",
    "\n",
    "    print(f\"Epoch: {epoch:4}   CER: {cers:1.5f}   WER: {wers:1.5f}\", flush=True)\n",
    "\n",
    "    return cers, wers, cers_normalized, wers_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: not found\n",
      "Checkpoint: saved\n"
     ]
    }
   ],
   "source": [
    "history_training = defaultdict(list)\n",
    "history_validation = defaultdict(list)\n",
    "\n",
    "if args.resume and os.path.isfile(CHECKPOINT_filename):\n",
    "    print(\"Checkpoint: loading '{}'\".format(CHECKPOINT_filename))\n",
    "    checkpoint = torch.load(CHECKPOINT_filename)\n",
    "\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    history_training = checkpoint['history_training']\n",
    "    history_validation = checkpoint['history_validation']\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    print(\"Checkpoint: loaded '{}' at epoch {}\".format(\n",
    "        CHECKPOINT_filename, checkpoint['epoch']))\n",
    "    print(tabulate(history_training, headers=\"keys\"), flush=True)\n",
    "    print(tabulate(history_validation, headers=\"keys\"), flush=True)\n",
    "else:\n",
    "    print(\"Checkpoint: not found\")\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': start_epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'history_training': history_training,\n",
    "        'history_validation': history_validation,\n",
    "    }, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c43f1389bb46788a4eca9528caaa8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0   Train: 40.12559   Validation: 39.63125\n",
      "greedy decoder\n",
      "Epoch:    0   Target: then back i turned m   Output: tttttttttttttttttttt\n",
      "Epoch:    0   CER: 4365.82812   WER: 75.59375\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0     40.1256\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0     39.6313       4365.83                  39.4246       75.5938                  3.28551\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7817 MB |    4145 GB |    4145 GB |\n",
      "|       from large pool |  101956 KB |    7816 MB |    4046 GB |    4046 GB |\n",
      "|       from small pool |     550 KB |       2 MB |      98 GB |      98 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7817 MB |    4145 GB |    4145 GB |\n",
      "|       from large pool |  109578 KB |    7816 MB |    4046 GB |    4046 GB |\n",
      "|       from small pool |     550 KB |       2 MB |      98 GB |      98 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  534992 KB |    5769 MB |    5841 GB |    5840 GB |\n",
      "|       from large pool |  533494 KB |    5767 MB |    5667 GB |    5666 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     174 GB |     174 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |  638680    |  638648    |\n",
      "|       from large pool |      12    |      26    |  200703    |  200691    |\n",
      "|       from small pool |      20    |      30    |  437977    |  437957    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |  638680    |  638647    |\n",
      "|       from large pool |      13    |      26    |  200703    |  200690    |\n",
      "|       from small pool |      20    |      30    |  437977    |  437957    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |  260898    |  260882    |\n",
      "|       from large pool |      11    |      20    |  101966    |  101955    |\n",
      "|       from small pool |       5    |       8    |  158932    |  158927    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   10   Train: 11.30164   Validation: 12.11473\n",
      "greedy decoder\n",
      "Epoch:   10   Target: then back i turned m   Output: ssssssssssseee      \n",
      "Epoch:   10   CER: 134.32812   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0     40.1256\n",
      "      1                0     38.0185\n",
      "      2                0     34.9734\n",
      "      3                0     31.2684\n",
      "      4                0     27.2764\n",
      "      5                0     23.4498\n",
      "      6                0     20.0533\n",
      "      7                0     17.1547\n",
      "      8                0     14.763\n",
      "      9                0     12.8554\n",
      "     10                0     11.3016\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0     39.6313      4365.83                  39.4246        75.5938                  3.28551\n",
      "     10     12.1147       134.328                  0.98916       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7817 MB |   35488 GB |   35488 GB |\n",
      "|       from large pool |  101956 KB |    7817 MB |   35371 GB |   35371 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     116 GB |     116 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7817 MB |   35488 GB |   35488 GB |\n",
      "|       from large pool |  109578 KB |    7817 MB |   35371 GB |   35371 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     116 GB |     116 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  514512 KB |    6335 MB |   53808 GB |   53808 GB |\n",
      "|       from large pool |  513014 KB |    6334 MB |   53613 GB |   53612 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     195 GB |     195 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    1094 K  |    1094 K  |\n",
      "|       from large pool |      12    |      26    |     419 K  |     419 K  |\n",
      "|       from small pool |      20    |      30    |     675 K  |     675 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    1094 K  |    1094 K  |\n",
      "|       from large pool |      13    |      26    |     419 K  |     419 K  |\n",
      "|       from small pool |      20    |      30    |     675 K  |     675 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |  473473    |  473457    |\n",
      "|       from large pool |      11    |      20    |  205126    |  205115    |\n",
      "|       from small pool |       5    |       8    |  268347    |  268342    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   20   Train: 5.53094   Validation: 6.74879\n",
      "greedy decoder\n",
      "Epoch:   20   Target: then back i turned m   Output: sssseke             \n",
      "Epoch:   20   CER: 134.68750   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7817 MB |   66824 GB |   66824 GB |\n",
      "|       from large pool |  101956 KB |    7817 MB |   66689 GB |   66689 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     134 GB |     134 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7817 MB |   66824 GB |   66824 GB |\n",
      "|       from large pool |  109578 KB |    7817 MB |   66689 GB |   66689 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     134 GB |     134 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  514512 KB |    6439 MB |  101413 GB |  101413 GB |\n",
      "|       from large pool |  513014 KB |    6437 MB |  101196 GB |  101196 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     216 GB |     216 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    1549 K  |    1549 K  |\n",
      "|       from large pool |      12    |      26    |     638 K  |     638 K  |\n",
      "|       from small pool |      20    |      30    |     911 K  |     911 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    1549 K  |    1549 K  |\n",
      "|       from large pool |      13    |      26    |     638 K  |     638 K  |\n",
      "|       from small pool |      20    |      30    |     911 K  |     911 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |  684836    |  684820    |\n",
      "|       from large pool |      11    |      20    |  307358    |  307347    |\n",
      "|       from small pool |       5    |       8    |  377478    |  377473    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   40   Train: 4.23226   Validation: 5.22908\n",
      "greedy decoder\n",
      "Epoch:   40   Target: then back i turned m   Output: eee                 \n",
      "Epoch:   40   CER: 134.87500   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7817 MB |  129516 GB |  129516 GB |\n",
      "|       from large pool |  101956 KB |    7817 MB |  129345 GB |  129345 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     170 GB |     170 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7817 MB |  129516 GB |  129516 GB |\n",
      "|       from large pool |  109578 KB |    7817 MB |  129345 GB |  129345 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     170 GB |     170 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  534992 KB |    6439 MB |  197117 GB |  197117 GB |\n",
      "|       from large pool |  533494 KB |    6437 MB |  196858 GB |  196857 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     259 GB |     259 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    2459 K  |    2459 K  |\n",
      "|       from large pool |      12    |      26    |    1075 K  |    1075 K  |\n",
      "|       from small pool |      20    |      30    |    1383 K  |    1383 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    2459 K  |    2459 K  |\n",
      "|       from large pool |      13    |      26    |    1075 K  |    1075 K  |\n",
      "|       from small pool |      20    |      30    |    1383 K  |    1383 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    1108 K  |    1108 K  |\n",
      "|       from large pool |      11    |      20    |     512 K  |     512 K  |\n",
      "|       from small pool |       5    |       8    |     595 K  |     595 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   60   Train: 4.07142   Validation: 4.97507\n",
      "greedy decoder\n",
      "Epoch:   60   Target: then back i turned m   Output: eee                 \n",
      "Epoch:   60   CER: 134.95312   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "     41                0     4.24005\n",
      "     42                0     4.20727\n",
      "     43                0     4.20134\n",
      "     44                0     4.18422\n",
      "     45                0     4.17529\n",
      "     46                0     4.16786\n",
      "     47                0     4.16866\n",
      "     48                0     4.1458\n",
      "     49                0     4.13627\n",
      "     50                0     4.12785\n",
      "     51                0     4.12883\n",
      "     52                0     4.10335\n",
      "     53                0     4.11381\n",
      "     54                0     4.08613\n",
      "     55                0     4.10285\n",
      "     56                0     4.08266\n",
      "     57                0     4.08009\n",
      "     58                0     4.07873\n",
      "     59                0     4.08494\n",
      "     60                0     4.07142\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "     50     5.06194       134.891                 0.99229        26.0625                  1\n",
      "     60     4.97507       134.953                 0.992886       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  192205 GB |  192205 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  191998 GB |  191998 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     206 GB |     206 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  192205 GB |  192204 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  191998 GB |  191998 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     206 GB |     206 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  514512 KB |    6439 MB |  292012 GB |  292012 GB |\n",
      "|       from large pool |  513014 KB |    6437 MB |  291710 GB |  291710 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     301 GB |     301 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    3369 K  |    3369 K  |\n",
      "|       from large pool |      12    |      26    |    1513 K  |    1513 K  |\n",
      "|       from small pool |      20    |      30    |    1856 K  |    1856 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    3369 K  |    3369 K  |\n",
      "|       from large pool |      13    |      26    |    1513 K  |    1513 K  |\n",
      "|       from small pool |      20    |      30    |    1856 K  |    1856 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    1531 K  |    1531 K  |\n",
      "|       from large pool |      11    |      20    |     717 K  |     717 K  |\n",
      "|       from small pool |       5    |       8    |     814 K  |     813 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   70   Train: 4.02520   Validation: 4.92443\n",
      "greedy decoder\n",
      "Epoch:   70   Target: then back i turned m   Output: eee                 \n",
      "Epoch:   70   CER: 134.96875   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "     41                0     4.24005\n",
      "     42                0     4.20727\n",
      "     43                0     4.20134\n",
      "     44                0     4.18422\n",
      "     45                0     4.17529\n",
      "     46                0     4.16786\n",
      "     47                0     4.16866\n",
      "     48                0     4.1458\n",
      "     49                0     4.13627\n",
      "     50                0     4.12785\n",
      "     51                0     4.12883\n",
      "     52                0     4.10335\n",
      "     53                0     4.11381\n",
      "     54                0     4.08613\n",
      "     55                0     4.10285\n",
      "     56                0     4.08266\n",
      "     57                0     4.08009\n",
      "     58                0     4.07873\n",
      "     59                0     4.08494\n",
      "     60                0     4.07142\n",
      "     61                0     4.06995\n",
      "     62                0     4.07212\n",
      "     63                0     4.06366\n",
      "     64                0     4.05879\n",
      "     65                0     4.05531\n",
      "     66                0     4.04853\n",
      "     67                0     4.05842\n",
      "     68                0     4.05188\n",
      "     69                0     4.04428\n",
      "     70                0     4.0252\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "     50     5.06194       134.891                 0.99229        26.0625                  1\n",
      "     60     4.97507       134.953                 0.992886       26.0625                  1\n",
      "     70     4.92443       134.969                 0.993034       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  223543 GB |  223543 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  223319 GB |  223318 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     224 GB |     224 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  223543 GB |  223543 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  223319 GB |  223318 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     224 GB |     224 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  285136 KB |    6439 MB |  339456 GB |  339456 GB |\n",
      "|       from large pool |  283638 KB |    6437 MB |  339133 GB |  339133 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     323 GB |     323 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    3824 K  |    3824 K  |\n",
      "|       from large pool |      12    |      26    |    1731 K  |    1731 K  |\n",
      "|       from small pool |      20    |      30    |    2092 K  |    2092 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    3824 K  |    3824 K  |\n",
      "|       from large pool |      13    |      26    |    1731 K  |    1731 K  |\n",
      "|       from small pool |      20    |      30    |    2092 K  |    2092 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    1743 K  |    1743 K  |\n",
      "|       from large pool |      11    |      20    |     820 K  |     820 K  |\n",
      "|       from small pool |       5    |       8    |     923 K  |     923 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   80   Train: 4.03227   Validation: 4.89371\n",
      "greedy decoder\n",
      "Epoch:   80   Target: then back i turned m   Output: eee                 \n",
      "Epoch:   80   CER: 134.98438   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "     41                0     4.24005\n",
      "     42                0     4.20727\n",
      "     43                0     4.20134\n",
      "     44                0     4.18422\n",
      "     45                0     4.17529\n",
      "     46                0     4.16786\n",
      "     47                0     4.16866\n",
      "     48                0     4.1458\n",
      "     49                0     4.13627\n",
      "     50                0     4.12785\n",
      "     51                0     4.12883\n",
      "     52                0     4.10335\n",
      "     53                0     4.11381\n",
      "     54                0     4.08613\n",
      "     55                0     4.10285\n",
      "     56                0     4.08266\n",
      "     57                0     4.08009\n",
      "     58                0     4.07873\n",
      "     59                0     4.08494\n",
      "     60                0     4.07142\n",
      "     61                0     4.06995\n",
      "     62                0     4.07212\n",
      "     63                0     4.06366\n",
      "     64                0     4.05879\n",
      "     65                0     4.05531\n",
      "     66                0     4.04853\n",
      "     67                0     4.05842\n",
      "     68                0     4.05188\n",
      "     69                0     4.04428\n",
      "     70                0     4.0252\n",
      "     71                0     4.02789\n",
      "     72                0     4.03199\n",
      "     73                0     4.04147\n",
      "     74                0     4.04215\n",
      "     75                0     4.03527\n",
      "     76                0     4.02748\n",
      "     77                0     4.0217\n",
      "     78                0     4.03178\n",
      "     79                0     4.01874\n",
      "     80                0     4.03227\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "     50     5.06194       134.891                 0.99229        26.0625                  1\n",
      "     60     4.97507       134.953                 0.992886       26.0625                  1\n",
      "     70     4.92443       134.969                 0.993034       26.0625                  1\n",
      "     80     4.89371       134.984                 0.993183       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  254890 GB |  254890 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  254647 GB |  254647 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     242 GB |     242 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  254890 GB |  254890 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  254647 GB |  254647 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     242 GB |     242 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  514512 KB |    6439 MB |  387696 GB |  387695 GB |\n",
      "|       from large pool |  513014 KB |    6437 MB |  387351 GB |  387351 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     344 GB |     344 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    4279 K  |    4279 K  |\n",
      "|       from large pool |      12    |      26    |    1950 K  |    1950 K  |\n",
      "|       from small pool |      20    |      30    |    2329 K  |    2328 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    4279 K  |    4279 K  |\n",
      "|       from large pool |      13    |      26    |    1950 K  |    1950 K  |\n",
      "|       from small pool |      20    |      30    |    2329 K  |    2328 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    1955 K  |    1955 K  |\n",
      "|       from large pool |      11    |      20    |     923 K  |     923 K  |\n",
      "|       from small pool |       5    |       8    |    1032 K  |    1032 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   90   Train: 4.00978   Validation: 4.87454\n",
      "greedy decoder\n",
      "Epoch:   90   Target: then back i turned m   Output: eee                 \n",
      "Epoch:   90   CER: 134.98438   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "     41                0     4.24005\n",
      "     42                0     4.20727\n",
      "     43                0     4.20134\n",
      "     44                0     4.18422\n",
      "     45                0     4.17529\n",
      "     46                0     4.16786\n",
      "     47                0     4.16866\n",
      "     48                0     4.1458\n",
      "     49                0     4.13627\n",
      "     50                0     4.12785\n",
      "     51                0     4.12883\n",
      "     52                0     4.10335\n",
      "     53                0     4.11381\n",
      "     54                0     4.08613\n",
      "     55                0     4.10285\n",
      "     56                0     4.08266\n",
      "     57                0     4.08009\n",
      "     58                0     4.07873\n",
      "     59                0     4.08494\n",
      "     60                0     4.07142\n",
      "     61                0     4.06995\n",
      "     62                0     4.07212\n",
      "     63                0     4.06366\n",
      "     64                0     4.05879\n",
      "     65                0     4.05531\n",
      "     66                0     4.04853\n",
      "     67                0     4.05842\n",
      "     68                0     4.05188\n",
      "     69                0     4.04428\n",
      "     70                0     4.0252\n",
      "     71                0     4.02789\n",
      "     72                0     4.03199\n",
      "     73                0     4.04147\n",
      "     74                0     4.04215\n",
      "     75                0     4.03527\n",
      "     76                0     4.02748\n",
      "     77                0     4.0217\n",
      "     78                0     4.03178\n",
      "     79                0     4.01874\n",
      "     80                0     4.03227\n",
      "     81                0     4.02237\n",
      "     82                0     4.02297\n",
      "     83                0     4.01918\n",
      "     84                0     4.01389\n",
      "     85                0     4.01599\n",
      "     86                0     4.01914\n",
      "     87                0     4.0159\n",
      "     88                0     4.01512\n",
      "     89                0     4.0067\n",
      "     90                0     4.00978\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "     50     5.06194       134.891                 0.99229        26.0625                  1\n",
      "     60     4.97507       134.953                 0.992886       26.0625                  1\n",
      "     70     4.92443       134.969                 0.993034       26.0625                  1\n",
      "     80     4.89371       134.984                 0.993183       26.0625                  1\n",
      "     90     4.87454       134.984                 0.993183       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  286233 GB |  286233 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  285972 GB |  285972 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     260 GB |     260 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  286233 GB |  286233 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  285972 GB |  285972 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     260 GB |     260 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  534992 KB |    6439 MB |  435417 GB |  435417 GB |\n",
      "|       from large pool |  533494 KB |    6437 MB |  435051 GB |  435051 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     365 GB |     365 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    4734 K  |    4734 K  |\n",
      "|       from large pool |      12    |      26    |    2169 K  |    2169 K  |\n",
      "|       from small pool |      20    |      30    |    2565 K  |    2565 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    4734 K  |    4734 K  |\n",
      "|       from large pool |      13    |      26    |    2169 K  |    2169 K  |\n",
      "|       from small pool |      20    |      30    |    2565 K  |    2565 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    2167 K  |    2167 K  |\n",
      "|       from large pool |      11    |      20    |    1026 K  |    1026 K  |\n",
      "|       from small pool |       5    |       8    |    1141 K  |    1141 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  110   Train: 4.00559   Validation: 4.85418\n",
      "greedy decoder\n",
      "Epoch:  110   Target: then back i turned m   Output: eee                 \n",
      "Epoch:  110   CER: 134.98438   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "     41                0     4.24005\n",
      "     42                0     4.20727\n",
      "     43                0     4.20134\n",
      "     44                0     4.18422\n",
      "     45                0     4.17529\n",
      "     46                0     4.16786\n",
      "     47                0     4.16866\n",
      "     48                0     4.1458\n",
      "     49                0     4.13627\n",
      "     50                0     4.12785\n",
      "     51                0     4.12883\n",
      "     52                0     4.10335\n",
      "     53                0     4.11381\n",
      "     54                0     4.08613\n",
      "     55                0     4.10285\n",
      "     56                0     4.08266\n",
      "     57                0     4.08009\n",
      "     58                0     4.07873\n",
      "     59                0     4.08494\n",
      "     60                0     4.07142\n",
      "     61                0     4.06995\n",
      "     62                0     4.07212\n",
      "     63                0     4.06366\n",
      "     64                0     4.05879\n",
      "     65                0     4.05531\n",
      "     66                0     4.04853\n",
      "     67                0     4.05842\n",
      "     68                0     4.05188\n",
      "     69                0     4.04428\n",
      "     70                0     4.0252\n",
      "     71                0     4.02789\n",
      "     72                0     4.03199\n",
      "     73                0     4.04147\n",
      "     74                0     4.04215\n",
      "     75                0     4.03527\n",
      "     76                0     4.02748\n",
      "     77                0     4.0217\n",
      "     78                0     4.03178\n",
      "     79                0     4.01874\n",
      "     80                0     4.03227\n",
      "     81                0     4.02237\n",
      "     82                0     4.02297\n",
      "     83                0     4.01918\n",
      "     84                0     4.01389\n",
      "     85                0     4.01599\n",
      "     86                0     4.01914\n",
      "     87                0     4.0159\n",
      "     88                0     4.01512\n",
      "     89                0     4.0067\n",
      "     90                0     4.00978\n",
      "     91                0     4.01849\n",
      "     92                0     4.0026\n",
      "     93                0     4.00247\n",
      "     94                0     4.00583\n",
      "     95                0     4.00591\n",
      "     96                0     4.00287\n",
      "     97                0     4.00157\n",
      "     98                0     4.01961\n",
      "     99                0     4.00741\n",
      "    100                0     4.00656\n",
      "    101                0     3.99359\n",
      "    102                0     3.99321\n",
      "    103                0     4.01048\n",
      "    104                0     3.98946\n",
      "    105                0     3.99331\n",
      "    106                0     3.98603\n",
      "    107                0     4.01059\n",
      "    108                0     3.99071\n",
      "    109                0     3.99207\n",
      "    110                0     4.00559\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "     50     5.06194       134.891                 0.99229        26.0625                  1\n",
      "     60     4.97507       134.953                 0.992886       26.0625                  1\n",
      "     70     4.92443       134.969                 0.993034       26.0625                  1\n",
      "     80     4.89371       134.984                 0.993183       26.0625                  1\n",
      "     90     4.87454       134.984                 0.993183       26.0625                  1\n",
      "    100     4.86219       134.984                 0.993183       26.0625                  1\n",
      "    110     4.85418       134.984                 0.993183       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  348917 GB |  348917 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  348620 GB |  348620 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     296 GB |     296 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  348917 GB |  348917 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  348620 GB |  348620 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     296 GB |     296 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  264656 KB |    6439 MB |  531267 GB |  531267 GB |\n",
      "|       from large pool |  263158 KB |    6437 MB |  530859 GB |  530859 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     408 GB |     408 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    5644 K  |    5644 K  |\n",
      "|       from large pool |      12    |      26    |    2606 K  |    2606 K  |\n",
      "|       from small pool |      20    |      30    |    3037 K  |    3037 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    5644 K  |    5644 K  |\n",
      "|       from large pool |      13    |      26    |    2606 K  |    2606 K  |\n",
      "|       from small pool |      20    |      30    |    3037 K  |    3037 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      25    |    2592 K  |    2592 K  |\n",
      "|       from large pool |      10    |      20    |    1232 K  |    1232 K  |\n",
      "|       from small pool |       5    |       8    |    1359 K  |    1359 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  140   Train: 3.99559   Validation: 4.84319\n",
      "greedy decoder\n",
      "Epoch:  140   Target: then back i turned m   Output: eee                 \n",
      "Epoch:  140   CER: 135.00000   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "     41                0     4.24005\n",
      "     42                0     4.20727\n",
      "     43                0     4.20134\n",
      "     44                0     4.18422\n",
      "     45                0     4.17529\n",
      "     46                0     4.16786\n",
      "     47                0     4.16866\n",
      "     48                0     4.1458\n",
      "     49                0     4.13627\n",
      "     50                0     4.12785\n",
      "     51                0     4.12883\n",
      "     52                0     4.10335\n",
      "     53                0     4.11381\n",
      "     54                0     4.08613\n",
      "     55                0     4.10285\n",
      "     56                0     4.08266\n",
      "     57                0     4.08009\n",
      "     58                0     4.07873\n",
      "     59                0     4.08494\n",
      "     60                0     4.07142\n",
      "     61                0     4.06995\n",
      "     62                0     4.07212\n",
      "     63                0     4.06366\n",
      "     64                0     4.05879\n",
      "     65                0     4.05531\n",
      "     66                0     4.04853\n",
      "     67                0     4.05842\n",
      "     68                0     4.05188\n",
      "     69                0     4.04428\n",
      "     70                0     4.0252\n",
      "     71                0     4.02789\n",
      "     72                0     4.03199\n",
      "     73                0     4.04147\n",
      "     74                0     4.04215\n",
      "     75                0     4.03527\n",
      "     76                0     4.02748\n",
      "     77                0     4.0217\n",
      "     78                0     4.03178\n",
      "     79                0     4.01874\n",
      "     80                0     4.03227\n",
      "     81                0     4.02237\n",
      "     82                0     4.02297\n",
      "     83                0     4.01918\n",
      "     84                0     4.01389\n",
      "     85                0     4.01599\n",
      "     86                0     4.01914\n",
      "     87                0     4.0159\n",
      "     88                0     4.01512\n",
      "     89                0     4.0067\n",
      "     90                0     4.00978\n",
      "     91                0     4.01849\n",
      "     92                0     4.0026\n",
      "     93                0     4.00247\n",
      "     94                0     4.00583\n",
      "     95                0     4.00591\n",
      "     96                0     4.00287\n",
      "     97                0     4.00157\n",
      "     98                0     4.01961\n",
      "     99                0     4.00741\n",
      "    100                0     4.00656\n",
      "    101                0     3.99359\n",
      "    102                0     3.99321\n",
      "    103                0     4.01048\n",
      "    104                0     3.98946\n",
      "    105                0     3.99331\n",
      "    106                0     3.98603\n",
      "    107                0     4.01059\n",
      "    108                0     3.99071\n",
      "    109                0     3.99207\n",
      "    110                0     4.00559\n",
      "    111                0     4.00471\n",
      "    112                0     3.9891\n",
      "    113                0     4.00456\n",
      "    114                0     4.00269\n",
      "    115                0     4.00445\n",
      "    116                0     3.99896\n",
      "    117                0     3.99821\n",
      "    118                0     3.99936\n",
      "    119                0     4.00789\n",
      "    120                0     3.99461\n",
      "    121                0     4.00531\n",
      "    122                0     3.99862\n",
      "    123                0     3.98748\n",
      "    124                0     3.99746\n",
      "    125                0     3.99949\n",
      "    126                0     3.99933\n",
      "    127                0     3.99414\n",
      "    128                0     3.99834\n",
      "    129                0     3.98134\n",
      "    130                0     3.99739\n",
      "    131                0     3.99248\n",
      "    132                0     3.98716\n",
      "    133                0     3.99181\n",
      "    134                0     3.99146\n",
      "    135                0     4.01283\n",
      "    136                0     3.98742\n",
      "    137                0     4.0017\n",
      "    138                0     3.99198\n",
      "    139                0     3.98897\n",
      "    140                0     3.99559\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "     50     5.06194       134.891                 0.99229        26.0625                  1\n",
      "     60     4.97507       134.953                 0.992886       26.0625                  1\n",
      "     70     4.92443       134.969                 0.993034       26.0625                  1\n",
      "     80     4.89371       134.984                 0.993183       26.0625                  1\n",
      "     90     4.87454       134.984                 0.993183       26.0625                  1\n",
      "    100     4.86219       134.984                 0.993183       26.0625                  1\n",
      "    110     4.85418       134.984                 0.993183       26.0625                  1\n",
      "    120     4.84895       134.984                 0.993183       26.0625                  1\n",
      "    130     4.8455        134.984                 0.993183       26.0625                  1\n",
      "    140     4.84319       135                     0.993427       26.0625                  1\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  442947 GB |  442946 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  442596 GB |  442596 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     350 GB |     350 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  442947 GB |  442946 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  442596 GB |  442596 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     350 GB |     350 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  514512 KB |    6439 MB |  674214 GB |  674214 GB |\n",
      "|       from large pool |  513014 KB |    6437 MB |  673742 GB |  673742 GB |\n",
      "|       from small pool |    1498 KB |       3 MB |     472 GB |     472 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    7009 K  |    7009 K  |\n",
      "|       from large pool |      12    |      26    |    3262 K  |    3262 K  |\n",
      "|       from small pool |      20    |      30    |    3746 K  |    3746 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    7009 K  |    7009 K  |\n",
      "|       from large pool |      13    |      26    |    3262 K  |    3262 K  |\n",
      "|       from small pool |      20    |      30    |    3746 K  |    3746 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    3227 K  |    3227 K  |\n",
      "|       from large pool |      11    |      20    |    1540 K  |    1540 K  |\n",
      "|       from small pool |       5    |       8    |    1687 K  |    1687 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  180   Train: 3.98565   Validation: 4.83998\n",
      "greedy decoder\n",
      "Epoch:  180   Target: then back i turned m   Output: eee                 \n",
      "Epoch:  180   CER: 135.00000   WER: 26.06250\n",
      "viterbi decoder\n",
      "Checkpoint: saved\n",
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "     41                0     4.24005\n",
      "     42                0     4.20727\n",
      "     43                0     4.20134\n",
      "     44                0     4.18422\n",
      "     45                0     4.17529\n",
      "     46                0     4.16786\n",
      "     47                0     4.16866\n",
      "     48                0     4.1458\n",
      "     49                0     4.13627\n",
      "     50                0     4.12785\n",
      "     51                0     4.12883\n",
      "     52                0     4.10335\n",
      "     53                0     4.11381\n",
      "     54                0     4.08613\n",
      "     55                0     4.10285\n",
      "     56                0     4.08266\n",
      "     57                0     4.08009\n",
      "     58                0     4.07873\n",
      "     59                0     4.08494\n",
      "     60                0     4.07142\n",
      "     61                0     4.06995\n",
      "     62                0     4.07212\n",
      "     63                0     4.06366\n",
      "     64                0     4.05879\n",
      "     65                0     4.05531\n",
      "     66                0     4.04853\n",
      "     67                0     4.05842\n",
      "     68                0     4.05188\n",
      "     69                0     4.04428\n",
      "     70                0     4.0252\n",
      "     71                0     4.02789\n",
      "     72                0     4.03199\n",
      "     73                0     4.04147\n",
      "     74                0     4.04215\n",
      "     75                0     4.03527\n",
      "     76                0     4.02748\n",
      "     77                0     4.0217\n",
      "     78                0     4.03178\n",
      "     79                0     4.01874\n",
      "     80                0     4.03227\n",
      "     81                0     4.02237\n",
      "     82                0     4.02297\n",
      "     83                0     4.01918\n",
      "     84                0     4.01389\n",
      "     85                0     4.01599\n",
      "     86                0     4.01914\n",
      "     87                0     4.0159\n",
      "     88                0     4.01512\n",
      "     89                0     4.0067\n",
      "     90                0     4.00978\n",
      "     91                0     4.01849\n",
      "     92                0     4.0026\n",
      "     93                0     4.00247\n",
      "     94                0     4.00583\n",
      "     95                0     4.00591\n",
      "     96                0     4.00287\n",
      "     97                0     4.00157\n",
      "     98                0     4.01961\n",
      "     99                0     4.00741\n",
      "    100                0     4.00656\n",
      "    101                0     3.99359\n",
      "    102                0     3.99321\n",
      "    103                0     4.01048\n",
      "    104                0     3.98946\n",
      "    105                0     3.99331\n",
      "    106                0     3.98603\n",
      "    107                0     4.01059\n",
      "    108                0     3.99071\n",
      "    109                0     3.99207\n",
      "    110                0     4.00559\n",
      "    111                0     4.00471\n",
      "    112                0     3.9891\n",
      "    113                0     4.00456\n",
      "    114                0     4.00269\n",
      "    115                0     4.00445\n",
      "    116                0     3.99896\n",
      "    117                0     3.99821\n",
      "    118                0     3.99936\n",
      "    119                0     4.00789\n",
      "    120                0     3.99461\n",
      "    121                0     4.00531\n",
      "    122                0     3.99862\n",
      "    123                0     3.98748\n",
      "    124                0     3.99746\n",
      "    125                0     3.99949\n",
      "    126                0     3.99933\n",
      "    127                0     3.99414\n",
      "    128                0     3.99834\n",
      "    129                0     3.98134\n",
      "    130                0     3.99739\n",
      "    131                0     3.99248\n",
      "    132                0     3.98716\n",
      "    133                0     3.99181\n",
      "    134                0     3.99146\n",
      "    135                0     4.01283\n",
      "    136                0     3.98742\n",
      "    137                0     4.0017\n",
      "    138                0     3.99198\n",
      "    139                0     3.98897\n",
      "    140                0     3.99559\n",
      "    141                0     3.99855\n",
      "    142                0     3.97898\n",
      "    143                0     4.00264\n",
      "    144                0     3.98482\n",
      "    145                0     3.98935\n",
      "    146                0     3.99457\n",
      "    147                0     3.99311\n",
      "    148                0     3.99809\n",
      "    149                0     3.98797\n",
      "    150                0     3.9927\n",
      "    151                0     3.98113\n",
      "    152                0     3.99715\n",
      "    153                0     3.98662\n",
      "    154                0     3.99732\n",
      "    155                0     3.97313\n",
      "    156                0     3.99284\n",
      "    157                0     4.00363\n",
      "    158                0     3.99158\n",
      "    159                0     3.98619\n",
      "    160                0     3.98967\n",
      "    161                0     3.98119\n",
      "    162                0     3.9947\n",
      "    163                0     3.99967\n",
      "    164                0     3.99221\n",
      "    165                0     3.98731\n",
      "    166                0     4.00265\n",
      "    167                0     3.98752\n",
      "    168                0     3.98431\n",
      "    169                0     4.00379\n",
      "    170                0     3.98676\n",
      "    171                0     4.00282\n",
      "    172                0     4.00085\n",
      "    173                0     3.99531\n",
      "    174                0     3.99678\n",
      "    175                0     3.99588\n",
      "    176                0     3.98912\n",
      "    177                0     3.99001\n",
      "    178                0     3.99451\n",
      "    179                0     3.99637\n",
      "    180                0     3.98565\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "     50     5.06194       134.891                 0.99229        26.0625                  1\n",
      "     60     4.97507       134.953                 0.992886       26.0625                  1\n",
      "     70     4.92443       134.969                 0.993034       26.0625                  1\n",
      "     80     4.89371       134.984                 0.993183       26.0625                  1\n",
      "     90     4.87454       134.984                 0.993183       26.0625                  1\n",
      "    100     4.86219       134.984                 0.993183       26.0625                  1\n",
      "    110     4.85418       134.984                 0.993183       26.0625                  1\n",
      "    120     4.84895       134.984                 0.993183       26.0625                  1\n",
      "    130     4.8455        134.984                 0.993183       26.0625                  1\n",
      "    140     4.84319       135                     0.993427       26.0625                  1\n",
      "    150     4.84166       135                     0.993427       26.0625                  1\n",
      "    160     4.84068       135                     0.993427       26.0625                  1\n",
      "    170     4.84018       135                     0.993427       26.0625                  1\n",
      "    180     4.83998       135                     0.993427       26.0625                  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  568310 GB |  568310 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  567887 GB |  567887 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     422 GB |     422 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  568310 GB |  568310 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  567887 GB |  567887 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     422 GB |     422 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  534992 KB |    6439 MB |     844 TB |     844 TB |\n",
      "|       from large pool |  533494 KB |    6437 MB |     844 TB |     844 TB |\n",
      "|       from small pool |    1498 KB |       3 MB |       0 TB |       0 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    8829 K  |    8829 K  |\n",
      "|       from large pool |      12    |      26    |    4137 K  |    4137 K  |\n",
      "|       from small pool |      20    |      30    |    4691 K  |    4691 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    8829 K  |    8829 K  |\n",
      "|       from large pool |      13    |      26    |    4137 K  |    4137 K  |\n",
      "|       from small pool |      20    |      30    |    4691 K  |    4691 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    4074 K  |    4074 K  |\n",
      "|       from large pool |      11    |      20    |    1950 K  |    1950 K  |\n",
      "|       from small pool |       5    |       8    |    2123 K  |    2123 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=max_epoch, unit_scale=1, disable=args.distributed) as pbar:\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        sum_loss = 0.\n",
    "        total_norm = 0.\n",
    "        for inputs, targets, tensors_lengths, target_lengths in loader_training:\n",
    "\n",
    "            loss = forward_loss(\n",
    "                inputs, targets, tensors_lengths, target_lengths)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            norm = 0.\n",
    "            if clip_norm > 0:\n",
    "                norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), clip_norm)\n",
    "                total_norm += norm\n",
    "            elif args.gradient:\n",
    "                for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
    "                    norm += p.grad.data.norm(2).item() ** 2\n",
    "                norm = norm ** .5\n",
    "                total_norm += norm\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if SIGNAL_RECEIVED:\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_loss': best_loss,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'history_training': history_training,\n",
    "                    'history_validation': history_validation,\n",
    "                }, False)\n",
    "                trigger_job_requeue()\n",
    "\n",
    "            pbar.update(1/len(loader_training))\n",
    "\n",
    "        total_norm = (total_norm ** .5) / len(loader_training)\n",
    "        if total_norm > 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch:4}   Gradient: {total_norm:4.5f}\", flush=True)\n",
    "\n",
    "        # Average loss\n",
    "        sum_loss = sum_loss / len(loader_training)\n",
    "        sum_loss_str = f\"Epoch: {epoch:4}   Train: {sum_loss:4.5f}\"\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history_training[\"epoch\"].append(epoch)\n",
    "        history_training[\"gradient_norm\"].append(total_norm)\n",
    "        history_training[\"sum_loss\"].append(sum_loss)\n",
    "\n",
    "        if not epoch % mod_epoch or epoch == max_epoch - 1:\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Switch to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                sum_loss = 0.\n",
    "                for inputs, targets, tensors_lengths, target_lengths in loader_validation:\n",
    "                    sum_loss += forward_loss(inputs, targets,\n",
    "                                             tensors_lengths, target_lengths).item()\n",
    "\n",
    "                    if SIGNAL_RECEIVED:\n",
    "                        break\n",
    "\n",
    "                # Average loss\n",
    "                sum_loss = sum_loss / len(loader_validation)\n",
    "                sum_loss_str += f\"   Validation: {sum_loss:.5f}\"\n",
    "                print(sum_loss_str, flush=True)\n",
    "\n",
    "                print(\"greedy decoder\", flush=True)\n",
    "                cer1, wer1, cern1, wern1 = forward_decode(\n",
    "                    inputs, targets, greedy_decode)\n",
    "\n",
    "                print(\"viterbi decoder\", flush=True)\n",
    "                # cer2, wer2, cern2, wern2 = forward_decode(inputs, targets, top_batch_viterbi_decode)\n",
    "\n",
    "                history_validation[\"epoch\"].append(epoch)\n",
    "                history_validation[\"sum_loss\"].append(sum_loss)\n",
    "                history_validation[\"greedy_cer\"].append(cer1)\n",
    "                history_validation[\"greedy_cer_normalized\"].append(cern1)\n",
    "                history_validation[\"greedy_wer\"].append(wer1)\n",
    "                history_validation[\"greedy_wer_normalized\"].append(wern1)\n",
    "                # history_validation[\"viterbi_cer\"].append(cer2)\n",
    "                # history_validation[\"viterbi_cer_normalized\"].append(cern2)\n",
    "                # history_validation[\"viterbi_wer\"].append(wer2)\n",
    "                # history_validation[\"viterbi_wer_normalized\"].append(wern2)\n",
    "\n",
    "                is_best = sum_loss < best_loss\n",
    "                best_loss = min(sum_loss, best_loss)\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_loss': best_loss,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'history_training': history_training,\n",
    "                    'history_validation': history_validation,\n",
    "                }, is_best)\n",
    "\n",
    "                print(tabulate(history_training, headers=\"keys\"), flush=True)\n",
    "                print(tabulate(history_validation, headers=\"keys\"), flush=True)\n",
    "                print(torch.cuda.memory_summary(), flush=True)\n",
    "\n",
    "                # scheduler.step(sum_loss)\n",
    "\n",
    "    # Create an empty file HALT_filename, mark the job as finished\n",
    "    if epoch == max_epoch - 1:\n",
    "        open(HALT_filename, 'a').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    gradient_norm    sum_loss\n",
      "-------  ---------------  ----------\n",
      "      0                0    40.1256\n",
      "      1                0    38.0185\n",
      "      2                0    34.9734\n",
      "      3                0    31.2684\n",
      "      4                0    27.2764\n",
      "      5                0    23.4498\n",
      "      6                0    20.0533\n",
      "      7                0    17.1547\n",
      "      8                0    14.763\n",
      "      9                0    12.8554\n",
      "     10                0    11.3016\n",
      "     11                0    10.0745\n",
      "     12                0     9.06195\n",
      "     13                0     8.26623\n",
      "     14                0     7.61511\n",
      "     15                0     7.10378\n",
      "     16                0     6.66027\n",
      "     17                0     6.3107\n",
      "     18                0     5.99656\n",
      "     19                0     5.756\n",
      "     20                0     5.53094\n",
      "     21                0     5.36743\n",
      "     22                0     5.19205\n",
      "     23                0     5.07226\n",
      "     24                0     4.93927\n",
      "     25                0     4.84922\n",
      "     26                0     4.78321\n",
      "     27                0     4.68995\n",
      "     28                0     4.64453\n",
      "     29                0     4.58742\n",
      "     30                0     4.53896\n",
      "     31                0     4.47678\n",
      "     32                0     4.44843\n",
      "     33                0     4.41811\n",
      "     34                0     4.37321\n",
      "     35                0     4.35532\n",
      "     36                0     4.32562\n",
      "     37                0     4.30676\n",
      "     38                0     4.27191\n",
      "     39                0     4.26464\n",
      "     40                0     4.23226\n",
      "     41                0     4.24005\n",
      "     42                0     4.20727\n",
      "     43                0     4.20134\n",
      "     44                0     4.18422\n",
      "     45                0     4.17529\n",
      "     46                0     4.16786\n",
      "     47                0     4.16866\n",
      "     48                0     4.1458\n",
      "     49                0     4.13627\n",
      "     50                0     4.12785\n",
      "     51                0     4.12883\n",
      "     52                0     4.10335\n",
      "     53                0     4.11381\n",
      "     54                0     4.08613\n",
      "     55                0     4.10285\n",
      "     56                0     4.08266\n",
      "     57                0     4.08009\n",
      "     58                0     4.07873\n",
      "     59                0     4.08494\n",
      "     60                0     4.07142\n",
      "     61                0     4.06995\n",
      "     62                0     4.07212\n",
      "     63                0     4.06366\n",
      "     64                0     4.05879\n",
      "     65                0     4.05531\n",
      "     66                0     4.04853\n",
      "     67                0     4.05842\n",
      "     68                0     4.05188\n",
      "     69                0     4.04428\n",
      "     70                0     4.0252\n",
      "     71                0     4.02789\n",
      "     72                0     4.03199\n",
      "     73                0     4.04147\n",
      "     74                0     4.04215\n",
      "     75                0     4.03527\n",
      "     76                0     4.02748\n",
      "     77                0     4.0217\n",
      "     78                0     4.03178\n",
      "     79                0     4.01874\n",
      "     80                0     4.03227\n",
      "     81                0     4.02237\n",
      "     82                0     4.02297\n",
      "     83                0     4.01918\n",
      "     84                0     4.01389\n",
      "     85                0     4.01599\n",
      "     86                0     4.01914\n",
      "     87                0     4.0159\n",
      "     88                0     4.01512\n",
      "     89                0     4.0067\n",
      "     90                0     4.00978\n",
      "     91                0     4.01849\n",
      "     92                0     4.0026\n",
      "     93                0     4.00247\n",
      "     94                0     4.00583\n",
      "     95                0     4.00591\n",
      "     96                0     4.00287\n",
      "     97                0     4.00157\n",
      "     98                0     4.01961\n",
      "     99                0     4.00741\n",
      "    100                0     4.00656\n",
      "    101                0     3.99359\n",
      "    102                0     3.99321\n",
      "    103                0     4.01048\n",
      "    104                0     3.98946\n",
      "    105                0     3.99331\n",
      "    106                0     3.98603\n",
      "    107                0     4.01059\n",
      "    108                0     3.99071\n",
      "    109                0     3.99207\n",
      "    110                0     4.00559\n",
      "    111                0     4.00471\n",
      "    112                0     3.9891\n",
      "    113                0     4.00456\n",
      "    114                0     4.00269\n",
      "    115                0     4.00445\n",
      "    116                0     3.99896\n",
      "    117                0     3.99821\n",
      "    118                0     3.99936\n",
      "    119                0     4.00789\n",
      "    120                0     3.99461\n",
      "    121                0     4.00531\n",
      "    122                0     3.99862\n",
      "    123                0     3.98748\n",
      "    124                0     3.99746\n",
      "    125                0     3.99949\n",
      "    126                0     3.99933\n",
      "    127                0     3.99414\n",
      "    128                0     3.99834\n",
      "    129                0     3.98134\n",
      "    130                0     3.99739\n",
      "    131                0     3.99248\n",
      "    132                0     3.98716\n",
      "    133                0     3.99181\n",
      "    134                0     3.99146\n",
      "    135                0     4.01283\n",
      "    136                0     3.98742\n",
      "    137                0     4.0017\n",
      "    138                0     3.99198\n",
      "    139                0     3.98897\n",
      "    140                0     3.99559\n",
      "    141                0     3.99855\n",
      "    142                0     3.97898\n",
      "    143                0     4.00264\n",
      "    144                0     3.98482\n",
      "    145                0     3.98935\n",
      "    146                0     3.99457\n",
      "    147                0     3.99311\n",
      "    148                0     3.99809\n",
      "    149                0     3.98797\n",
      "    150                0     3.9927\n",
      "    151                0     3.98113\n",
      "    152                0     3.99715\n",
      "    153                0     3.98662\n",
      "    154                0     3.99732\n",
      "    155                0     3.97313\n",
      "    156                0     3.99284\n",
      "    157                0     4.00363\n",
      "    158                0     3.99158\n",
      "    159                0     3.98619\n",
      "    160                0     3.98967\n",
      "    161                0     3.98119\n",
      "    162                0     3.9947\n",
      "    163                0     3.99967\n",
      "    164                0     3.99221\n",
      "    165                0     3.98731\n",
      "    166                0     4.00265\n",
      "    167                0     3.98752\n",
      "    168                0     3.98431\n",
      "    169                0     4.00379\n",
      "    170                0     3.98676\n",
      "    171                0     4.00282\n",
      "    172                0     4.00085\n",
      "    173                0     3.99531\n",
      "    174                0     3.99678\n",
      "    175                0     3.99588\n",
      "    176                0     3.98912\n",
      "    177                0     3.99001\n",
      "    178                0     3.99451\n",
      "    179                0     3.99637\n",
      "    180                0     3.98565\n",
      "    181                0     3.98782\n",
      "    182                0     4.00116\n",
      "    183                0     3.99002\n",
      "    184                0     3.98925\n",
      "    185                0     3.99523\n",
      "    186                0     3.99038\n",
      "    187                0     3.99557\n",
      "    188                0     3.98465\n",
      "    189                0     3.99074\n",
      "    190                0     3.98331\n",
      "    191                0     3.99965\n",
      "    192                0     4.00518\n",
      "    193                0     3.98418\n",
      "    194                0     3.9916\n",
      "    195                0     3.97436\n",
      "    196                0     3.99302\n",
      "    197                0     3.98839\n",
      "    198                0     3.98668\n",
      "    199                0     3.98831\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized  viterbi_cer\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------  -------------\n",
      "      0    39.6313       4365.83                 39.4246         75.5938                  3.28551\n",
      "     10    12.1147        134.328                 0.98916        26.0625                  1\n",
      "     20     6.74879       134.688                 0.990994       26.0625                  1\n",
      "     30     5.60439       134.812                 0.991775       26.0625                  1\n",
      "     40     5.22908       134.875                 0.992234       26.0625                  1\n",
      "     50     5.06194       134.891                 0.99229        26.0625                  1\n",
      "     60     4.97507       134.953                 0.992886       26.0625                  1\n",
      "     70     4.92443       134.969                 0.993034       26.0625                  1\n",
      "     80     4.89371       134.984                 0.993183       26.0625                  1\n",
      "     90     4.87454       134.984                 0.993183       26.0625                  1\n",
      "    100     4.86219       134.984                 0.993183       26.0625                  1\n",
      "    110     4.85418       134.984                 0.993183       26.0625                  1\n",
      "    120     4.84895       134.984                 0.993183       26.0625                  1\n",
      "    130     4.8455        134.984                 0.993183       26.0625                  1\n",
      "    140     4.84319       135                     0.993427       26.0625                  1\n",
      "    150     4.84166       135                     0.993427       26.0625                  1\n",
      "    160     4.84068       135                     0.993427       26.0625                  1\n",
      "    170     4.84018       135                     0.993427       26.0625                  1\n",
      "    180     4.83998       135                     0.993427       26.0625                  1\n",
      "    190     4.83992       135                     0.993427       26.0625                  1\n",
      "    199     4.8399        135                     0.993427       26.0625                  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  627864 GB |  627864 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  627407 GB |  627407 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     457 GB |     457 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  627864 GB |  627864 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  627407 GB |  627407 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     457 GB |     457 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  514512 KB |    6439 MB |     933 TB |     933 TB |\n",
      "|       from large pool |  513014 KB |    6437 MB |     933 TB |     933 TB |\n",
      "|       from small pool |    1498 KB |       3 MB |       0 TB |       0 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    9693 K  |    9693 K  |\n",
      "|       from large pool |      12    |      26    |    4552 K  |    4552 K  |\n",
      "|       from small pool |      20    |      30    |    5140 K  |    5140 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    9693 K  |    9693 K  |\n",
      "|       from large pool |      13    |      26    |    4552 K  |    4552 K  |\n",
      "|       from small pool |      20    |      30    |    5140 K  |    5140 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    4478 K  |    4477 K  |\n",
      "|       from large pool |      11    |      20    |    2147 K  |    2147 K  |\n",
      "|       from small pool |       5    |       8    |    2330 K  |    2330 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(history_training, headers=\"keys\"), flush=True)\n",
    "print(tabulate(history_validation, headers=\"keys\"), flush=True)\n",
    "print(torch.cuda.memory_summary(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (21,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8d8bbc4cea8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m          history_validation[\"greedy_cer\"], label=\"greedy\")\n\u001b[1;32m      3\u001b[0m plt.plot(history_validation[\"epoch\"],\n\u001b[0;32m----> 4\u001b[0;31m          history_validation[\"viterbi_cer\"], label=\"viterbi\")\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio-built/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m     return gca().plot(\n\u001b[1;32m   2795\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2796\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio-built/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio-built/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio-built/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/audio-built/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (21,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"greedy_cer\"], label=\"greedy\")\n",
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"viterbi_cer\"], label=\"viterbi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"greedy_wer\"], label=\"greedy\")\n",
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"viterbi_wer\"], label=\"viterbi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"greedy_cer_normalized\"], label=\"greedy\")\n",
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"viterbi_cer_normalized\"], label=\"viterbi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"greedy_wer_normalized\"], label=\"greedy\")\n",
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"viterbi_wer_normalized\"], label=\"viterbi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc570696bd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_training[\"epoch\"],\n",
    "         history_training[\"sum_loss\"], label=\"training\")\n",
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"sum_loss\"], label=\"validation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc6880c9590>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_training[\"epoch\"],\n",
    "         history_training[\"sum_loss\"], label=\"training\")\n",
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"sum_loss\"], label=\"validation\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  102506 KB |    7892 MB |  627864 GB |  627864 GB |\n",
      "|       from large pool |  101956 KB |    7891 MB |  627407 GB |  627407 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     457 GB |     457 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  110128 KB |    7892 MB |  627864 GB |  627864 GB |\n",
      "|       from large pool |  109578 KB |    7891 MB |  627407 GB |  627407 GB |\n",
      "|       from small pool |     550 KB |       2 MB |     457 GB |     457 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   10780 MB |   11784 MB |   18982 MB |    8202 MB |\n",
      "|       from large pool |   10776 MB |   11780 MB |   18976 MB |    8200 MB |\n",
      "|       from small pool |       4 MB |       4 MB |       6 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  514512 KB |    6439 MB |     933 TB |     933 TB |\n",
      "|       from large pool |  513014 KB |    6437 MB |     933 TB |     933 TB |\n",
      "|       from small pool |    1498 KB |       3 MB |       0 TB |       0 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      32    |      53    |    9693 K  |    9693 K  |\n",
      "|       from large pool |      12    |      26    |    4552 K  |    4552 K  |\n",
      "|       from small pool |      20    |      30    |    5140 K  |    5140 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      33    |      53    |    9693 K  |    9693 K  |\n",
      "|       from large pool |      13    |      26    |    4552 K  |    4552 K  |\n",
      "|       from small pool |      20    |      30    |    5140 K  |    5140 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      21    |      25    |      10    |\n",
      "|       from large pool |      13    |      19    |      22    |       9    |\n",
      "|       from small pool |       2    |       2    |       3    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      25    |    4478 K  |    4477 K  |\n",
      "|       from large pool |      11    |      20    |    2147 K  |    2147 K  |\n",
      "|       from small pool |       5    |       8    |    2330 K  |    2330 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         434272777 function calls (429946959 primitive calls) in 57949.714 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 3109 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       37    0.001    0.000 57949.623 1566.206 base_events.py:1686(_run_once)\n",
      "   155/29    0.001    0.000 42534.115 1466.694 gen.py:716(run)\n",
      "   254/80    0.000    0.000 42534.115  531.676 {method 'send' of 'generator' objects}\n",
      "   197/65    0.002    0.000 42534.109  654.371 gen.py:184(wrapper)\n",
      "      165    0.000    0.000 42534.105  257.782 kernelbase.py:347(process_one)\n",
      "       98    0.002    0.000 42534.102  434.021 kernelbase.py:225(dispatch_shell)\n",
      "128857/157    0.139    0.000 42534.059  270.918 {built-in method builtins.next}\n",
      "      436    0.000    0.000 42534.031   97.555 events.py:86(_run)\n",
      "      436    0.000    0.000 42534.031   97.555 {method 'run' of 'Context' objects}\n",
      "      287    0.000    0.000 42534.029  148.202 ioloop.py:735(_run_callback)\n",
      "       23    0.000    0.000 42534.022 1849.305 ioloop.py:690(<lambda>)\n",
      "       23    0.000    0.000 42534.022 1849.305 gen.py:784(inner)\n",
      "       87    0.001    0.000 42534.013  488.897 kernelbase.py:512(execute_request)\n",
      "       43    0.001    0.000 42533.915  989.161 ipkernel.py:262(do_execute)\n",
      "       43    0.000    0.000 42533.815  989.158 zmqshell.py:534(run_cell)\n",
      "       43    0.000    0.000 42533.815  989.158 interactiveshell.py:2831(run_cell)\n",
      "       43    0.000    0.000 42532.646  989.131 interactiveshell.py:2865(_run_cell)\n",
      "       43    0.000    0.000 42532.556  989.129 async_helpers.py:58(_pseudo_sync_runner)\n",
      "       43    0.001    0.000 42532.556  989.129 {method 'send' of 'coroutine' objects}\n",
      "       43    0.002    0.000 42532.555  989.129 interactiveshell.py:2920(run_cell_async)\n",
      "\n",
      "\n",
      "\n",
      "stop time: 2020-04-17 08:13:05.520756\n"
     ]
    }
   ],
   "source": [
    "# Print performance\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "ps = (\n",
    "    pstats\n",
    "    .Stats(pr, stream=s)\n",
    "    .strip_dirs()\n",
    "    .sort_stats(\"cumtime\")\n",
    "    .print_stats(20)\n",
    ")\n",
    "print(s.getvalue(), flush=True)\n",
    "print(\"stop time: {}\".format(str(datetime.now())), flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
