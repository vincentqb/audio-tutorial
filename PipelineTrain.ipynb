{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import cProfile\n",
    "import hashlib\n",
    "import itertools\n",
    "import os\n",
    "import pstats\n",
    "import string\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn, topk\n",
    "from torch.optim import Adadelta, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import SPEECHCOMMANDS, LIBRISPEECH\n",
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200302.145135\n",
      "2 GPUs\n"
     ]
    }
   ],
   "source": [
    "audio_backend = \"soundfile\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "non_blocking = pin_memory\n",
    "\n",
    "excluded_dir = [\"_background_noise_\"]\n",
    "folder_speechcommands = './SpeechCommands/speech_commands_v0.02'\n",
    "\n",
    "char_null = \"-\"\n",
    "char_pad = \" \"\n",
    "# labels = [char_null, char_pad] + [d for d in next(os.walk(folder_speechcommands))[1] if d not in excluded_dir]\n",
    "labels = [char_null + char_pad + string.ascii_lowercase]\n",
    "\n",
    "# vocab_size = len(labels) + 2\n",
    "shuffle = True\n",
    "drop_last = True\n",
    "\n",
    "# audio, self.sr, window_stride=(160, 80), fft_size=512, num_filt=20, num_coeffs=13\n",
    "n_mfcc = 13\n",
    "melkwargs = {\n",
    "    'n_fft': 512,\n",
    "    'n_mels': 20,\n",
    "    'hop_length': 80,\n",
    "}\n",
    "sample_rate = 16000\n",
    "\n",
    "# max number of sentences per batch\n",
    "batch_size = 2048\n",
    "# batch_size = 32\n",
    "optimizer_params = {\n",
    "    \"lr\": 1.0,\n",
    "    \"eps\": 1e-8,\n",
    "    # \"rho\": 0.95,\n",
    "    \"weight_decay\": .1,\n",
    "}\n",
    "\n",
    "# hidden_size = 128\n",
    "# num_layers = 3\n",
    "hidden_size = 8\n",
    "num_layers = 1\n",
    "\n",
    "max_epoch = 200\n",
    "mod_epoch = 10\n",
    "clip_norm = 0.  # 10.\n",
    "zero_infinity = False\n",
    "\n",
    "training_percentage = 80.\n",
    "validation_percentage = 10.\n",
    "MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "print_length = 20\n",
    "\n",
    "dtstamp = datetime.now().strftime(\"%y%m%d.%H%M%S\")\n",
    "print(dtstamp)\n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "print(num_devices, \"GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Profiling performance\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "torchaudio.set_audio_backend(audio_backend)\n",
    "mfcc = MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc, melkwargs=melkwargs).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coder:\n",
    "    def __init__(self, labels):\n",
    "        labels = list(collections.OrderedDict.fromkeys(list(\"\".join(labels))))\n",
    "        self.length = len(labels)\n",
    "        enumerated = list(enumerate(labels))\n",
    "        flipped = [(sub[1], sub[0]) for sub in enumerated]\n",
    "\n",
    "        d1 = collections.OrderedDict(enumerated)\n",
    "        d2 = collections.OrderedDict(flipped)\n",
    "        self.mapping = {**d1, **d2}\n",
    "\n",
    "    def _map(self, iterable):\n",
    "        # iterable to iterable\n",
    "        return [self.mapping[i] for i in iterable]\n",
    "\n",
    "    def encode(self, iterable):\n",
    "        if isinstance(iterable[0], list):\n",
    "            return [self.encode(i) for i in iterable]\n",
    "        else:\n",
    "            return self._map(iterable)\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        if isinstance(tensor[0], list):\n",
    "            return [self.decode(t) for t in tensor]\n",
    "        else:\n",
    "            return \"\".join(self._map(tensor))\n",
    "\n",
    "\n",
    "coder = Coder(labels)\n",
    "encode = coder.encode\n",
    "decode = coder.decode\n",
    "vocab_size = coder.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMemoryCache:\n",
    "    def __init__(self, iterable):\n",
    "        self.iterable = iterable\n",
    "        self.iter = iter(iterable)\n",
    "        self.done = False\n",
    "        self.vals = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.done:\n",
    "            return iter(self.vals)\n",
    "        # chain vals so far & then gen the rest\n",
    "        return itertools.chain(self.vals, self._gen_iter())\n",
    "\n",
    "    def _gen_iter(self):\n",
    "        # gen new vals, appending as it goes\n",
    "        for new_val in self.iter:\n",
    "            self.vals.append(new_val)\n",
    "            yield new_val\n",
    "        self.done = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.iterable)\n",
    "\n",
    "\n",
    "class MapMemoryCache(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a dataset so that, whenever a new item is returned, it is saved to memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self._id = id(self)\n",
    "        self._cache = [None] * len(dataset)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        if self._cache[n]:\n",
    "            return self._cache[n]\n",
    "\n",
    "        item = self.dataset[n]\n",
    "        self._cache[n] = item\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "def process_datapoint(item):\n",
    "    transformed = item[0].to(device, non_blocking=non_blocking)\n",
    "    target = item[2].lower()\n",
    "    # target = \"\".join(filter(str.isalnum, target))\n",
    "    target = \"\".join(c for c in target if c.isalnum() or c == char_pad)\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    transformed = mfcc(transformed)\n",
    "    transformed = transformed[0, ...].transpose(0, -1)\n",
    "    # transformed = transformed.view(-1, 1)\n",
    "    target = encode(target)\n",
    "    target = torch.tensor(target, dtype=torch.long, device=transformed.device)\n",
    "    return transformed, target\n",
    "\n",
    "\n",
    "class PROCESSED_LIBRISPEECH(LIBRISPEECH):\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        try:\n",
    "            item = super().__getitem__(n)\n",
    "            return process_datapoint(item)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return None\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            item = super().__next__()\n",
    "            return process_datapoint(item)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return self.__next__()\n",
    "\n",
    "\n",
    "def datasets():\n",
    "    root = \"./\"\n",
    "\n",
    "    training = PROCESSED_LIBRISPEECH(root, url=\"train-clean-100\", download=True)\n",
    "    training = MapMemoryCache(training)\n",
    "    validation = PROCESSED_LIBRISPEECH(root, url=\"dev-clean\", download=True)\n",
    "    validation = MapMemoryCache(validation)\n",
    "\n",
    "    return training, validation, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "\n",
    "    We want to keep files in the same training, validation, or testing sets even\n",
    "    if new ones are added over time. This makes it less likely that testing\n",
    "    samples will accidentally be reused in training when long runs are restarted\n",
    "    for example. To keep this stability, a hash of the filename is taken and used\n",
    "    to determine which set it should belong to. This determination only depends on\n",
    "    the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "    It's also useful to associate particular files as related (for example words\n",
    "    spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "    Args:\n",
    "        filename: File path of the data sample.\n",
    "        validation_percentage: How much of the data set to use for validation.\n",
    "        testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "    base_name = os.path.basename(filename)\n",
    "    # We want to ignore anything after '_nohash_' in the file name when\n",
    "    # deciding which set to put a wav in, so the data set creator has a way of\n",
    "    # grouping wavs that are close variations of each other.\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode(\"utf-8\")\n",
    "    # This looks a bit magical, but we need to decide whether this file should\n",
    "    # go into the training, testing, or validation sets, and we want to keep\n",
    "    # existing files in the same set even if more files are subsequently\n",
    "    # added.\n",
    "    # To do that, we need a stable way of deciding based on just the file name\n",
    "    # itself, so we do a hash of that and then use that to generate a\n",
    "    # probability value that we use to assign it.\n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1)) * (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "    return result\n",
    "\n",
    "\n",
    "class FILTERED_SPEECHCOMMANDS(SPEECHCOMMANDS):\n",
    "    def __init__(self, tag, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if training_percentage < 100.:\n",
    "            testing_percentage = (100. - training_percentage - validation_percentage)\n",
    "            self._walker = list(filter(lambda x: which_set(x, validation_percentage, testing_percentage) == tag, self._walker))\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "def process_datapoint(item):\n",
    "    transformed = item[0].to(device, non_blocking=non_blocking)\n",
    "    target = item[2]\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    transformed = mfcc(transformed)\n",
    "    transformed = transformed[0, ...].transpose(0, -1)\n",
    "    # transformed = transformed.view(-1, 1)\n",
    "    target = encode(target)\n",
    "    target = torch.tensor(target, dtype=torch.long, device=transformed.device)\n",
    "    return transformed, target\n",
    "\n",
    "\n",
    "class PROCESSED_SPEECHCOMMANDS(FILTERED_SPEECHCOMMANDS):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        item = super().__getitem__(n)\n",
    "        return process_datapoint(item)\n",
    "\n",
    "    def __next__(self):\n",
    "        item = super().__next__()\n",
    "        return process_datapoint(item)\n",
    "\n",
    "\n",
    "def datasets():\n",
    "    root = \"./\"\n",
    "\n",
    "    training = PROCESSED_SPEECHCOMMANDS(\"training\", root, download=True)\n",
    "    training = MapMemoryCache(training)\n",
    "    validation = PROCESSED_SPEECHCOMMANDS(\"validation\", root, download=True)\n",
    "    validation = MapMemoryCache(validation)\n",
    "    testing = PROCESSED_SPEECHCOMMANDS(\"testing\", root, download=True)\n",
    "    testing = MapMemoryCache(testing)\n",
    "\n",
    "    return training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    tensors = [b[0] for b in batch if b]\n",
    "    targets = [b[1] for b in batch if b]\n",
    "\n",
    "    # input_lengths = [t.shape[0] for t in tensors]\n",
    "    # target_lengths = [len(t) for t in targets]\n",
    "\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "\n",
    "    return tensors, targets  # , input_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "[Wav2Letter](https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Wav2Letter(nn.Module):\n",
    "    \"\"\"Wav2Letter Speech Recognition model\n",
    "        https://arxiv.org/pdf/1609.03193.pdf\n",
    "        This specific architecture accepts mfcc or power spectrums speech signals\n",
    "\n",
    "        Args:\n",
    "            num_features (int): number of mfcc features\n",
    "            num_classes (int): number of unique grapheme class labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.layers = nn.Sequential(\n",
    "            # PrintLayer(),\n",
    "            nn.Conv1d(num_features, 250, 48, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv1d(250, 2000, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, 2000, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass through Wav2Letter network than\n",
    "            takes log probability of output\n",
    "        Args:\n",
    "            batch (int): mini batch of data\n",
    "            shape (batch, num_features, frame_len)\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, num_classes, output_len)\n",
    "        \"\"\"\n",
    "        # print(batch.shape)\n",
    "        # y_pred shape (batch_size, num_classes, output_len)\n",
    "        y_pred = self.layers(batch)\n",
    "\n",
    "        # compute log softmax probability on graphemes\n",
    "        log_probs = nn.functional.log_softmax(y_pred, dim=1)\n",
    "        log_probs = log_probs.transpose(1, 2)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        bidirectional = False\n",
    "        if bidirectional:\n",
    "            self.directions = 2\n",
    "        else:\n",
    "            self.directions = 1\n",
    "        # self.layer = nn.GRU(num_features, hidden_size, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        # self.layer = nn.LSTM(num_features, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # https://discuss.pytorch.org/t/lstm-to-bi-lstm/12967\n",
    "        self.layer = nn.LSTM(num_features, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.hidden2class = nn.Linear(self.directions*hidden_size, num_classes)\n",
    "        \n",
    "        # https://discuss.pytorch.org/t/lstm-dataparallel-hidden-state-not-split-between-workers/43630\n",
    "        # https://discuss.pytorch.org/t/when-to-initialize-lstm-hidden-state/2323/2\n",
    "        # https://discuss.pytorch.org/t/dynamic-parameter-declaration-in-forward-function/427\n",
    "        h0, h1 = self.init_hidden()\n",
    "        self.register_parameter('hidden0', torch.nn.Parameter(h0))\n",
    "        self.register_parameter('hidden1', torch.nn.Parameter(h1))\n",
    "        \n",
    "        # self.hidden = None\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(self.directions*num_layers, batch_size//num_devices, hidden_size).cuda(),\n",
    "                torch.zeros(self.directions*num_layers, batch_size//num_devices, hidden_size).cuda())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # https://github.com/pytorch/pytorch/issues/21108\n",
    "        # https://discuss.pytorch.org/t/rnn-module-weights-are-not-part-of-single-contiguous-chunk-of-memory/6011/21\n",
    "        if self.training:\n",
    "            self.layer.flatten_parameters()        \n",
    "\n",
    "        inputs = inputs.transpose(-1, -2).contiguous()\n",
    "        outputs, hidden = self.layer(inputs, (self.hidden0, self.hidden1))\n",
    "        self.hidden0 = torch.nn.Parameter(hidden[0].detach())\n",
    "        self.hidden1 = torch.nn.Parameter(hidden[1].detach())\n",
    "\n",
    "        outputs = self.hidden2class(outputs)\n",
    "\n",
    "        log_probs = nn.functional.log_softmax(outputs, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(outputs):\n",
    "    \"\"\"Greedy Decoder. Returns highest probability of class labels for each timestep\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): shape (input length, batch size, number of classes (including blank))\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: class labels per time step.\n",
    "    \"\"\"\n",
    "    _, indices = topk(outputs, k=1, dim=-1)\n",
    "    return indices[..., 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Wav2Letter(n_mfcc, vocab_size)\n",
    "# model = BiLSTM(1, vocab_size)\n",
    "model = BiLSTM(n_mfcc, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, _ = datasets()\n",
    "loader_training = DataLoader(\n",
    "    training, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle, drop_last=drop_last,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "loader_validation = DataLoader(\n",
    "    validation, batch_size=batch_size, collate_fn=collate_fn, shuffle=False, drop_last=drop_last,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "# model = torch.jit.script(model)\n",
    "\n",
    "model = nn.DataParallel(model, [0,1]) if num_devices > 1 else model\n",
    "\n",
    "model = model.to(device, non_blocking=non_blocking)\n",
    "\n",
    "# optimizer = Adadelta(model.parameters(), **optimizer_params)\n",
    "optimizer = Adam(model.parameters(), **optimizer_params)\n",
    "criterion = torch.nn.CTCLoss(zero_infinity=zero_infinity)\n",
    "\n",
    "best_loss = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_loss(inputs, targets):\n",
    "\n",
    "    inputs = inputs.to(device, non_blocking=non_blocking)\n",
    "    targets = targets.to(device, non_blocking=non_blocking)\n",
    "    outputs = model(inputs).transpose(0, 1)\n",
    "\n",
    "    this_batch_size = outputs.shape[1]\n",
    "    input_lengths = torch.full(\n",
    "        (this_batch_size,), outputs.shape[0], dtype=torch.long, device=outputs.device\n",
    "    )\n",
    "    target_lengths = torch.tensor(\n",
    "        [target.shape[0] for target in targets], dtype=torch.long, device=targets.device\n",
    "    )\n",
    "    \n",
    "    # CTC\n",
    "    # https://pytorch.org/docs/master/nn.html#torch.nn.CTCLoss\n",
    "    # https://discuss.pytorch.org/t/ctcloss-with-warp-ctc-help/8788/3\n",
    "    \n",
    "    # outputs: input length, batch size, number of classes (including blank)\n",
    "    # targets: batch size, max target length\n",
    "    # input_lengths: batch size\n",
    "    # target_lengths: batch size\n",
    "\n",
    "    return criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "\n",
    "def forward_and_decode(inputs, targets):\n",
    "    output = model(inputs).transpose(0, 1)\n",
    "    output = output[:, 0, :]\n",
    "    output = greedy_decoder(output)\n",
    "    output = decode(output.tolist())\n",
    "    target = decode(targets.tolist()[0])\n",
    "\n",
    "    output = output.ljust(print_length)[:print_length]\n",
    "    target = target.ljust(print_length)[:print_length]\n",
    "    return f\"Epoch: {epoch:4}   Target: {target}   Output: {output}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c3255cb21245ddb16af4c56cc20455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0   Target: forward-               Output: aoiiioooooooaaaooooo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0   Target: nine----               Output: oooooooooooooooooooo\n",
      "Epoch:    0   Train: 121.94335   Validation: 157.29672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   10   Target: house---               Output: --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   10   Target: nine----               Output: --------------------\n",
      "Epoch:   10   Train: 126.50078   Validation: 161.94757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   20   Target: five----               Output: --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   20   Target: nine----               Output: --------------------\n",
      "Epoch:   20   Train: 126.50078   Validation: 161.94757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   30   Target: eight---               Output: --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   30   Target: nine----               Output: --------------------\n",
      "Epoch:   30   Train: 126.50079   Validation: 161.94757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   40   Target: three---               Output: ng------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   40   Target: nine----               Output: -------hgnnnnrrrrrrr\n",
      "Epoch:   40   Train: 123.53684   Validation: 155.91153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   50   Target: three---               Output: ooooooooooooooooossr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   50   Target: nine----               Output: iiii--neeeeeeeeeeeee\n",
      "Epoch:   50   Train: 123.82193   Validation: 156.99923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   60   Target: cat-----               Output: tt------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   60   Target: nine----               Output: --------------------\n",
      "Epoch:   60   Train: 123.18832   Validation: 156.68481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   70   Target: left----               Output: eeeeerrrs-fnnnrrrree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   70   Target: nine----               Output: ii-----------iiiiii-\n",
      "Epoch:   70   Train: 123.58428   Validation: 157.39418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   80   Target: wow-----               Output: xttevhhf------ff----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   80   Target: nine----               Output: hhhaaaaaaaaaaaaaaaaa\n",
      "Epoch:   80   Train: 123.81069   Validation: 158.09318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   90   Target: four----               Output: n----------deeeed---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   90   Target: nine----               Output: wd------------------\n",
      "Epoch:   90   Train: 123.60199   Validation: 157.99268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  100   Target: left----               Output: eooooooooooooooooooo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  100   Target: nine----               Output: hhiioooooooooooooooo\n",
      "Epoch:  100   Train: 123.69879   Validation: 156.63168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  110   Target: seven---               Output: -ddddggnnnnnnnnnnnii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  110   Target: nine----               Output: ---nnnnnnnn---------\n",
      "Epoch:  110   Train: 123.75424   Validation: 160.39369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  120   Target: bed-----               Output: ox-------xxxxxx-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  120   Target: nine----               Output: stttttpppppppppppppp\n",
      "Epoch:  120   Train: 123.48081   Validation: 157.49075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  130   Target: seven---               Output: iiyhhhhhhhhhhhhhhhhh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  130   Target: nine----               Output: ----------xxhhhhxhhh\n",
      "Epoch:  130   Train: 123.82382   Validation: 157.76252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  140   Target: on------               Output: oooi----wrrrrrrreeew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  140   Target: nine----               Output: hlnnnnnnnnnnn-nn----\n",
      "Epoch:  140   Train: 123.50780   Validation: 155.24287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  150   Target: seven---               Output: eeeeewwwwwwwwwwwwwee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  150   Target: nine----               Output: ffffss--------------\n",
      "Epoch:  150   Train: 123.19481   Validation: 157.48153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  160   Target: marvin--               Output: --eooooooooooooooooo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  160   Target: nine----               Output: vvweeewwwrrrrrrrrrrr\n",
      "Epoch:  160   Train: 123.43975   Validation: 157.77403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  170   Target: stop----               Output: haaa----aaawwwaaaaww\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  170   Target: nine----               Output: -----------www---wrr\n",
      "Epoch:  170   Train: 123.35503   Validation: 157.42846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  180   Target: dog-----               Output: ----rrrrreeeeeeeeeee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  180   Target: nine----               Output: iiiiiiiiieaaaaarrrrr\n",
      "Epoch:  180   Train: 123.26610   Validation: 157.43400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  190   Target: stop----               Output: -seeeeeeeeei--eeeee-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/ATen/native/cudnn/RNN.cpp:1266: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  190   Target: nine----               Output: iooooooir--------nto\n",
      "Epoch:  190   Train: 124.05420   Validation: 156.92091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_loss_training = []\n",
    "sum_loss_validation = []\n",
    "\n",
    "with tqdm(total=max_epoch, unit_scale=1) as pbar:\n",
    "    for epoch in range(max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        sum_loss = 0.\n",
    "        for inputs, targets in loader_training:\n",
    "\n",
    "            loss = forward_and_loss(inputs, targets)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            optimizer.step()\n",
    "            pbar.update(1/len(loader_training))\n",
    "\n",
    "        # Average loss\n",
    "        sum_loss = sum_loss / len(loader_training)\n",
    "        sum_loss_training.append((epoch, sum_loss))\n",
    "        sum_loss_str = f\"Epoch: {epoch:4}   Train: {sum_loss:4.5f}\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            if not epoch % mod_epoch:\n",
    "            \n",
    "                # Switch to evaluation mode\n",
    "                model.eval()\n",
    "        \n",
    "                print(forward_and_decode(inputs, targets))\n",
    "\n",
    "                sum_loss = 0.\n",
    "                for inputs, targets in loader_validation:\n",
    "\n",
    "                    loss = forward_and_loss(inputs, targets)\n",
    "                    sum_loss += loss.item()\n",
    "\n",
    "                # Average loss\n",
    "                sum_loss = sum_loss / len(loader_validation)\n",
    "                sum_loss_validation.append((epoch, sum_loss))\n",
    "                sum_loss_str += f\"   Validation: {sum_loss:.5f}\"\n",
    "\n",
    "                print(forward_and_decode(inputs, targets))\n",
    "\n",
    "                print(sum_loss_str)\n",
    "\n",
    "                if sum_loss < best_loss:\n",
    "                    # Save model\n",
    "                    torch.save(model.state_dict(), f\"./model.{dtstamp}.{epoch}.ph\")\n",
    "                    best_loss = sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(*zip(*sum_loss_training), label=\"training\")\n",
    "plt.plot(*zip(*sum_loss_validation), label=\"validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         196570581 function calls (195533569 primitive calls) in 1210.377 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 2439 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       42    0.000    0.000 1210.091   28.812 interactiveshell.py:3293(run_code)\n",
      "    47/42    0.000    0.000 1210.090   28.812 {built-in method builtins.exec}\n",
      "     41/5    0.000    0.000 1207.339  241.468 gen.py:716(run)\n",
      "    65/18    0.000    0.000 1207.338   67.074 {method 'send' of 'generator' objects}\n",
      "        4    0.000    0.000 1207.337  301.834 base_events.py:1686(_run_once)\n",
      "       81    0.000    0.000 1207.337   14.905 events.py:86(_run)\n",
      "       81    0.000    0.000 1207.337   14.905 {method 'run' of 'Context' objects}\n",
      "       60    0.000    0.000 1207.337   20.122 ioloop.py:735(_run_callback)\n",
      "        3    0.000    0.000 1207.336  402.445 ioloop.py:690(<lambda>)\n",
      "        3    0.000    0.000 1207.336  402.445 gen.py:784(inner)\n",
      "    52/14    0.001    0.000 1207.335   86.238 gen.py:184(wrapper)\n",
      "       26    0.000    0.000 1207.334   46.436 kernelbase.py:225(dispatch_shell)\n",
      "       39    0.000    0.000 1207.334   30.957 kernelbase.py:347(process_one)\n",
      "328271/39    0.390    0.000 1207.320   30.957 {built-in method builtins.next}\n",
      "       26    0.001    0.000 1207.313   46.435 kernelbase.py:512(execute_request)\n",
      "    39/17    0.000    0.000 1207.309   71.018 gen.py:700(__init__)\n",
      "       13    0.000    0.000 1207.305   92.870 kernelbase.py:363(dispatch_queue)\n",
      "       13    0.000    0.000 1207.286   92.868 ipkernel.py:262(do_execute)\n",
      "       13    0.000    0.000 1207.246   92.865 zmqshell.py:534(run_cell)\n",
      "       13    0.000    0.000 1207.246   92.865 interactiveshell.py:2831(run_cell)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), f\"./model.{dtstamp}.{epoch}.ph\")\n",
    "\n",
    "# Print performance\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).strip_dirs().sort_stats(\"cumtime\").print_stats(20)\n",
    "print(s.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
