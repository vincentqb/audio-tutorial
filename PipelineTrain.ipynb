{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import cProfile\n",
    "import hashlib\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pstats\n",
    "import string\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn, topk\n",
    "from torch.optim import Adadelta, Adam, SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import SPEECHCOMMANDS, LIBRISPEECH\n",
    "from torchaudio.transforms import MFCC, Resample\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Empty CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Profiling performance\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GPUs\n",
      "200308.073619\n"
     ]
    }
   ],
   "source": [
    "audio_backend = \"soundfile\"\n",
    "torchaudio.set_audio_backend(audio_backend)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_devices = torch.cuda.device_count()\n",
    "print(num_devices, \"GPUs\")\n",
    "\n",
    "# max number of sentences per batch\n",
    "batch_size = 2048\n",
    "\n",
    "training_percentage = 100.\n",
    "validation_percentage = 0.\n",
    "\n",
    "data_loader_training_params = {\n",
    "    \"num_workers\": 0,\n",
    "    \"pin_memory\": False,\n",
    "    \"shuffle\": True,\n",
    "    \"drop_last\": True,\n",
    "}\n",
    "data_loader_validation_params = data_loader_training_params.copy()\n",
    "data_loader_validation_params[\"shuffle\"] = False\n",
    "\n",
    "non_blocking = data_loader_training_params[\"pin_memory\"]\n",
    "\n",
    "\n",
    "# text preprocessing\n",
    "\n",
    "char_null = \"-\"\n",
    "char_pad = \"*\"\n",
    "\n",
    "labels = [char_null + char_pad + string.ascii_lowercase]\n",
    "\n",
    "# excluded_dir = [\"_background_noise_\"]\n",
    "# folder_speechcommands = './SpeechCommands/speech_commands_v0.02'\n",
    "# labels = [char_null, char_pad] + [d for d in next(os.walk(folder_speechcommands))[1] if d not in excluded_dir]\n",
    "\n",
    "\n",
    "# audio\n",
    "\n",
    "sample_rate_original = 16000\n",
    "sample_rate_new = 1600\n",
    "# resample = Resample(sample_rate_original, sample_rate_new)\n",
    "\n",
    "n_mfcc = 13\n",
    "melkwargs = {\n",
    "    'n_fft': 512,\n",
    "    'n_mels': 20,\n",
    "    'hop_length': 80,  # (160, 80)\n",
    "}\n",
    "mfcc = MFCC(sample_rate=sample_rate_original, n_mfcc=n_mfcc, melkwargs=melkwargs).to(device)\n",
    "# mfcc = None\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "optimizer_params_adadelta = {\n",
    "    \"lr\": 1.0,\n",
    "    \"eps\": 1e-8,\n",
    "    \"rho\": 0.95,\n",
    "    # \"weight_decay\": .01,\n",
    "}\n",
    "\n",
    "optimizer_params_adam = {\n",
    "    \"lr\": .05,\n",
    "    \"eps\": 1e-8,\n",
    "    \"weight_decay\": .01,\n",
    "}\n",
    "\n",
    "optimizer_params_sgd = {\n",
    "    \"lr\": .001,\n",
    "    \"weight_decay\": .0001,\n",
    "}\n",
    "\n",
    "Optimizer = Adadelta\n",
    "optimizer_params = optimizer_params_adadelta\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "lstm_params = {\n",
    "    \"num_layers\": 3,\n",
    "    \"batch_first\": True,\n",
    "    \"bidirectional\": True,\n",
    "    # \"dropout\": 0.1,\n",
    "}\n",
    "\n",
    "clip_norm = 0.  # 10.\n",
    "\n",
    "zero_infinity = False\n",
    "\n",
    "max_epoch = 200\n",
    "mod_epoch = 10\n",
    "\n",
    "dtstamp = datetime.now().strftime(\"%y%m%d.%H%M%S\")\n",
    "print(dtstamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "class Coder:\n",
    "    def __init__(self, labels):\n",
    "        labels = list(collections.OrderedDict.fromkeys(list(\"\".join(labels))))\n",
    "        self.length = len(labels)\n",
    "        enumerated = list(enumerate(labels))\n",
    "        flipped = [(sub[1], sub[0]) for sub in enumerated]\n",
    "\n",
    "        d1 = collections.OrderedDict(enumerated)\n",
    "        d2 = collections.OrderedDict(flipped)\n",
    "        self.mapping = {**d1, **d2}\n",
    "\n",
    "    def _map(self, iterable):\n",
    "        # iterable to iterable\n",
    "        return [self.mapping[i] for i in iterable]\n",
    "\n",
    "    def encode(self, iterable):\n",
    "        if isinstance(iterable[0], list):\n",
    "            return [self.encode(i) for i in iterable]\n",
    "        else:\n",
    "            return self._map(iterable)\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        if isinstance(tensor[0], list):\n",
    "            return [self.decode(t) for t in tensor]\n",
    "        else:\n",
    "            return \"\".join(self._map(tensor))\n",
    "\n",
    "\n",
    "coder = Coder(labels)\n",
    "encode = coder.encode\n",
    "decode = coder.decode\n",
    "vocab_size = coder.length\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class IterableMemoryCache:\n",
    "    def __init__(self, iterable):\n",
    "        self.iterable = iterable\n",
    "        self.iter = iter(iterable)\n",
    "        self.done = False\n",
    "        self.vals = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.done:\n",
    "            return iter(self.vals)\n",
    "        # chain vals so far & then gen the rest\n",
    "        return itertools.chain(self.vals, self._gen_iter())\n",
    "\n",
    "    def _gen_iter(self):\n",
    "        # gen new vals, appending as it goes\n",
    "        for new_val in self.iter:\n",
    "            self.vals.append(new_val)\n",
    "            yield new_val\n",
    "        self.done = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapMemoryCache(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a dataset so that, whenever a new item is returned, it is saved to memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self._id = id(self)\n",
    "        self._cache = [None] * len(dataset)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        if self._cache[n]:\n",
    "            return self._cache[n]\n",
    "\n",
    "        item = self.dataset[n]\n",
    "        self._cache[n] = item\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# @torch.jit.script\n",
    "def process_datapoint(item):\n",
    "    transformed = item[0].to(device, non_blocking=non_blocking)\n",
    "    target = item[2].lower()\n",
    "    # target = \"\".join(filter(str.isalnum, target))\n",
    "    target = \"\".join(c for c in target if c.isalnum() or c == char_pad)\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    transformed = mfcc(transformed)\n",
    "    transformed = transformed[0, ...].transpose(0, -1)\n",
    "    # transformed = transformed.view(-1, 1)\n",
    "    target = encode(target)\n",
    "    target = torch.tensor(target, dtype=torch.long, device=transformed.device)\n",
    "    return transformed, target\n",
    "\n",
    "\n",
    "class PROCESSED_LIBRISPEECH(LIBRISPEECH):\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        try:\n",
    "            item = super().__getitem__(n)\n",
    "            return process_datapoint(item)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return None\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            item = super().__next__()\n",
    "            return process_datapoint(item)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return self.__next__()\n",
    "\n",
    "\n",
    "def datasets():\n",
    "    root = \"./\"\n",
    "\n",
    "    training = PROCESSED_LIBRISPEECH(root, url=\"train-clean-100\", download=True)\n",
    "    training = MapMemoryCache(training)\n",
    "    validation = PROCESSED_LIBRISPEECH(root, url=\"dev-clean\", download=True)\n",
    "    validation = MapMemoryCache(validation)\n",
    "\n",
    "    return training, validation, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "\n",
    "    We want to keep files in the same training, validation, or testing sets even\n",
    "    if new ones are added over time. This makes it less likely that testing\n",
    "    samples will accidentally be reused in training when long runs are restarted\n",
    "    for example. To keep this stability, a hash of the filename is taken and used\n",
    "    to determine which set it should belong to. This determination only depends on\n",
    "    the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "    It's also useful to associate particular files as related (for example words\n",
    "    spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "    Args:\n",
    "        filename: File path of the data sample.\n",
    "        validation_percentage: How much of the data set to use for validation.\n",
    "        testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "    \n",
    "    MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "    base_name = os.path.basename(filename)\n",
    "\n",
    "    # We want to ignore anything after '_nohash_' in the file name when\n",
    "    # deciding which set to put a wav in, so the data set creator has a way of\n",
    "    # grouping wavs that are close variations of each other.\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode(\"utf-8\")\n",
    "    \n",
    "    # This looks a bit magical, but we need to decide whether this file should\n",
    "    # go into the training, testing, or validation sets, and we want to keep\n",
    "    # existing files in the same set even if more files are subsequently\n",
    "    # added.\n",
    "    # To do that, we need a stable way of deciding based on just the file name\n",
    "    # itself, so we do a hash of that and then use that to generate a\n",
    "    # probability value that we use to assign it.\n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) % (MAX_NUM_WAVS_PER_CLASS + 1)) * (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "    \n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class FILTERED_SPEECHCOMMANDS(SPEECHCOMMANDS):\n",
    "    def __init__(self, tag, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if training_percentage < 100.:\n",
    "            testing_percentage = (100. - training_percentage - validation_percentage)\n",
    "            which_set = lambda x: which_set(x, validation_percentage, testing_percentage) == tag\n",
    "            self._walker = list(filter(which_set, self._walker))\n",
    "\n",
    "\n",
    "# @torch.jit.script\n",
    "def process_datapoint(item):\n",
    "    transformed = item[0].to(device, non_blocking=non_blocking)\n",
    "    target = item[2]\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    if mfcc is not None:\n",
    "        transformed = mfcc(transformed)\n",
    "    else:\n",
    "        transformed.unsqueeze(1)\n",
    "        transformed = resample(transformed)\n",
    "    transformed = transformed[0, ...].transpose(0, -1)\n",
    "    # transformed = transformed.view(-1, 1)\n",
    "    # print(target)\n",
    "    target = encode(target)\n",
    "    target = torch.tensor(target, dtype=torch.long, device=transformed.device)\n",
    "    # print(target)\n",
    "    return transformed, target\n",
    "\n",
    "\n",
    "class PROCESSED_SPEECHCOMMANDS(FILTERED_SPEECHCOMMANDS):\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        item = super().__getitem__(n)\n",
    "        return process_datapoint(item)\n",
    "\n",
    "    def __next__(self):\n",
    "        item = super().__next__()\n",
    "        return process_datapoint(item)\n",
    "\n",
    "\n",
    "def datasets():\n",
    "    root = \"./\"\n",
    "\n",
    "    training = PROCESSED_SPEECHCOMMANDS(\"training\", root, download=True)\n",
    "    training = MapMemoryCache(training)\n",
    "    validation = PROCESSED_SPEECHCOMMANDS(\"validation\", root, download=True)\n",
    "    validation = MapMemoryCache(validation)\n",
    "    testing = PROCESSED_SPEECHCOMMANDS(\"testing\", root, download=True)\n",
    "    testing = MapMemoryCache(testing)\n",
    "\n",
    "    return training, validation, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, _ = datasets()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "training_unprocessed = FILTERED_SPEECHCOMMANDS(\"training\", \"./\", download=True)\n",
    "\n",
    "counter = Counter([t[2] for t in training_unprocessed])\n",
    "\n",
    "plt.bar(counters.keys(), counter.values(), align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    tensors = [b[0] for b in batch if b]\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "\n",
    "    targets = [b[1] for b in batch if b]\n",
    "    target_lengths = torch.tensor(\n",
    "        [target.shape[0] for target in targets], dtype=torch.long, device=tensors.device\n",
    "    )\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "    # print(targets.shape)\n",
    "    # print(decode(targets.tolist()))\n",
    "    \n",
    "    return tensors, targets, target_lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "[Wav2Letter](https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m): \n",
    "    if isinstance(m, nn.Linear):\n",
    "        size = m.weight.size()\n",
    "        fan_out = size[0] # number of rows\n",
    "        fan_in = size[1] # number of columns\n",
    "        variance = math.sqrt(2.0/(fan_in + fan_out))\n",
    "        m.weight.data.normal_(0.0, variance)\n",
    "\n",
    "\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Wav2Letter(nn.Module):\n",
    "    \"\"\"Wav2Letter Speech Recognition model\n",
    "        https://arxiv.org/pdf/1609.03193.pdf\n",
    "        This specific architecture accepts mfcc or power spectrums speech signals\n",
    "\n",
    "        Args:\n",
    "            num_features (int): number of mfcc features\n",
    "            num_classes (int): number of unique grapheme class labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(num_features, 250, 48, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv1d(250, 2000, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, 2000, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass through Wav2Letter network than\n",
    "            takes log probability of output\n",
    "        Args:\n",
    "            batch (int): mini batch of data\n",
    "            shape (batch, num_features, frame_len)\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, num_classes, output_len)\n",
    "        \"\"\"\n",
    "        # batch: (batch_size, num_features, seq_len)\n",
    "        y_pred = self.layers(batch)\n",
    "        # y_pred: (batch_size, num_classes, output_len)\n",
    "        y_pred = y_pred.transpose(-1, -2)\n",
    "        # y_pred: (batch_size, output_len, num_classes)\n",
    "        return nn.functional.log_softmax(y_pred, dim=-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, lstm_params):\n",
    "        super().__init__()\n",
    "        \n",
    "        bidirectional = lstm_params[\"bidirectional\"]\n",
    "        \n",
    "        if bidirectional:\n",
    "            directions = 2\n",
    "        else:\n",
    "            directions = 1\n",
    "\n",
    "        self.layer = nn.LSTM(num_features, hidden_size=num_classes, **lstm_params)\n",
    "        self.hidden2class = nn.Linear(directions*num_classes, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        self.layer.flatten_parameters()\n",
    "        # batch: batch, num_features, seq_len\n",
    "        batch = batch.transpose(-1, -2).contiguous()\n",
    "        # batch: batch, seq_len, num_features\n",
    "        outputs, _ = self.layer(batch)\n",
    "        # outputs: batch, seq_len, directions*num_features\n",
    "        outputs = self.hidden2class(outputs)\n",
    "        # outputs: batch, seq_len, num_features\n",
    "        return nn.functional.log_softmax(outputs, dim=-1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(outputs):\n",
    "    \"\"\"Greedy Decoder. Returns highest probability of class labels for each timestep\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): shape (input length, batch size, number of classes (including blank))\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: class labels per time step.\n",
    "    \"\"\"\n",
    "    _, indices = topk(outputs, k=1, dim=-1)\n",
    "    return indices[..., 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "loader_training = DataLoader(\n",
    "    training, batch_size=batch_size, collate_fn=collate_fn, **data_loader_training_params\n",
    ")\n",
    "\n",
    "loader_validation = DataLoader(\n",
    "    validation, batch_size=batch_size, collate_fn=collate_fn, **data_loader_validation_params\n",
    ")\n",
    "\n",
    "num_features = next(iter(loader_training))[0].shape[1]\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Wav2Letter(num_features, vocab_size)\n",
    "model = LSTMModel(num_features, vocab_size, lstm_params)\n",
    "\n",
    "# model = torch.jit.script(model)\n",
    "# model = nn.DataParallel(model, [0,1]) if num_devices > 1 else model\n",
    "model = nn.DataParallel(model) if num_devices > 1 else model\n",
    "model = model.to(device, non_blocking=non_blocking)\n",
    "# model.apply(weight_init)\n",
    "\n",
    "optimizer = Optimizer(model.parameters(), **optimizer_params)\n",
    "scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "# scheduler = ReduceLROnPlateau(optimizer)\n",
    "\n",
    "criterion = torch.nn.CTCLoss(zero_infinity=zero_infinity)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = torch.nn.NLLLoss()\n",
    "\n",
    "best_loss = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_loss(inputs, targets, target_lengths):\n",
    "\n",
    "    inputs = inputs.to(device, non_blocking=non_blocking)\n",
    "    targets = targets.to(device, non_blocking=non_blocking)\n",
    "    \n",
    "    # keep batch first for data parallel\n",
    "    outputs = model(inputs).transpose(0, 1)\n",
    "\n",
    "    this_batch_size = outputs.shape[1]\n",
    "    seq_len = outputs.shape[0]\n",
    "    input_lengths = torch.full((this_batch_size,), seq_len, dtype=torch.long, device=outputs.device)\n",
    "    \n",
    "    # CTC    \n",
    "    # outputs: input length, batch size, number of classes (including blank)\n",
    "    # targets: batch size, max target length\n",
    "    # input_lengths: batch size\n",
    "    # target_lengths: batch size\n",
    "\n",
    "    return criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "\n",
    "def forward_and_decode(inputs, targets):\n",
    "    output = model(inputs)\n",
    "    output = output.transpose(0, 1)\n",
    "    output = output[:, 0, :]\n",
    "    output = greedy_decoder(output)\n",
    "    output = decode(output.tolist())\n",
    "    target = decode(targets.tolist()[0])\n",
    "\n",
    "    print_length = 20\n",
    "    output = output.ljust(print_length)[:print_length]\n",
    "    target = target.ljust(print_length)[:print_length]\n",
    "\n",
    "    return f\"Epoch: {epoch:4}   Target: {target}   Output: {output}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e85cb9eb9c442f79626755cb4c3af89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    0   Target: one-----               Output: iiiiiiiiiiiiiiiiiiii\n",
      "Epoch:    0   Target: go                     Output: iiiiiiiiiiiiiiiiiiii\n",
      "Epoch:    0   Train: 177.84086   Validation: 167.90147\n",
      "Epoch:   10   Target: visual--               Output: --------------------\n",
      "Epoch:   10   Target: go                     Output: --------------------\n",
      "Epoch:   10   Train: 5.84394   Validation: 5.50079\n",
      "Epoch:   20   Target: happy---               Output: --------------------\n",
      "Epoch:   20   Target: go                     Output: --------------------\n",
      "Epoch:   20   Train: 3.61143   Validation: 3.58275\n",
      "Epoch:   30   Target: four----               Output: --------------------\n",
      "Epoch:   30   Target: go                     Output: --------------------\n",
      "Epoch:   30   Train: 3.37425   Validation: 3.36859\n",
      "Epoch:   40   Target: zero----               Output: --------------------\n",
      "Epoch:   40   Target: go                     Output: --------------------\n",
      "Epoch:   40   Train: 3.30349   Validation: 3.30018\n",
      "Epoch:   50   Target: no------               Output: --------------------\n",
      "Epoch:   50   Target: go                     Output: --------------------\n",
      "Epoch:   50   Train: 3.26424   Validation: 3.26178\n",
      "Epoch:   60   Target: six-----               Output: --------------------\n",
      "Epoch:   60   Target: go                     Output: --------------------\n",
      "Epoch:   60   Train: 3.23627   Validation: 3.23410\n",
      "Epoch:   70   Target: yes-----               Output: --------------------\n",
      "Epoch:   70   Target: go                     Output: --------------------\n",
      "Epoch:   70   Train: 3.21459   Validation: 3.21238\n",
      "Epoch:   80   Target: up------               Output: --------------------\n",
      "Epoch:   80   Target: go                     Output: --------------------\n",
      "Epoch:   80   Train: 3.19724   Validation: 3.19489\n",
      "Epoch:  100   Target: dog-----               Output: --------------------\n",
      "Epoch:  100   Target: go                     Output: --------------------\n",
      "Epoch:  100   Train: 3.17003   Validation: 3.16765\n",
      "Epoch:  110   Target: on------               Output: --------------------\n",
      "Epoch:  110   Target: go                     Output: --------------------\n",
      "Epoch:  110   Train: 3.15910   Validation: 3.15659\n",
      "Epoch:  120   Target: one-----               Output: --------------------\n",
      "Epoch:  120   Target: go                     Output: --------------------\n",
      "Epoch:  120   Train: 3.14924   Validation: 3.14666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  170   Target: bird----               Output: --------------------\n",
      "Epoch:  170   Target: go                     Output: --------------------\n",
      "Epoch:  170   Train: 3.11118   Validation: 3.10860\n",
      "Epoch:  180   Target: one-----               Output: --------------------\n",
      "Epoch:  180   Target: go                     Output: --------------------\n",
      "Epoch:  180   Train: 3.10525   Validation: 3.10258\n",
      "Epoch:  190   Target: no------               Output: --------------------\n",
      "Epoch:  190   Target: go                     Output: --------------------\n",
      "Epoch:  190   Train: 3.09975   Validation: 3.09698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_loss_training = []\n",
    "sum_loss_validation = []\n",
    "\n",
    "with tqdm(total=max_epoch, unit_scale=1) as pbar:\n",
    "    for epoch in range(max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        sum_loss = 0.\n",
    "        for inputs, targets, target_lengths in loader_training:\n",
    "\n",
    "            loss = forward_and_loss(inputs, targets, target_lengths)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1/len(loader_training))\n",
    "\n",
    "        # Average loss\n",
    "        sum_loss = sum_loss / len(loader_training)\n",
    "        sum_loss_training.append((epoch, sum_loss))\n",
    "        sum_loss_str = f\"Epoch: {epoch:4}   Train: {sum_loss:4.5f}\"\n",
    "        \n",
    "        # scheduler.step()\n",
    "        # scheduler.step(sum_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            if not epoch % mod_epoch:\n",
    "            \n",
    "                # Switch to evaluation mode\n",
    "                model.eval()\n",
    "        \n",
    "                print(forward_and_decode(inputs, targets))\n",
    "\n",
    "                sum_loss = 0.\n",
    "                for inputs, targets, target_lengths in loader_validation:\n",
    "\n",
    "                    sum_loss += forward_and_loss(inputs, targets, target_lengths).item()\n",
    "\n",
    "                # Average loss\n",
    "                sum_loss = sum_loss / len(loader_validation)\n",
    "                sum_loss_validation.append((epoch, sum_loss))\n",
    "                sum_loss_str += f\"   Validation: {sum_loss:.5f}\"\n",
    "\n",
    "                print(forward_and_decode(inputs, targets))\n",
    "\n",
    "                print(sum_loss_str)\n",
    "\n",
    "                if sum_loss < best_loss:\n",
    "                    # Save model\n",
    "                    torch.save(model.state_dict(), f\"./model.{dtstamp}.{epoch}.ph\")\n",
    "                    best_loss = sum_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(*zip(*sum_loss_training), label=\"training\")\n",
    "plt.plot(*zip(*sum_loss_validation), label=\"validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum_loss_training = [(a, math.log(b)) for (a,b) in sum_loss_training]\n",
    "sum_loss_validation = [(a, math.log(b)) for (a,b) in sum_loss_validation]\n",
    "\n",
    "plt.plot(*zip(*sum_loss_training), label=\"training\")\n",
    "plt.plot(*zip(*sum_loss_validation), label=\"validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         274203085 function calls (272584553 primitive calls) in 2391.520 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 2448 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        7    0.000    0.000 2391.518  341.645 base_events.py:1686(_run_once)\n",
      "     51/7    0.000    0.000 2391.518  341.645 gen.py:716(run)\n",
      "       97    0.000    0.000 2391.517   24.655 events.py:86(_run)\n",
      "    80/23    0.000    0.000 2391.517  103.979 {method 'send' of 'generator' objects}\n",
      "       97    0.000    0.000 2391.517   24.655 {method 'run' of 'Context' objects}\n",
      "       73    0.000    0.000 2391.517   32.761 ioloop.py:735(_run_callback)\n",
      "        5    0.000    0.000 2391.515  478.303 ioloop.py:690(<lambda>)\n",
      "        5    0.000    0.000 2391.515  478.303 gen.py:784(inner)\n",
      "    64/18    0.001    0.000 2391.514  132.862 gen.py:184(wrapper)\n",
      "       48    0.000    0.000 2391.513   49.823 kernelbase.py:347(process_one)\n",
      "       32    0.001    0.000 2391.512   74.735 kernelbase.py:225(dispatch_shell)\n",
      "332315/48    0.403    0.000 2391.497   49.823 {built-in method builtins.next}\n",
      "       32    0.001    0.000 2391.487   74.734 kernelbase.py:512(execute_request)\n",
      "    48/22    0.000    0.000 2391.474  108.703 gen.py:700(__init__)\n",
      "       16    0.000    0.000 2391.467  149.467 kernelbase.py:363(dispatch_queue)\n",
      "       16    0.000    0.000 2391.454  149.466 ipkernel.py:262(do_execute)\n",
      "       16    0.000    0.000 2391.414  149.463 zmqshell.py:534(run_cell)\n",
      "       16    0.000    0.000 2391.414  149.463 interactiveshell.py:2831(run_cell)\n",
      "       16    0.000    0.000 2391.413  149.463 interactiveshell.py:2865(_run_cell)\n",
      "       16    0.000    0.000 2391.370  149.461 async_helpers.py:58(_pseudo_sync_runner)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), f\"./model.{dtstamp}.{epoch}.ph\")\n",
    "\n",
    "# Print performance\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).strip_dirs().sort_stats(\"cumtime\").print_stats(20)\n",
    "print(s.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
