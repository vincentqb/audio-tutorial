{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/pytorch/issues/13883\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('forkserver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2020-04-25 15:47:41.459476\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import cProfile\n",
    "import hashlib\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pstats\n",
    "import re\n",
    "import shutil\n",
    "import signal\n",
    "import statistics\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torchaudio\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from torch import nn, topk\n",
    "from torch.optim import SGD, Adadelta, Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import LIBRISPEECH, SPEECHCOMMANDS\n",
    "from torchaudio.datasets.utils import diskcache_iterator\n",
    "from torchaudio.transforms import MFCC, Resample\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "print(\"start time: {}\".format(str(datetime.now())), flush=True)\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "    in_notebook = True\n",
    "except NameError:\n",
    "    matplotlib.use(\"Agg\")\n",
    "    in_notebook = False\n",
    "\n",
    "# Empty CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Profiling performance\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--workers', default=0, type=int,\n",
    "                    metavar='N', help='number of data loading workers')\n",
    "parser.add_argument('--resume', default='', type=str,\n",
    "                    metavar='PATH', help='path to latest checkpoint')\n",
    "\n",
    "parser.add_argument('--epochs', default=200, type=int,\n",
    "                    metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int,\n",
    "                    metavar='N', help='manual epoch number')\n",
    "parser.add_argument('--print-freq', default=10, type=int,\n",
    "                    metavar='N', help='print frequency in epochs')\n",
    "\n",
    "parser.add_argument('--arch', metavar='ARCH', default='wav2letter',\n",
    "                    choices=[\"wav2letter\", \"lstm\"], help='model architecture')\n",
    "parser.add_argument('--batch-size', default=64, type=int,\n",
    "                    metavar='N', help='mini-batch size')\n",
    "parser.add_argument('--learning-rate', default=1., type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "# parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "parser.add_argument('--weight-decay', default=1e-5,\n",
    "                    type=float, metavar='W', help='weight decay')\n",
    "parser.add_argument(\"--eps\", metavar='EPS', type=float, default=1e-8)\n",
    "parser.add_argument(\"--rho\", metavar='RHO', type=float, default=.95)\n",
    "\n",
    "parser.add_argument('--world-size', default=1, type=int,\n",
    "                    help='number of distributed processes')\n",
    "parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456',\n",
    "                    type=str, help='url used to set up distributed training')\n",
    "parser.add_argument('--dist-backend', default='nccl',\n",
    "                    type=str, help='distributed backend')\n",
    "parser.add_argument('--distributed', action=\"store_true\")\n",
    "\n",
    "parser.add_argument('--dataset', default='librispeech', type=str)\n",
    "parser.add_argument('--gradient', action=\"store_true\")\n",
    "parser.add_argument('--jit', action=\"store_true\")\n",
    "\n",
    "if in_notebook:\n",
    "    args, _ = parser.parse_known_args()\n",
    "else:\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 32\n",
    "args.model = \"wav2letter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal handler installed\n"
     ]
    }
   ],
   "source": [
    "MAIN_PID = os.getpid()\n",
    "CHECKPOINT_filename = args.resume if args.resume else 'checkpoint.pth.tar'\n",
    "CHECKPOINT_tempfile = CHECKPOINT_filename + '.temp'\n",
    "HALT_filename = CHECKPOINT_filename + '.HALT'\n",
    "SIGNAL_RECEIVED = False\n",
    "\n",
    "# HALT file is used as a sign of job completion.\n",
    "# Make sure no HALT file left from previous runs.\n",
    "if os.path.isfile(HALT_filename):\n",
    "    os.remove(HALT_filename)\n",
    "\n",
    "# Remove CHECKPOINT_tempfile, in case the signal arrives in the\n",
    "# middle of copying from CHECKPOINT_tempfile to CHECKPOINT_filename\n",
    "if os.path.isfile(CHECKPOINT_tempfile):\n",
    "    os.remove(CHECKPOINT_tempfile)\n",
    "\n",
    "\n",
    "def SIGTERM_handler(a, b):\n",
    "    print('received sigterm')\n",
    "    pass\n",
    "\n",
    "\n",
    "def signal_handler(a, b):\n",
    "    global SIGNAL_RECEIVED\n",
    "    print('Signal received', a, datetime.now().strftime(\n",
    "        \"%y%m%d.%H%M%S\"), flush=True)\n",
    "    SIGNAL_RECEIVED = True\n",
    "\n",
    "    # If HALT file exists, which means the job is done, exit peacefully.\n",
    "    if os.path.isfile(HALT_filename):\n",
    "        print('Job is done, exiting')\n",
    "        exit(0)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def trigger_job_requeue():\n",
    "    # Submit a new job to resume from checkpoint.\n",
    "    if os.path.isfile(CHECKPOINT_filename) and \\\n",
    "       os.environ['SLURM_PROCID'] == '0' and \\\n",
    "       os.getpid() == MAIN_PID:\n",
    "        print('pid: ', os.getpid(), ' ppid: ', os.getppid(), flush=True)\n",
    "        print('time is up, back to slurm queue', flush=True)\n",
    "        command = 'scontrol requeue ' + os.environ['SLURM_JOB_ID']\n",
    "        print(command)\n",
    "        if os.system(command):\n",
    "            raise RuntimeError('requeue failed')\n",
    "        print('New job submitted to the queue', flush=True)\n",
    "    exit(0)\n",
    "\n",
    "\n",
    "# Install signal handler\n",
    "signal.signal(signal.SIGUSR1, signal_handler)\n",
    "signal.signal(signal.SIGTERM, SIGTERM_handler)\n",
    "print('Signal handler installed', flush=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=CHECKPOINT_filename):\n",
    "    \"\"\"\n",
    "    Save the model to a temporary file first,\n",
    "    then copy it to filename, in case the signal interrupts\n",
    "    the torch.save() process.\n",
    "    \"\"\"\n",
    "    if not args.distributed or os.environ['SLURM_PROCID'] == '0':\n",
    "        torch.save(state, CHECKPOINT_tempfile)\n",
    "        if os.path.isfile(CHECKPOINT_tempfile):\n",
    "            os.rename(CHECKPOINT_tempfile, filename)\n",
    "        if is_best:\n",
    "            shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        print(\"Checkpoint: saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use #nodes as world_size\n",
    "if 'SLURM_NNODES' in os.environ:\n",
    "    args.world_size = int(os.environ['SLURM_NNODES'])\n",
    "\n",
    "args.distributed = args.distributed or args.world_size > 1\n",
    "\n",
    "if args.distributed:\n",
    "    os.environ['RANK'] = os.environ['SLURM_PROCID']\n",
    "    os.environ['WORLD_SIZE'] = str(args.world_size)\n",
    "    print('in distributed', os.environ['RANK'],\n",
    "          os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'], flush=True)\n",
    "    dist.init_process_group(backend=args.dist_backend,\n",
    "                            init_method=args.dist_url, world_size=args.world_size)\n",
    "\n",
    "    print('init process', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(arch='wav2letter', batch_size=32, dataset='librispeech', dist_backend='nccl', dist_url='tcp://224.66.41.62:23456', distributed=False, epochs=200, eps=1e-08, gradient=False, jit=False, learning_rate=1.0, model='wav2letter', print_freq=10, resume='', rho=0.95, start_epoch=0, weight_decay=1e-05, workers=0, world_size=1)\n"
     ]
    }
   ],
   "source": [
    "if not args.distributed or os.environ['SLURM_PROCID'] == '0':\n",
    "    print(args, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GPUs\n"
     ]
    }
   ],
   "source": [
    "audio_backend = \"soundfile\"\n",
    "torchaudio.set_audio_backend(audio_backend)\n",
    "\n",
    "root = \"/datasets01/\"\n",
    "folder_in_archive = \"librispeech/062419/\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_devices = torch.cuda.device_count()\n",
    "# num_devices = 1\n",
    "print(num_devices, \"GPUs\", flush=True)\n",
    "\n",
    "# max number of sentences per batch\n",
    "batch_size = args.batch_size\n",
    "# batch_size = 2048\n",
    "# batch_size = 512\n",
    "# batch_size = 256\n",
    "# batch_size = 64\n",
    "# batch_size = 1\n",
    "\n",
    "training_percentage = 90.\n",
    "validation_percentage = 5.\n",
    "\n",
    "data_loader_training_params = {\n",
    "    \"num_workers\": args.workers,\n",
    "    \"pin_memory\": True,\n",
    "    \"shuffle\": True,\n",
    "    \"drop_last\": True,\n",
    "}\n",
    "data_loader_validation_params = data_loader_training_params.copy()\n",
    "data_loader_validation_params[\"shuffle\"] = False\n",
    "\n",
    "non_blocking = True\n",
    "\n",
    "\n",
    "# text preprocessing\n",
    "\n",
    "char_null = \"-\"\n",
    "char_space = \" \"\n",
    "char_pad = \"*\"\n",
    "char_apostrophe = \"'\"\n",
    "\n",
    "labels = [char_null + char_pad + char_apostrophe + string.ascii_lowercase]\n",
    "\n",
    "# excluded_dir = [\"_background_noise_\"]\n",
    "# folder_speechcommands = './SpeechCommands/speech_commands_v0.02'\n",
    "# labels = [char_null, char_pad] + [d for d in next(os.walk(folder_speechcommands))[1] if d not in excluded_dir]\n",
    "\n",
    "\n",
    "# audio\n",
    "\n",
    "sample_rate_original = 16000\n",
    "sample_rate_new = 8000\n",
    "# resample = Resample(sample_rate_original, sample_rate_new)\n",
    "resample = None\n",
    "\n",
    "n_mfcc = 13\n",
    "melkwargs = {\n",
    "    'n_fft': 512,\n",
    "    'n_mels': 20,\n",
    "    'hop_length': 80,  # (160, 80)\n",
    "}\n",
    "mfcc = MFCC(sample_rate=sample_rate_original,\n",
    "            n_mfcc=n_mfcc, melkwargs=melkwargs)\n",
    "# mfcc = None\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "optimizer_params_adadelta = {\n",
    "    \"lr\": args.learning_rate,\n",
    "    \"eps\": args.eps,\n",
    "    \"rho\": args.rho,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "}\n",
    "\n",
    "optimizer_params_adam = {\n",
    "    \"lr\": args.learning_rate,\n",
    "    \"eps\": args.eps,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "}\n",
    "\n",
    "optimizer_params_sgd = {\n",
    "    \"lr\": args.learning_rate,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "}\n",
    "\n",
    "optimizer_params_adadelta = {\n",
    "    \"lr\": args.learning_rate,\n",
    "    \"eps\": args.eps,\n",
    "    \"rho\": args.rho,\n",
    "    \"weight_decay\": args.weight_decay,\n",
    "}\n",
    "\n",
    "Optimizer = Adadelta\n",
    "optimizer_params = optimizer_params_sgd\n",
    "\n",
    "gamma = 0.96\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "num_features = n_mfcc if n_mfcc else 1\n",
    "\n",
    "lstm_params = {\n",
    "    \"hidden_size\": 800,\n",
    "    \"num_layers\": 5,\n",
    "    \"batch_first\": False,\n",
    "    \"bidirectional\": False,\n",
    "    \"dropout\": 0.,\n",
    "}\n",
    "\n",
    "clip_norm = 0.  # 10.\n",
    "\n",
    "zero_infinity = False\n",
    "\n",
    "start_epoch = args.start_epoch\n",
    "max_epoch = args.epochs\n",
    "mod_epoch = args.print_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 29\n"
     ]
    }
   ],
   "source": [
    "class Coder:\n",
    "    def __init__(self, labels):\n",
    "        labels = list(collections.OrderedDict.fromkeys(list(\"\".join(labels))))\n",
    "        self.length = len(labels)\n",
    "        enumerated = list(enumerate(labels))\n",
    "        flipped = [(sub[1], sub[0]) for sub in enumerated]\n",
    "\n",
    "        d1 = collections.OrderedDict(enumerated)\n",
    "        d2 = collections.OrderedDict(flipped)\n",
    "        self.mapping = {**d1, **d2}\n",
    "        self.mapping[char_space] = self.mapping[char_pad]\n",
    "\n",
    "    def _map(self, iterable):\n",
    "        # iterable to iterable\n",
    "        return [self.mapping[i] for i in iterable]\n",
    "\n",
    "    def encode(self, iterable):\n",
    "        if isinstance(iterable[0], list):\n",
    "            return [self.encode(i) for i in iterable]\n",
    "        else:\n",
    "            return self._map(iterable)\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        if isinstance(tensor[0], list):\n",
    "            return [self.decode(t) for t in tensor]\n",
    "        else:\n",
    "            # not idempotent, since clean string\n",
    "            return \"\".join(self._map(tensor)).replace(char_null, \"\").replace(char_pad, char_space).strip()\n",
    "\n",
    "\n",
    "coder = Coder(labels)\n",
    "encode = coder.encode\n",
    "decode = coder.decode\n",
    "vocab_size = coder.length\n",
    "print(\"vocab_size\", vocab_size, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableMemoryCache:\n",
    "\n",
    "    def __init__(self, iterable):\n",
    "        self.iterable = iterable\n",
    "        self._iter = iter(iterable)\n",
    "        self._done = False\n",
    "        self._values = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self._done:\n",
    "            return iter(self._values)\n",
    "        return itertools.chain(self._values, self._gen_iter())\n",
    "\n",
    "    def _gen_iter(self):\n",
    "        for new_value in self._iter:\n",
    "            self._values.append(new_value)\n",
    "            yield new_value\n",
    "        self._done = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._iterable)\n",
    "\n",
    "\n",
    "class MapMemoryCache(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a dataset so that, whenever a new item is returned, it is saved to memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self._cache = [None] * len(dataset)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        if self._cache[n]:\n",
    "            return self._cache[n]\n",
    "\n",
    "        item = self.dataset[n]\n",
    "        self._cache[n] = item\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "class Processed(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, process_datapoint, dataset):\n",
    "        self.process_datapoint = process_datapoint\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        try:\n",
    "            item = self.dataset[n]\n",
    "            return self.process_datapoint(item)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return None\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            item = next(self.dataset)\n",
    "            return self.process_datapoint(item)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return self.__next__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc = mfcc.to(device)\n",
    "# resample = resample.to(device)\n",
    "\n",
    "# @torch.jit.script\n",
    "\n",
    "\n",
    "def process_datapoint(item):\n",
    "    transformed = item[0]  # .to(device, non_blocking=non_blocking)\n",
    "    target = item[2].lower().replace(char_space, char_pad)\n",
    "\n",
    "    # apply mfcc, tranpose for pad_sequence\n",
    "    if resample is not None:\n",
    "        transformed = resample(transformed)\n",
    "\n",
    "    if mfcc is not None:\n",
    "        transformed = mfcc(transformed)\n",
    "    else:\n",
    "        transformed = transformed.unsqueeze(1)\n",
    "\n",
    "    transformed = transformed[0, ...].transpose(0, -1)\n",
    "\n",
    "    target = \" \" + target + \" \"\n",
    "    target = encode(target)\n",
    "    target = torch.tensor(target, dtype=torch.long, device=transformed.device)\n",
    "\n",
    "    transformed = transformed  # .to(\"cpu\")\n",
    "    target = target  # .to(\"cpu\")\n",
    "    return transformed, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gives_error(d, i):\n",
    "    try:\n",
    "        d[i]\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "\n",
    "if False:\n",
    "    a = LIBRISPEECH(root, \"dev-clean\",\n",
    "                    folder_in_archive=folder_in_archive, download=False)\n",
    "    la = [i for i in range(len(a)) if gives_error(a, i)]\n",
    "    print(la)\n",
    "\n",
    "    b = LIBRISPEECH(root, \"train-clean-100\",\n",
    "                    folder_in_archive=folder_in_archive, download=False)\n",
    "    lb = [i for i in range(len(b)) if gives_error(b, i)]\n",
    "    print(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_librispeech():\n",
    "\n",
    "    def create(tag):\n",
    "\n",
    "        if isinstance(tag, str):\n",
    "            data = LIBRISPEECH(\n",
    "                root, tag, folder_in_archive=folder_in_archive, download=False)\n",
    "        else:\n",
    "            data = torch.utils.data.ConcatDataset([LIBRISPEECH(\n",
    "                root, t, folder_in_archive=folder_in_archive, download=False) for t in tag])\n",
    "\n",
    "        data = Processed(process_datapoint, data)\n",
    "        data = diskcache_iterator(data)\n",
    "        # data = MapMemoryCache(data)\n",
    "        return data\n",
    "\n",
    "    # return create(\"train-clean-100\"), create(\"dev-clean\"), None\n",
    "    return create([\"train-clean-100\", \"train-clean-360\", \"train-other-500\"]), create([\"dev-clean\", \"dev-other\"]), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_set(filename, validation_percentage, testing_percentage):\n",
    "    \"\"\"Determines which data partition the file should belong to.\n",
    "\n",
    "    We want to keep files in the same training, validation, or testing sets even\n",
    "    if new ones are added over time. This makes it less likely that testing\n",
    "    samples will accidentally be reused in training when long runs are restarted\n",
    "    for example. To keep this stability, a hash of the filename is taken and used\n",
    "    to determine which set it should belong to. This determination only depends on\n",
    "    the name and the set proportions, so it won't change as other files are added.\n",
    "\n",
    "    It's also useful to associate particular files as related (for example words\n",
    "    spoken by the same person), so anything after '_nohash_' in a filename is\n",
    "    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n",
    "    'bobby_nohash_1.wav' are always in the same set, for example.\n",
    "\n",
    "    Args:\n",
    "        filename: File path of the data sample.\n",
    "        validation_percentage: How much of the data set to use for validation.\n",
    "        testing_percentage: How much of the data set to use for testing.\n",
    "\n",
    "    Returns:\n",
    "        String, one of 'training', 'validation', or 'testing'.\n",
    "    \"\"\"\n",
    "\n",
    "    MAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\n",
    "\n",
    "    base_name = os.path.basename(filename)\n",
    "\n",
    "    # We want to ignore anything after '_nohash_' in the file name when\n",
    "    # deciding which set to put a wav in, so the data set creator has a way of\n",
    "    # grouping wavs that are close variations of each other.\n",
    "    hash_name = re.sub(r'_nohash_.*$', '', base_name).encode(\"utf-8\")\n",
    "\n",
    "    # This looks a bit magical, but we need to decide whether this file should\n",
    "    # go into the training, testing, or validation sets, and we want to keep\n",
    "    # existing files in the same set even if more files are subsequently\n",
    "    # added.\n",
    "    # To do that, we need a stable way of deciding based on just the file name\n",
    "    # itself, so we do a hash of that and then use that to generate a\n",
    "    # probability value that we use to assign it.\n",
    "    hash_name_hashed = hashlib.sha1(hash_name).hexdigest()\n",
    "    percentage_hash = ((int(hash_name_hashed, 16) % (\n",
    "        MAX_NUM_WAVS_PER_CLASS + 1)) * (100.0 / MAX_NUM_WAVS_PER_CLASS))\n",
    "\n",
    "    if percentage_hash < validation_percentage:\n",
    "        result = 'validation'\n",
    "    elif percentage_hash < (testing_percentage + validation_percentage):\n",
    "        result = 'testing'\n",
    "    else:\n",
    "        result = 'training'\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_speechcommands(tag, training_percentage, data):\n",
    "    if training_percentage < 100.:\n",
    "        testing_percentage = (\n",
    "            100. - training_percentage - validation_percentage)\n",
    "\n",
    "        def which_set_filter(x): return which_set(\n",
    "            x, validation_percentage, testing_percentage) == tag\n",
    "        data._walker = list(filter(which_set_filter, data._walker))\n",
    "    return data\n",
    "\n",
    "\n",
    "def datasets_speechcommands():\n",
    "\n",
    "    root = \"./\"\n",
    "\n",
    "    def create(tag):\n",
    "        data = SPEECHCOMMANDS(root, download=True)\n",
    "        data = filter_speechcommands(tag, training_percentage, data)\n",
    "        data = Processed(process_datapoint, data)\n",
    "        data = MapMemoryCache(data)\n",
    "        return data\n",
    "\n",
    "    return create(\"training\"), create(\"validation\"), create(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"librispeech\":\n",
    "    training, validation, _ = datasets_librispeech()\n",
    "elif args.dataset == \"speechcommand\":\n",
    "    training, validation, _ = datasets_speechcommands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    from collections import Counter\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    training_unprocessed = SPEECHCOMMANDS(\"./\", download=True)\n",
    "    training_unprocessed = filter_speechcommands(\n",
    "        training_percentage, training_unprocessed)\n",
    "\n",
    "    counter = Counter([t[2] for t in training_unprocessed])\n",
    "    counter = OrderedDict(counter.most_common())\n",
    "\n",
    "    plt.bar(counter.keys(), counter.values(), align='center')\n",
    "\n",
    "    if resample is not None:\n",
    "        waveform, sample_rate = training_unprocessed[0][0], training_unprocessed[0][1]\n",
    "\n",
    "        fn = \"sound.wav\"\n",
    "        torchaudio.save(fn, waveform, sample_rate_new)\n",
    "        ipd.Audio(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "[Wav2Letter](https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        size = m.weight.size()\n",
    "        fan_out = size[0]  # number of rows\n",
    "        fan_in = size[1]  # number of columns\n",
    "        variance = math.sqrt(2.0/(fan_in + fan_out))\n",
    "        m.weight.data.normal_(0.0, variance)\n",
    "\n",
    "\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x, flush=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Letter(nn.Module):\n",
    "    \"\"\"Wav2Letter Speech Recognition model\n",
    "        https://arxiv.org/pdf/1609.03193.pdf\n",
    "        This specific architecture accepts mfcc or power spectrums speech signals\n",
    "\n",
    "        Args:\n",
    "            num_features (int): number of mfcc features\n",
    "            num_classes (int): number of unique grapheme class labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_features, out_channels=250,\n",
    "                      kernel_size=48, stride=2, padding=23),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=250,\n",
    "                      kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=250, out_channels=2000,\n",
    "                      kernel_size=32, stride=1, padding=16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=2000, out_channels=2000,\n",
    "                      kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=2000, out_channels=num_classes,\n",
    "                      kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass through Wav2Letter network than\n",
    "            takes log probability of output\n",
    "        Args:\n",
    "            batch (int): mini batch of data\n",
    "            shape (batch, num_features, frame_len)\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, num_classes, output_len)\n",
    "        \"\"\"\n",
    "        # batch: (batch_size, num_features, seq_len)\n",
    "        y_pred = self.layers(batch)\n",
    "        # y_pred: (batch_size, num_classes, output_len)\n",
    "        y_pred = y_pred.transpose(-1, -2)\n",
    "        # y_pred: (batch_size, output_len, num_classes)\n",
    "        return nn.functional.log_softmax(y_pred, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes, hidden_size, num_layers, bidirectional, dropout, batch_first):\n",
    "        super().__init__()\n",
    "\n",
    "        directions = bidirectional + 1\n",
    "\n",
    "        self.layer = nn.LSTM(\n",
    "            num_features, hidden_size=hidden_size,\n",
    "            num_layers=num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=batch_first\n",
    "        )\n",
    "        # self.activation = nn.ReLU(inplace=True)\n",
    "        self.hidden2class = nn.Linear(directions*hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        self.layer.flatten_parameters()\n",
    "        # print(\"forward\", flush=True)\n",
    "        # batch: batch, num_features, seq_len\n",
    "        # print(batch.shape, flush=True)\n",
    "        batch = batch.transpose(-1, -2).contiguous()\n",
    "        # batch: batch, seq_len, num_features\n",
    "        # print(batch.shape, flush=True)\n",
    "        outputs, _ = self.layer(batch)\n",
    "        # outputs = self.activation(outputs)\n",
    "        # outputs: batch, seq_len, directions*num_features\n",
    "        outputs = self.hidden2class(outputs)\n",
    "        # outputs: batch, seq_len, num_features\n",
    "        # print(outputs.shape, flush=True)\n",
    "        return nn.functional.log_softmax(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(outputs):\n",
    "    \"\"\"Greedy Decoder. Returns highest probability of class labels for each timestep\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): shape (input length, batch size, number of classes (including blank))\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: class labels per time step.\n",
    "    \"\"\"\n",
    "    _, indices = topk(outputs, k=1, dim=-1)\n",
    "    return indices[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_transitions():\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    c = None\n",
    "\n",
    "    for _, label in training:\n",
    "        # Count bigrams\n",
    "        count = [((a.item(), b.item())) for (a, b) in zip(label, label[1:])]\n",
    "        count = Counter(count)\n",
    "        if c is None:\n",
    "            c = count\n",
    "        else:\n",
    "            c = c + count\n",
    "\n",
    "    # Encode as transition matrix\n",
    "\n",
    "    ind = torch.tensor(list(zip(*[a for (a, b) in c.items()])))\n",
    "    val = torch.tensor([b for (a, b) in c.items()], dtype=torch.float)\n",
    "\n",
    "    transitions = torch.sparse_coo_tensor(indices=ind, values=val, size=[\n",
    "                                          vocab_size, vocab_size]).coalesce().to_dense()\n",
    "    transitions = (transitions/torch.max(torch.tensor(1.),\n",
    "                                         transitions.max(dim=1)[0]).unsqueeze(1))\n",
    "\n",
    "    return transitions\n",
    "\n",
    "\n",
    "# transitions = build_transitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/PetrochukM/afaa3613a99a8e7213d2efdd02ae4762\n",
    "# https://github.com/napsternxg/pytorch-practice/blob/master/Viterbi%20decoding%20and%20CRF.ipynb\n",
    "\n",
    "\n",
    "def viterbi_decode(tag_sequence: torch.Tensor, transition_matrix: torch.Tensor, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform Viterbi decoding in log space over a sequence given a transition matrix\n",
    "    specifying pairwise (transition) potentials between tags and a matrix of shape\n",
    "    (sequence_length, num_tags) specifying unary potentials for possible tags per\n",
    "    timestep.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag_sequence : torch.Tensor, required.\n",
    "        A tensor of shape (sequence_length, num_tags) representing scores for\n",
    "        a set of tags over a given sequence.\n",
    "    transition_matrix : torch.Tensor, required.\n",
    "        A tensor of shape (num_tags, num_tags) representing the binary potentials\n",
    "        for transitioning between a given pair of tags.\n",
    "    top_k : int, required.\n",
    "        Integer defining the top number of paths to decode.\n",
    "    Returns\n",
    "    -------\n",
    "    viterbi_path : List[int]\n",
    "        The tag indices of the maximum likelihood tag sequence.\n",
    "    viterbi_score : float\n",
    "        The score of the viterbi path.\n",
    "    \"\"\"\n",
    "    sequence_length, num_tags = tag_sequence.size()\n",
    "\n",
    "    path_scores = []\n",
    "    path_indices = []\n",
    "    # At the beginning, the maximum number of permutations is 1; therefore, we unsqueeze(0)\n",
    "    # to allow for 1 permutation.\n",
    "    path_scores.append(tag_sequence[0, :].unsqueeze(0))\n",
    "    # assert path_scores[0].size() == (n_permutations, num_tags)\n",
    "\n",
    "    # Evaluate the scores for all possible paths.\n",
    "    for timestep in range(1, sequence_length):\n",
    "        # Add pairwise potentials to current scores.\n",
    "        # assert path_scores[timestep - 1].size() == (n_permutations, num_tags)\n",
    "        summed_potentials = path_scores[timestep -\n",
    "                                        1].unsqueeze(2) + transition_matrix\n",
    "        summed_potentials = summed_potentials.view(-1, num_tags)\n",
    "\n",
    "        # Best pairwise potential path score from the previous timestep.\n",
    "        max_k = min(summed_potentials.size()[0], top_k)\n",
    "        scores, paths = torch.topk(summed_potentials, k=max_k, dim=0)\n",
    "        # assert scores.size() == (n_permutations, num_tags)\n",
    "        # assert paths.size() == (n_permutations, num_tags)\n",
    "\n",
    "        scores = tag_sequence[timestep, :] + scores\n",
    "        # assert scores.size() == (n_permutations, num_tags)\n",
    "        path_scores.append(scores)\n",
    "        path_indices.append(paths.squeeze())\n",
    "\n",
    "    # Construct the most likely sequence backwards.\n",
    "    path_scores = path_scores[-1].view(-1)\n",
    "    max_k = min(path_scores.size()[0], top_k)\n",
    "    viterbi_scores, best_paths = torch.topk(path_scores, k=max_k, dim=0)\n",
    "\n",
    "    viterbi_paths = []\n",
    "    for i in range(max_k):\n",
    "\n",
    "        viterbi_path = [best_paths[i].item()]\n",
    "        for backward_timestep in reversed(path_indices):\n",
    "            viterbi_path.append(\n",
    "                int(backward_timestep.view(-1)[viterbi_path[-1]]))\n",
    "\n",
    "        # Reverse the backward path.\n",
    "        viterbi_path.reverse()\n",
    "\n",
    "        # Viterbi paths uses (num_tags * n_permutations) nodes; therefore, we need to modulo.\n",
    "        viterbi_path = [j % num_tags for j in viterbi_path]\n",
    "        viterbi_paths.append(viterbi_path)\n",
    "\n",
    "    return viterbi_paths, viterbi_scores\n",
    "\n",
    "\n",
    "def batch_viterbi_decode(tag_sequence: torch.Tensor, transition_matrix: torch.Tensor, top_k: int = 5):\n",
    "\n",
    "    outputs = []\n",
    "    scores = []\n",
    "    for i in range(tag_sequence.shape[1]):\n",
    "        paths, score = viterbi_decode(tag_sequence[:, i, :], transitions)\n",
    "        outputs.append(paths)\n",
    "        scores.append(score)\n",
    "\n",
    "    return torch.tensor(outputs).transpose(0, -1), torch.cat(scores)\n",
    "\n",
    "\n",
    "def top_batch_viterbi_decode(tag_sequence: torch.Tensor):\n",
    "    output, _ = batch_viterbi_decode(tag_sequence, transitions, top_k=1)\n",
    "    return output[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance_list(r, h):\n",
    "\n",
    "    # initialisation\n",
    "    d = [[0] * (len(h)+1)] * (len(r)+1)\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i].append(d[i-1][j-1])\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion = d[i][j-1] + 1\n",
    "                deletion = d[i-1][j] + 1\n",
    "                d[i].append(min(substitution, insertion, deletion))\n",
    "\n",
    "    return d[len(r)][len(h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance_list(r, h):\n",
    "\n",
    "    # initialisation\n",
    "    dold = [0] * (len(h)+1)\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        dnew = [0]\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                dnew.append(dold[j-1])\n",
    "            else:\n",
    "                substitution = dold[j-1] + 1\n",
    "                insertion = dnew[j-1] + 1\n",
    "                deletion = dold[j] + 1\n",
    "                dnew.append(min(substitution, insertion, deletion))\n",
    "\n",
    "        dold = dnew\n",
    "\n",
    "    return dnew[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://martin-thoma.com/word-error-rate-calculation/\n",
    "\n",
    "\n",
    "def levenshtein_distance(r, h, device=None):\n",
    "\n",
    "    # initialisation\n",
    "    d = torch.zeros((len(r)+1, len(h)+1), dtype=torch.long, device=device)\n",
    "    d[0, :] = torch.arange(0, len(h)+1, dtype=torch.long, device=device)\n",
    "    d[:, 0] = torch.arange(0, len(r)+1, dtype=torch.long, device=device)\n",
    "\n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "\n",
    "            if r[i-1] == h[j-1]:\n",
    "                d[i, j] = d[i-1, j-1]\n",
    "            else:\n",
    "                substitution = d[i-1, j-1] + 1\n",
    "                insertion = d[i, j-1] + 1\n",
    "                deletion = d[i-1, j] + 1\n",
    "                d[i, j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    dist = d[len(r), len(h)].item()\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.52 ms ± 5.78 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "r = \"abcdddee\"\n",
    "h = \"abcddde\"\n",
    "%timeit levenshtein_distance(r, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.7 µs ± 1.68 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "r = \"abcdddee\"\n",
    "h = \"abcddde\"\n",
    "%timeit levenshtein_distance_list(r, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.arch == \"wav2letter\":\n",
    "    model = Wav2Letter(num_features, vocab_size)\n",
    "\n",
    "    def model_length_function(tensor):\n",
    "        return int(tensor.shape[0])//2 + 1\n",
    "\n",
    "elif args.arch == \"lstm\":\n",
    "    model = LSTMModel(num_features, vocab_size, **lstm_params)\n",
    "\n",
    "    def model_length_function(tensor):\n",
    "        return int(tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_after_model = {}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    tensors = [b[0] for b in batch if b]\n",
    "\n",
    "    if False:\n",
    "        for tensor in tensors:\n",
    "            shape = int(tensor.shape[0])\n",
    "            if shape not in shape_after_model:\n",
    "                tensor = tensor.t().unsqueeze(0)\n",
    "\n",
    "                training = model.training\n",
    "                model.eval()\n",
    "                output = model(tensor)\n",
    "                model.train(training)\n",
    "\n",
    "                shape_after_model[shape] = int(output.shape[1])\n",
    "\n",
    "        tensors_lengths = torch.tensor(\n",
    "            [shape_after_model[int(t.shape[0])] for t in tensors], dtype=torch.long, device=tensors[0].device\n",
    "        )\n",
    "        # print(tensors_lengths)\n",
    "\n",
    "    if True:\n",
    "        tensors_lengths = torch.tensor(\n",
    "            [model_length_function(t) for t in tensors], dtype=torch.long, device=tensors[0].device\n",
    "        )\n",
    "        # print(tensors_lengths)\n",
    "\n",
    "    if False:\n",
    "        # (batch, seq_len, num_directions * hidden_size)\n",
    "        tensors_lengths = torch.tensor(\n",
    "            [int(t.shape[-1]) for t in tensors], dtype=torch.long, device=tensors[0].device\n",
    "        )\n",
    "\n",
    "    # print([int(t.shape[0]) for t in tensors])\n",
    "    # print([shape_after_model[int(t.shape[0])] for t in tensors])\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "\n",
    "    targets = [b[1] for b in batch if b]\n",
    "    target_lengths = torch.tensor(\n",
    "        [target.shape[0] for target in targets], dtype=torch.long, device=tensors.device\n",
    "    )\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "    # print(targets.shape, flush=True)\n",
    "    # print(decode(targets.tolist()), flush=True)\n",
    "\n",
    "    return tensors, targets, tensors_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model cuda\n"
     ]
    }
   ],
   "source": [
    "if args.jit:\n",
    "    model = torch.jit.script(model)\n",
    "\n",
    "if not args.distributed:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "else:\n",
    "    model.cuda()\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    # model = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)\n",
    "\n",
    "model = model.to(device, non_blocking=non_blocking)\n",
    "print('model cuda', flush=True)\n",
    "# model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 23282529\n",
      "Approximate space taken: 93.1\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if not args.distributed or os.environ['SLURM_PROCID'] == '0':\n",
    "    n = count_parameters(model)\n",
    "    print(f\"Number of parameters: {n}\", flush=True)\n",
    "    # Each float32 is 4 bytes.\n",
    "    print(f\"Approximate space taken: {n * 4 / (10 ** 6):.1f} MB\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17069.309952"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).total_memory / 10**6  # Convert to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9936896"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "c = torch.cuda.memory_cached(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = c-a  # free inside cache\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   92696 KB |   92696 KB |   92696 KB |       0 B  |\n",
      "|       from large pool |   91835 KB |   91835 KB |   91835 KB |       0 B  |\n",
      "|       from small pool |     861 KB |     861 KB |     861 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   92696 KB |   92696 KB |   92696 KB |       0 B  |\n",
      "|       from large pool |   91835 KB |   91835 KB |   91835 KB |       0 B  |\n",
      "|       from small pool |     861 KB |     861 KB |     861 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  102400 KB |  102400 KB |  102400 KB |       0 B  |\n",
      "|       from large pool |  100352 KB |  100352 KB |  100352 KB |       0 B  |\n",
      "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    9704 KB |   20208 KB |   20209 KB |   10505 KB |\n",
      "|       from large pool |    8517 KB |   18771 KB |   18771 KB |   10254 KB |\n",
      "|       from small pool |    1187 KB |    1438 KB |    1438 KB |     251 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      22    |      22    |      22    |       0    |\n",
      "|       from large pool |       9    |       9    |       9    |       0    |\n",
      "|       from small pool |      13    |      13    |      13    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      22    |      22    |      22    |       0    |\n",
      "|       from large pool |       9    |       9    |       9    |       0    |\n",
      "|       from small pool |      13    |      13    |      13    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       4    |       4    |       4    |       0    |\n",
      "|       from large pool |       3    |       3    |       3    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       2    |       2    |       2    |       0    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(model.parameters(), **optimizer_params)\n",
    "scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "# scheduler = ReduceLROnPlateau(optimizer)\n",
    "\n",
    "criterion = torch.nn.CTCLoss(zero_infinity=zero_infinity)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = torch.nn.NLLLoss()\n",
    "\n",
    "best_loss = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8788 173\n"
     ]
    }
   ],
   "source": [
    "loader_training = DataLoader(\n",
    "    training, batch_size=batch_size, collate_fn=collate_fn, **data_loader_training_params\n",
    ")\n",
    "\n",
    "loader_validation = DataLoader(\n",
    "    validation, batch_size=batch_size, collate_fn=collate_fn, **data_loader_validation_params\n",
    ")\n",
    "\n",
    "print(len(loader_training), len(loader_validation), flush=True)\n",
    "\n",
    "# num_features = next(iter(loader_training))[0].shape[1]\n",
    "# print(num_features, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_loss(inputs, targets, tensors_lengths, target_lengths):\n",
    "\n",
    "    inputs = inputs.to(device, non_blocking=non_blocking)\n",
    "    targets = targets.to(device, non_blocking=non_blocking)\n",
    "\n",
    "    # keep batch first for data parallel\n",
    "    outputs = model(inputs).transpose(0, 1)\n",
    "\n",
    "    this_batch_size = outputs.shape[1]\n",
    "    seq_len = outputs.shape[0]\n",
    "    # input_lengths = torch.full((this_batch_size,), seq_len, dtype=torch.long, device=outputs.device)\n",
    "    input_lengths = tensors_lengths\n",
    "\n",
    "    # CTC\n",
    "    # outputs: input length, batch size, number of classes (including blank)\n",
    "    # targets: batch size, max target length\n",
    "    # input_lengths: batch size\n",
    "    # target_lengths: batch size\n",
    "\n",
    "    return criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "\n",
    "def forward_decode(inputs, targets, decoder):\n",
    "\n",
    "    inputs = inputs.to(device, non_blocking=True)\n",
    "    output = model(inputs).to(\"cpu\")\n",
    "    output = decoder(output)\n",
    "\n",
    "    output = decode(output.tolist())\n",
    "    target = decode(targets.tolist())\n",
    "\n",
    "    print_length = 20\n",
    "    output_print = output[0].ljust(print_length)[:print_length]\n",
    "    target_print = target[0].ljust(print_length)[:print_length]\n",
    "    print(\n",
    "        f\"Epoch: {epoch:4}   Target: {target_print}   Output: {output_print}\", flush=True)\n",
    "\n",
    "    cers = [levenshtein_distance(a, b) for a, b in zip(target, output)]\n",
    "    cers_normalized = [d/len(a) for a, d in zip(target, cers)]\n",
    "    cers = statistics.mean(cers)\n",
    "    cers_normalized = statistics.mean(cers_normalized)\n",
    "\n",
    "    output = [o.split(char_space) for o in output]\n",
    "    target = [o.split(char_space) for o in target]\n",
    "\n",
    "    wers = [levenshtein_distance(a, b) for a, b in zip(target, output)]\n",
    "    wers_normalized = [d/len(a) for a, d in zip(target, wers)]\n",
    "    wers = statistics.mean(wers)\n",
    "    wers_normalized = statistics.mean(wers_normalized)\n",
    "\n",
    "    print(f\"Epoch: {epoch:4}   CER: {cers:1.5f}   WER: {wers:1.5f}\", flush=True)\n",
    "\n",
    "    return cers, wers, cers_normalized, wers_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: not found\n",
      "Checkpoint: saved\n"
     ]
    }
   ],
   "source": [
    "history_loader = defaultdict(list)\n",
    "history_training = defaultdict(list)\n",
    "history_validation = defaultdict(list)\n",
    "\n",
    "if args.resume and os.path.isfile(CHECKPOINT_filename):\n",
    "    print(\"Checkpoint: loading '{}'\".format(CHECKPOINT_filename))\n",
    "    checkpoint = torch.load(CHECKPOINT_filename)\n",
    "\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    history_training = checkpoint['history_training']\n",
    "    history_validation = checkpoint['history_validation']\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    print(\"Checkpoint: loaded '{}' at epoch {}\".format(\n",
    "        CHECKPOINT_filename, checkpoint['epoch']))\n",
    "    print(tabulate(history_training, headers=\"keys\"), flush=True)\n",
    "    print(tabulate(history_validation, headers=\"keys\"), flush=True)\n",
    "else:\n",
    "    print(\"Checkpoint: not found\")\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': start_epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_loss': best_loss,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict(),\n",
    "        'history_training': history_training,\n",
    "        'history_validation': history_validation,\n",
    "    }, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812c91679dec4efdb2d812b1d426d954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory after training: 3054077440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory after training: 3054077440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory after training: 3054077440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory after training: 3054077440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory after training: 3054077440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory after training: 3054077440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory after training: 3054077440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory after training: 3054077440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2fc312134299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             loss = forward_loss(\n\u001b[0;32m---> 11\u001b[0;31m                 inputs, targets, tensors_lengths, target_lengths)\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-8cb246ef7670>\u001b[0m in \u001b[0;36mforward_loss\u001b[0;34m(inputs, targets, tensors_lengths, target_lengths)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=max_epoch, unit_scale=1, disable=args.distributed) as pbar:\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        model.train()\n",
    "\n",
    "        sum_loss = 0.\n",
    "        total_norm = 0.\n",
    "        for inputs, targets, tensors_lengths, target_lengths in loader_training:\n",
    "\n",
    "            loss = forward_loss(\n",
    "                inputs, targets, tensors_lengths, target_lengths)\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            norm = 0.\n",
    "            if clip_norm > 0:\n",
    "                norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), clip_norm)\n",
    "                total_norm += norm\n",
    "            elif args.gradient:\n",
    "                for p in list(filter(lambda p: p.grad is not None, model.parameters())):\n",
    "                    norm += p.grad.data.norm(2).item() ** 2\n",
    "                norm = norm ** .5\n",
    "                total_norm += norm\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            memory = torch.cuda.max_memory_allocated()\n",
    "            # print(f\"memory in training: {memory}\", flush=True)\n",
    "\n",
    "            history_loader[\"epoch\"].append(pbar.n)\n",
    "            history_loader[\"memory\"].append(memory)\n",
    "\n",
    "            if SIGNAL_RECEIVED:\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_loss': best_loss,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'history_training': history_training,\n",
    "                    'history_validation': history_validation,\n",
    "                }, False)\n",
    "                trigger_job_requeue()\n",
    "\n",
    "            pbar.update(1/len(loader_training))\n",
    "\n",
    "        total_norm = (total_norm ** .5) / len(loader_training)\n",
    "        if total_norm > 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch:4}   Gradient: {total_norm:4.5f}\", flush=True)\n",
    "\n",
    "        # Average loss\n",
    "        sum_loss = sum_loss / len(loader_training)\n",
    "        sum_loss_str = f\"Epoch: {epoch:4}   Train: {sum_loss:4.5f}\"\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        memory = torch.cuda.max_memory_allocated()\n",
    "        print(f\"memory after training: {memory}\", flush=True)\n",
    "\n",
    "        history_training[\"epoch\"].append(epoch)\n",
    "        history_training[\"gradient_norm\"].append(total_norm)\n",
    "        history_training[\"sum_loss\"].append(sum_loss)\n",
    "        history_training[\"max_memory_allocated\"].append(memory)\n",
    "\n",
    "        if not epoch % mod_epoch or epoch == max_epoch - 1:\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Switch to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                sum_loss = 0.\n",
    "                for inputs, targets, tensors_lengths, target_lengths in loader_validation:\n",
    "                    sum_loss += forward_loss(inputs, targets,\n",
    "                                             tensors_lengths, target_lengths).item()\n",
    "\n",
    "                    if SIGNAL_RECEIVED:\n",
    "                        break\n",
    "\n",
    "                # Average loss\n",
    "                sum_loss = sum_loss / len(loader_validation)\n",
    "                sum_loss_str += f\"   Validation: {sum_loss:.5f}\"\n",
    "                print(sum_loss_str, flush=True)\n",
    "\n",
    "                print(\"greedy decoder\", flush=True)\n",
    "                cer1, wer1, cern1, wern1 = forward_decode(\n",
    "                    inputs, targets, greedy_decode)\n",
    "\n",
    "                print(\"viterbi decoder\", flush=True)\n",
    "                # cer2, wer2, cern2, wern2 = forward_decode(inputs, targets, top_batch_viterbi_decode)\n",
    "\n",
    "                memory = torch.cuda.max_memory_allocated()\n",
    "                print(f\"memory after validation: {memory}\", flush=True)\n",
    "\n",
    "                history_validation[\"epoch\"].append(epoch)\n",
    "                history_validation[\"sum_loss\"].append(sum_loss)\n",
    "                history_validation[\"greedy_cer\"].append(cer1)\n",
    "                history_validation[\"greedy_cer_normalized\"].append(cern1)\n",
    "                history_validation[\"greedy_wer\"].append(wer1)\n",
    "                history_validation[\"greedy_wer_normalized\"].append(wern1)\n",
    "                # history_validation[\"viterbi_cer\"].append(cer2)\n",
    "                # history_validation[\"viterbi_cer_normalized\"].append(cern2)\n",
    "                # history_validation[\"viterbi_wer\"].append(wer2)\n",
    "                # history_validation[\"viterbi_wer_normalized\"].append(wern2)\n",
    "                history_validation[\"max_memory_allocated\"].append(memory)\n",
    "\n",
    "                is_best = sum_loss < best_loss\n",
    "                best_loss = min(sum_loss, best_loss)\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_loss': best_loss,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'history_training': history_training,\n",
    "                    'history_validation': history_validation,\n",
    "                }, is_best)\n",
    "\n",
    "                print(tabulate(history_training, headers=\"keys\"), flush=True)\n",
    "                print(tabulate(history_validation, headers=\"keys\"), flush=True)\n",
    "                print(torch.cuda.memory_summary(), flush=True)\n",
    "\n",
    "                # scheduler.step(sum_loss)\n",
    "\n",
    "    # Create an empty file HALT_filename, mark the job as finished\n",
    "    if epoch == max_epoch - 1:\n",
    "        open(HALT_filename, 'a').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    gradient_norm    sum_loss    max_memory_allocated\n",
      "-------  ---------------  ----------  ----------------------\n",
      "      0                0         nan              3054077440\n",
      "      1                0         nan              3054077440\n",
      "      2                0         nan              3054077440\n",
      "      3                0         nan              3054077440\n",
      "      4                0         nan              3054077440\n",
      "      5                0         nan              3054077440\n",
      "      6                0         nan              3054077440\n",
      "      7                0         nan              3054077440\n",
      "      8                0         nan              3054077440\n",
      "      9                0         nan              3054077440\n",
      "     10                0         nan              3054077440\n",
      "     11                0         nan              3054077440\n",
      "     12                0         nan              3054077440\n",
      "     13                0         nan              3054077440\n",
      "     14                0         nan              3054077440\n",
      "     15                0         nan              3054077440\n",
      "     16                0         nan              3054077440\n",
      "     17                0         nan              3054077440\n",
      "     18                0         nan              3054077440\n",
      "     19                0         nan              3054077440\n",
      "     20                0         nan              3054077440\n",
      "     21                0         nan              3054077440\n",
      "  epoch    sum_loss    greedy_cer    greedy_cer_normalized    greedy_wer    greedy_wer_normalized    max_memory_allocated\n",
      "-------  ----------  ------------  -----------------------  ------------  -----------------------  ----------------------\n",
      "      0         nan       1571.38                  41.2335       12.7188                        1              3054077440\n",
      "     10         nan       1571.38                  41.2335       12.7188                        1              3054077440\n",
      "     20         nan       1571.38                  41.2335       12.7188                        1              3054077440\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  366158 KB |    2689 MB |     891 TB |     891 TB |\n",
      "|       from large pool |  363332 KB |    2686 MB |     890 TB |     890 TB |\n",
      "|       from small pool |    2826 KB |       4 MB |       1 TB |       1 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  366158 KB |    2689 MB |     891 TB |     891 TB |\n",
      "|       from large pool |  363332 KB |    2686 MB |     890 TB |     890 TB |\n",
      "|       from small pool |    2826 KB |       4 MB |       1 TB |       1 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    8280 MB |    8280 MB |   21866 MB |   13586 MB |\n",
      "|       from large pool |    8274 MB |    8274 MB |   21858 MB |   13584 MB |\n",
      "|       from small pool |       6 MB |       6 MB |       8 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   41394 KB |     986 MB |     850 TB |     850 TB |\n",
      "|       from large pool |   40124 KB |     985 MB |     848 TB |     848 TB |\n",
      "|       from small pool |    1270 KB |       3 MB |       1 TB |       1 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      73    |     103    |   45307 K  |   45307 K  |\n",
      "|       from large pool |      31    |      51    |   24617 K  |   24617 K  |\n",
      "|       from small pool |      42    |      59    |   20690 K  |   20690 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      73    |     103    |   45307 K  |   45307 K  |\n",
      "|       from large pool |      31    |      51    |   24617 K  |   24617 K  |\n",
      "|       from small pool |      42    |      59    |   20690 K  |   20690 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      37    |      37    |      93    |      56    |\n",
      "|       from large pool |      34    |      34    |      89    |      55    |\n",
      "|       from small pool |       3    |       3    |       4    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      13    |      23    |   18057 K  |   18057 K  |\n",
      "|       from large pool |       8    |      19    |   12153 K  |   12153 K  |\n",
      "|       from small pool |       5    |       7    |    5904 K  |    5904 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(history_training, headers=\"keys\"), flush=True)\n",
    "print(tabulate(history_validation, headers=\"keys\"), flush=True)\n",
    "print(torch.cuda.memory_summary(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(history_loader, headers=\"keys\"), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f98453228d0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_loader[\"epoch\"],\n",
    "         history_loader[\"memory\"], label=\"memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 20]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_validation[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f996c427890>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAX/klEQVR4nO3df5DU9Z3n8efrAgoSiazM3hEmHuqCP9DJBBrkLmoIRmUtFYx7UbJXWPlRY6iQ7JrLbmK2XLFSSSUerhV0l4grEnL+WO9Mgj9w183WnlQsomkCGX4lBrOutnAyyrlCFMOP9/3Rn+Hatofp6ZnpAT6vR9W3pvv9/Xy//Z7vNK/p+fS3+SoiMDOzPPy7oW7AzMyax6FvZpYRh76ZWUYc+mZmGXHom5llZNhQN9CbsWPHxoQJE4a6DTOzo8q6detejYiW6voRH/oTJkygWCwOdRtmZkcVSf9aq+7pHTOzjDj0zcwy4tA3M8vIET+nb2YGsG/fPkqlEnv37h3qVo4oI0aMoLW1leHDh9c13qFvZkeFUqnEiSeeyIQJE5A01O0cESKC1157jVKpxKmnnlrXNp7eMbOjwt69ezn55JMd+BUkcfLJJ/fprx+HvpkdNRz479bXY+LQNzPLiEPfzOwIMnPmzEH9QKpD38xsgOzfv3+oW+iVQ9/MrE5f//rXOfPMM7n44ouZN28eixcvZubMmXzta1/jIx/5CN/5znfo6uri6quvZtq0aUybNo2nn34agN/+9rd8+tOfZtq0aXzoQx9i1apVALz11ltce+21tLW1cc011/DWW28BcM8993DDDTcceuy7776bL33pS/3+HnzKppkddW55dDNbtr8xoPs8+/2jufmKyT2uLxaLPPzww6xfv579+/czZcoUpk6dCsDrr7/OU089BcAnP/lJbrjhBs4//3xefPFFLr30UrZu3co3vvENZs2axfLly3n99deZPn06H/vYx7jrrrs44YQT6OzspLOzkylTpgAc+kVw6623Mnz4cO69917uuuuufn+fDn0zszr85Cc/Yc6cOYwcORKAK6644tC6a6655tDtH//4x2zZsuXQ/TfeeIPdu3fz5JNP8sgjj7B48WKgfArqiy++yJo1a/jiF78IQFtbG21tbQCMGjWKWbNm8dhjj3HWWWexb98+zj333H5/Hw59MzvqHO4V+WCJiB7XjRo16tDtgwcPsnbt2kO/HCq3f/jhhznjjDPetX1Pp11+9rOf5Zvf/CZnnnkmn/rUpxrs/J08p29mVofzzz+fRx99lL1797Jnzx4ef/zxmuMuueQS7rzzzkP3N2zYAMCll17KHXfcceiXx/r16wG48MILue+++wDYtGkTnZ2dh7Y977zzeOmll7j//vuZN2/egHwfDn0zszpMmzaNK6+8kg9+8IN8/OMfp1Ao8L73ve9d45YsWUKxWKStrY2zzz6b7373uwDcdNNN7Nu3j7a2Ns455xxuuukmABYsWMCePXsOzd9Pnz79Hfv7xCc+wYc//GHGjBkzIN+HDvcny5GgUCiEL6JiZlu3buWss84a0h727NnDe9/7Xt58800uvPBCli1bduiN18Fy+eWXc8MNN3DRRRf1OKbWsZG0LiIK1WP9St/MrE4dHR20t7czZcoUrr766kEN/Ndff51JkyYxcuTIwwZ+X/X6Rq6k5cDlwM6IOKei/gVgIbAfeDwi/rxi3SnAFmBRRCxOtanACmAksBr4kzjS/8wwM6tw//33N+2xTjrpJJ577rkB3289r/RXALMrC5I+CswB2iJiMrC4apvbgSeqakuBDmBiWmZjZtYHfp34bn09Jr2GfkSsAXZVlRcA34qIt9OYnd0rJM0FfgNsrqiNA0ZHxNr06n4lMLdPnZpZ1kaMGMFrr73m4K/Q/f/pjxgxou5tGj1PfxJwgaRvAHuBL0fEzySNAr4CXAx8uWL8eKBUcb+UajVJ6qD8VwGnnHJKgy2a2bGktbWVUqlEV1fXULdyROm+cla9Gg39YcAYYAYwDXhI0mnALcDtEbGn6sMGtT550OOv64hYBiyD8tk7DfZoZseQ4cOH1311KOtZo6FfAn6QpmqelXQQGAucB/yRpFuBk4CDkvYCDwOVv4page2Nt21mZo1oNPR/BMwC/rekScBxwKsRcUH3AEmLgD0RcWe6v1vSDOAZYD5wR38aNzOzvuv1jVxJDwBrgTMklSR9BlgOnCZpE/AgcF0dp18uAP4W2AY8z7vP7jEzs0HW6yv9iOjpP3z4r71st6jqfhE4p/ZoMzNrBn8i18wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy0g9V85aLmlnukpWZf0Lkn4laXO6Ji6SLpa0TtLG9HVWxfipqb5N0hJVXTndzMwGXz2v9FcAsysLkj4KzAHaImIysDitehW4IiLOBa4Dvl+x2VKgA5iYlnfs08zMBl+voR8Ra4BdVeUFwLci4u00Zmf6uj4itqcxm4ERko6XNA4YHRFr07V0VwJzB+qbMDOz+jQ6pz8JuEDSM5KekjStxpirgfXpF8N4oFSxrpRqNUnqkFSUVOzq6mqwRTMzq9Zo6A8DxgAzgD8DHqqco5c0Gfg2cH13qcY+oqedR8SyiChERKGlpaXBFs3MrFqjoV8CfhBlzwIHgbEAklqBHwLzI+L5ivGtFdu3AtsxM7OmajT0fwTMApA0CTgOeFXSScDjwI0R8XT34IjYAeyWNCP9RTAfWNWvzs3MrM/qOWXzAWAtcIakkqTPAMuB09JpnA8C16U3aBcCfwDcJGlDWn4/7WoB8LfANuB54ImB/3bMzOxwVM7qI1ehUIhisTjUbZiZHVUkrYuIQnXdn8g1M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwy4tA3M8uIQ9/MLCMOfTOzjDj0zcwyUs+Vs5ZL2pmuklVZ/4KkX0naLOnWivqNkraldZdW1KdK2pjWLam8kLqZmTVHPa/0VwCzKwuSPgrMAdoiYjKwONXPBq4FJqdt/kbSe9JmS4EOYGJa3rFPMzMbfL2GfkSsAXZVlRcA34qIt9OYnak+B3gwIt6OiH+hfD3c6ZLGAaMjYm26lu5KYO5AfRNmZlafRuf0JwEXSHpG0lOSpqX6eOClinGlVBufblfXa5LUIakoqdjV1dVgi2ZmVq3R0B8GjAFmAH8GPJTm6GvN08dh6jVFxLKIKEREoaWlpcEWzcysWqOhXwJ+EGXPAgeBsan+gYpxrcD2VG+tUTczsyZqNPR/BMwCkDQJOA54FXgEuFbS8ZJOpfyG7bMRsQPYLWlG+otgPrCq392bmVmfDOttgKQHgJnAWEkl4GZgObA8ncb5O+C69AbtZkkPAVuA/cDnI+JA2tUCymcCjQSeSIuZmTWRyll95CoUClEsFoe6DTOzo4qkdRFRqK77E7lmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGeg19Scsl7UxXyequLZL0sqQNabks1YdL+p6kjZK2SrqxYpupqb5N0pJ02UQzM2uiel7prwBm16jfHhHtaVmdav8FOD4izgWmAtdLmpDWLQU6KF83d2IP+zQzs0HUa+hHxBpgV537C2CUpGGUr4X7O+ANSeOA0RGxNl1LdyUwt8GezcysQf2Z018oqTNN/4xJtf8F/BbYAbwILI6IXcB4oFSxbSnVapLUIakoqdjV1dWPFs3MrFKjob8UOB1opxzwt6X6dOAA8H7gVOC/SToNqDV/3+MV2SNiWUQUIqLQ0tLSYItmZlatodCPiFci4kBEHATuphz2AJ8E/j4i9kXETuBpoED5lX1rxS5age2Nt21mZo1oKPTTHH23q4DuM3teBGapbBQwA/hlROwAdkuakc7amQ+s6kffZmbWgGG9DZD0ADATGCupBNwMzJTUTnmK5gXg+jT8r4F7Kf8SEHBvRHSmdQsonwk0EngiLWZm1kS9hn5EzKtRvqeHsXson7ZZa10ROKdP3ZmZ2YDyJ3LNzDLi0Dczy4hD38wsIw59M7OM9PpG7tHqlkc3s2X7G0PdhplZQ85+/2huvmLygO/Xr/TNzDJyzL7SH4zfkGZmRzu/0jczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy0ivoS9puaSdkjZV1BZJelnShrRcVrGuTdJaSZslbZQ0ItWnpvvbJC1Jl000M7MmqueV/gpgdo367RHRnpbVAJKGAf8D+FxETKZ8mcV9afxSoAOYmJZa+zQzs0HUa+hHxBpgV537uwTojIhfpG1fi4gD6ULqoyNibUQEsBKY22jTZmbWmP7M6S+U1Jmmf8ak2iQgJP2DpJ9L+vNUHw+UKrYtpVpNkjokFSUVu7q6+tGimZlVajT0lwKnA+3ADuC2VB8GnA/8cfp6laSLgFrz99HTziNiWUQUIqLQ0tLSYItmZlatodCPiFci4kBEHATuBqanVSXgqYh4NSLeBFYDU1K9tWIXrcD2xts2M7NGNBT6aY6+21VA95k9/wC0STohvan7EWBLROwAdkuakc7amQ+s6kffZmbWgF4voiLpAcpn4YyVVAJuBmZKaqc8RfMCcD1ARPxfSX8F/CytWx0Rj6ddLaB8JtBI4Im0mJlZE6l8Ms2Rq1AoRLFYHOo2zMyOKpLWRUShuu5P5JqZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhnpNfQlLZe0U9KmitoiSS9L2pCWy6q2OUXSHklfrqhNlbRR0jZJS9JlE83MrInqeaW/Aphdo357RLSnZXX1Ot59OcSlQAcwMS219mlmZoOo19CPiDXArnp3KGku8Btgc0VtHDA6ItZG+fqMK4G5fW/XzMz6oz9z+gsldabpnzEAkkYBXwFuqRo7HihV3C+lmpmZNVGjob8UOB1oB3YAt6X6LZSnffZUja81f9/jFdkldUgqSip2dXU12KKZmVUb1shGEfFK921JdwOPpbvnAX8k6VbgJOCgpL3Aw0BrxS5age2H2f8yYBlAoVDo8ZeDmZn1TUOhL2lcROxId68CNgFExAUVYxYBeyLiznR/t6QZwDPAfOCOfvRtZmYN6DX0JT0AzATGSioBNwMzJbVTnqJ5Abi+jsdaQPlMoJGUz+ypPrvHzMwGWa+hHxHzapTvqWO7RVX3i8A5dXdmZmYDzp/INTPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLSK+hL2m5pJ2SNlXUFkl6WdKGtFyW6hdLWidpY/o6q2Kbqam+TdISSRqcb8nMzHpSzyv9FcDsGvXbI6I9LatT7VXgiog4F7gO+H7F+KVABzAxLbX2aWZmg6jX0I+INcCuenYWEesjYnu6uxkYIel4SeOA0RGxNiICWAnMbbRpMzNrTH/m9BdK6kzTP2NqrL8aWB8RbwPjgVLFulKq1SSpQ1JRUrGrq6sfLZqZWaVGQ38pcDrQDuwAbqtcKWky8G3g+u5SjX1ETzuPiGURUYiIQktLS4MtmplZtYZCPyJeiYgDEXEQuBuY3r1OUivwQ2B+RDyfyiWgtWIXrcB2zMysqRoK/TRH3+0qYFOqnwQ8DtwYEU93D4iIHcBuSTPSWTvzgVUNd21mZg0Z1tsASQ8AM4GxkkrAzcBMSe2Up2he4P9P4ywE/gC4SdJNqXZJROwEFlA+E2gk8ERazMysiVQ+mebIVSgUolgsDnUbZmZHFUnrIqJQXfcncs3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4w49M3MMuLQNzPLiEPfzCwjDn0zs4z0GvqSlkvaKWlTRW2RpJclbUjLZRXrbpS0TdKvJF1aUZ8qaWNatyRdNtHMzJqonlf6K4DZNeq3R0R7WlYDSDobuBaYnLb5G0nvSeOXAh3AxLTU2qeZmQ2iXkM/ItYAu+rc3xzgwYh4OyL+BdgGTE8XUh8dEWujfH3GlcDcRps2M7PG9GdOf6GkzjT9MybVxgMvVYwppdr4dLu6XpOkDklFScWurq5+tGhmZpUaDf2lwOlAO7ADuC3Va83Tx2HqNUXEsogoREShpaWlwRbNzKxaQ6EfEa9ExIGIOAjcDUxPq0rAByqGtgLbU721Rt3MzJqoodBPc/TdrgK6z+x5BLhW0vGSTqX8hu2zEbED2C1pRjprZz6wqh99m5lZA4b1NkDSA8BMYKykEnAzMFNSO+UpmheA6wEiYrOkh4AtwH7g8xFxIO1qAeUzgUYCT6TFzMyaSOWTaY5chUIhisXiULdhZnZUkbQuIgrVdX8i18wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0Dczy4hD38wsI0f8/7IpqQv41wY3Hwu8OoDtDBT31Tfuq2/cV98cq339x4h416UHj/jQ7w9JxVr/tehQc1994776xn31TW59eXrHzCwjDn0zs4wc66G/bKgb6IH76hv31Tfuq2+y6uuYntM3M7N3OtZf6ZuZWQWHvplZRo6J0Jc0W9KvJG2T9NUa6yVpSVrfKWlKE3r6gKR/lrRV0mZJf1JjzExJ/yZpQ1r+crD7So/7gqSN6TGLNdYPxfE6o+I4bJD0hqQ/rRrTlOMlabmknZI2VdR+T9I/Svp1+jqmh20P+1wchL7+u6Rfpp/TDyWd1MO2h/2ZD0JfiyS9XPGzuqyHbZt9vP6uoqcXJG3oYdvBPF41s6Fpz7GIOKoX4D3A88BpwHHAL4Czq8ZcBjwBCJgBPNOEvsYBU9LtE4HnavQ1E3hsCI7ZC8DYw6xv+vGq8TP9P5Q/XNL04wVcCEwBNlXUbgW+mm5/Ffh2I8/FQejrEmBYuv3tWn3V8zMfhL4WAV+u4+fc1ONVtf424C+H4HjVzIZmPceOhVf604FtEfGbiPgd8CAwp2rMHGBllP0UOEnSuMFsKiJ2RMTP0+3dwFZg/GA+5gBq+vGqchHwfEQ0+knsfomINcCuqvIc4Hvp9veAuTU2ree5OKB9RcSTEbE/3f0p0DpQj9efvurU9OPVTZKATwAPDNTj1esw2dCU59ixEPrjgZcq7pd4d7jWM2bQSJoAfAh4psbq/yTpF5KekDS5SS0F8KSkdZI6aqwf0uMFXEvP/xiH4ngB/PuI2AHlf7TA79cYM9TH7dOU/0Krpbef+WBYmKadlvcwVTGUx+sC4JWI+HUP65tyvKqyoSnPsWMh9FWjVn0eaj1jBoWk9wIPA38aEW9Urf455SmMDwJ3AD9qRk/AhyNiCvCHwOclXVi1fiiP13HAlcD/rLF6qI5XvYbyuP0FsB+4r4chvf3MB9pS4HSgHdhBeSql2pAdL2Aeh3+VP+jHq5ds6HGzGrU+HbNjIfRLwAcq7rcC2xsYM+AkDaf8Q70vIn5QvT4i3oiIPen2amC4pLGD3VdEbE9fdwI/pPwnY6UhOV7JHwI/j4hXqlcM1fFKXume4kpfd9YYM1TPs+uAy4E/jjTxW62On/mAiohXIuJARBwE7u7h8YbqeA0DPg78XU9jBvt49ZANTXmOHQuh/zNgoqRT06vEa4FHqsY8AsxPZ6XMAP6t+8+owZLmDO8BtkbEX/Uw5j+kcUiaTvnn8dog9zVK0ondtym/EbipaljTj1eFHl+BDcXxqvAIcF26fR2wqsaYep6LA0rSbOArwJUR8WYPY+r5mQ90X5XvAV3Vw+M1/XglHwN+GRGlWisH+3gdJhua8xwbjHenm71QPtvkOcrvav9Fqn0O+Fy6LeCv0/qNQKEJPZ1P+c+uTmBDWi6r6mshsJnyO/A/Bf5zE/o6LT3eL9JjHxHHKz3uCZRD/H0VtaYfL8q/dHYA+yi/svoMcDLwT8Cv09ffS2PfD6w+3HNxkPvaRnmOt/s59t3qvnr6mQ9yX99Pz51OyqE07kg4Xqm+ovs5VTG2mcerp2xoynPM/w2DmVlGjoXpHTMzq5ND38wsIw59M7OMOPTNzDLi0Dczy4hD38wsIw59M7OM/D/uc0mqN/hN9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"greedy_cer\" in history_validation:\n",
    "    plt.plot(history_validation[\"epoch\"],\n",
    "             history_validation[\"greedy_cer\"], label=\"greedy\")\n",
    "if \"viterbi_cer\" in history_validation:\n",
    "    plt.plot(history_validation[\"epoch\"],\n",
    "             history_validation[\"viterbi_cer\"], label=\"viterbi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f996c2fbd50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"greedy_wer\" in history_validation:\n",
    "    plt.plot(history_validation[\"epoch\"],\n",
    "             history_validation[\"greedy_wer\"], label=\"greedy\")\n",
    "if \"viterbi_wer\" in history_validation:\n",
    "    plt.plot(history_validation[\"epoch\"],\n",
    "             history_validation[\"viterbi_wer\"], label=\"viterbi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9817d29e50>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"greedy_cer_normalized\" in history_validation:\n",
    "    plt.plot(history_validation[\"epoch\"],\n",
    "             history_validation[\"greedy_cer_normalized\"], label=\"greedy\")\n",
    "if \"viterbi_cer_normalized\" in history_validation:\n",
    "    plt.plot(history_validation[\"epoch\"],\n",
    "             history_validation[\"viterbi_cer_normalized\"], label=\"viterbi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f996c128210>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if \"greedy_wer_normalized\" in history_validation:\n",
    "    plt.plot(history_validation[\"epoch\"],\n",
    "             history_validation[\"greedy_wer_normalized\"], label=\"greedy\")\n",
    "if \"viterbi_wer_normalized\" in history_validation:\n",
    "    plt.plot(history_validation[\"epoch\"],\n",
    "             history_validation[\"viterbi_wer_normalized\"], label=\"viterbi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f996c4e9050>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_training[\"epoch\"],\n",
    "         history_training[\"sum_loss\"], label=\"training\")\n",
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"sum_loss\"], label=\"validation\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9844c651d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_training[\"epoch\"],\n",
    "         history_training[\"sum_loss\"], label=\"training\")\n",
    "plt.plot(history_validation[\"epoch\"],\n",
    "         history_validation[\"sum_loss\"], label=\"validation\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  366158 KB |    2689 MB |     891 TB |     891 TB |\n",
      "|       from large pool |  363332 KB |    2686 MB |     890 TB |     890 TB |\n",
      "|       from small pool |    2826 KB |       4 MB |       1 TB |       1 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  366158 KB |    2689 MB |     891 TB |     891 TB |\n",
      "|       from large pool |  363332 KB |    2686 MB |     890 TB |     890 TB |\n",
      "|       from small pool |    2826 KB |       4 MB |       1 TB |       1 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    8280 MB |    8280 MB |   21866 MB |   13586 MB |\n",
      "|       from large pool |    8274 MB |    8274 MB |   21858 MB |   13584 MB |\n",
      "|       from small pool |       6 MB |       6 MB |       8 MB |       2 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   41394 KB |     986 MB |     850 TB |     850 TB |\n",
      "|       from large pool |   40124 KB |     985 MB |     848 TB |     848 TB |\n",
      "|       from small pool |    1270 KB |       3 MB |       1 TB |       1 TB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      73    |     103    |   45307 K  |   45307 K  |\n",
      "|       from large pool |      31    |      51    |   24617 K  |   24617 K  |\n",
      "|       from small pool |      42    |      59    |   20690 K  |   20690 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      73    |     103    |   45307 K  |   45307 K  |\n",
      "|       from large pool |      31    |      51    |   24617 K  |   24617 K  |\n",
      "|       from small pool |      42    |      59    |   20690 K  |   20690 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      37    |      37    |      93    |      56    |\n",
      "|       from large pool |      34    |      34    |      89    |      55    |\n",
      "|       from small pool |       3    |       3    |       4    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      13    |      23    |   18057 K  |   18057 K  |\n",
      "|       from large pool |       8    |      19    |   12153 K  |   12153 K  |\n",
      "|       from small pool |       5    |       7    |    5904 K  |    5904 K  |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2600897995 function calls (2495912391 primitive calls) in 165152.601 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 3190 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       30    0.001    0.000 165152.637 5505.088 base_events.py:1686(_run_once)\n",
      "   199/25    0.002    0.000 165147.282 6605.891 gen.py:716(run)\n",
      "   342/97    0.000    0.000 165147.282 1702.549 {method 'send' of 'generator' objects}\n",
      "   265/84    0.004    0.000 165147.274 1966.039 gen.py:184(wrapper)\n",
      "      228    0.000    0.000 165147.267  724.330 kernelbase.py:351(process_one)\n",
      "      131    0.002    0.000 165147.264 1260.666 kernelbase.py:225(dispatch_shell)\n",
      "517695/218    0.641    0.000 165147.212  757.556 {built-in method builtins.next}\n",
      "      551    0.001    0.000 165147.205  299.723 events.py:86(_run)\n",
      "      551    0.000    0.000 165147.205  299.723 {method 'run' of 'Context' objects}\n",
      "      366    0.000    0.000 165147.204  451.222 ioloop.py:735(_run_callback)\n",
      "       19    0.000    0.000 165147.194 8691.958 ioloop.py:690(<lambda>)\n",
      "       19    0.000    0.000 165147.194 8691.958 gen.py:784(inner)\n",
      "      113    0.002    0.000 165147.148 1461.479 kernelbase.py:516(execute_request)\n",
      "       56    0.001    0.000 165147.032 2949.054 ipkernel.py:262(do_execute)\n",
      "       56    0.000    0.000 165146.878 2949.051 zmqshell.py:534(run_cell)\n",
      "       56    0.000    0.000 165146.878 2949.051 interactiveshell.py:2831(run_cell)\n",
      "       56    0.000    0.000 165144.730 2949.013 interactiveshell.py:2865(_run_cell)\n",
      "       56    0.000    0.000 165144.624 2949.011 async_helpers.py:58(_pseudo_sync_runner)\n",
      "       56    0.001    0.000 165144.624 2949.011 {method 'send' of 'coroutine' objects}\n",
      "       56    0.003    0.000 165144.622 2949.011 interactiveshell.py:2920(run_cell_async)\n",
      "\n",
      "\n",
      "\n",
      "stop time: 2020-04-27 13:40:14.328514\n"
     ]
    }
   ],
   "source": [
    "# Print performance\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "ps = (\n",
    "    pstats\n",
    "    .Stats(pr, stream=s)\n",
    "    .strip_dirs()\n",
    "    .sort_stats(\"cumtime\")\n",
    "    .print_stats(20)\n",
    ")\n",
    "print(s.getvalue(), flush=True)\n",
    "print(\"stop time: {}\".format(str(datetime.now())), flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
