{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb\n",
    "\n",
    "\n",
    "import collections\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn, topk\n",
    "from torch.optim import Adadelta\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "audio_backend = \"soundfile\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "non_blocking = pin_memory\n",
    "\n",
    "labels = [\n",
    "        \"-\",\n",
    "        \"*\",\n",
    "        \"backward\",\n",
    "        \"bed\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"dog\",\n",
    "        \"down\",\n",
    "        \"eight\",\n",
    "        \"five\",\n",
    "        \"follow\",\n",
    "        \"forward\",\n",
    "        \"four\",\n",
    "        \"go\",\n",
    "        \"happy\",\n",
    "        \"house\",\n",
    "        \"learn\",\n",
    "        \"left\",\n",
    "        \"marvin\",\n",
    "        \"nine\",\n",
    "        \"no\",\n",
    "        \"off\",\n",
    "        \"on\",\n",
    "        \"one\",\n",
    "        \"right\",\n",
    "        \"seven\",\n",
    "        \"sheila\",\n",
    "        \"six\",\n",
    "        \"stop\",\n",
    "        \"three\",\n",
    "        \"tree\",\n",
    "        \"two\",\n",
    "        \"up\",\n",
    "        \"visual\",\n",
    "        \"wow\",\n",
    "        \"yes\",\n",
    "        \"zero\",\n",
    "]\n",
    "vocab_size = len(labels) + 2\n",
    "\n",
    "# audio, self.sr, window_stride=(160, 80), fft_size=512, num_filt=20, num_coeffs=13\n",
    "n_mfcc = 13\n",
    "melkwargs = {\n",
    "    'n_fft': 512,\n",
    "    'n_mels': 20,\n",
    "    'hop_length': 80,\n",
    "}\n",
    "sample_rate = 16000\n",
    "\n",
    "batch_size = 512  # max number of sentences per batch\n",
    "optimizer_params = {\n",
    "    \"lr\": 1.0,\n",
    "    \"eps\": 1e-8,\n",
    "    \"rho\": 0.95,\n",
    "}\n",
    "\n",
    "max_epoch = 80\n",
    "clip_norm = 0.\n",
    "\n",
    "dtstamp = datetime.now().strftime(\"%y%m%d.%H%M%S\")\n",
    "\n",
    "torchaudio.set_audio_backend(audio_backend)\n",
    "mfcc = MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc, melkwargs=melkwargs)\n",
    "\n",
    "\n",
    "class Coder:\n",
    "    def __init__(self, labels):\n",
    "        self.max_length = max(map(len, labels))\n",
    "\n",
    "        labels = list(collections.OrderedDict.fromkeys(list(\"\".join(labels))))\n",
    "        enumerated = list(enumerate(labels))\n",
    "        flipped = [(sub[1], sub[0]) for sub in enumerated]\n",
    "\n",
    "        d1 = collections.OrderedDict(enumerated)\n",
    "        d2 = collections.OrderedDict(flipped)\n",
    "        self.mapping = {**d1, **d2}\n",
    "\n",
    "    def _map_and_pad(self, iterable, fillwith):\n",
    "        iterable = [self.mapping[i] for i in iterable]  # map with dict\n",
    "        iterable += [fillwith] * (self.max_length-len(iterable))  # add padding\n",
    "        return iterable\n",
    "\n",
    "    def encode(self, iterable, device):\n",
    "        if isinstance(iterable[0], list):\n",
    "            return torch.stack([self.encode(i) for i in iterable])\n",
    "        else:\n",
    "            iterable = self._map_and_pad(iterable, self.mapping[\"*\"])\n",
    "            return torch.tensor(iterable, dtype=torch.long, device=device)\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        if hasattr(tensor, \"tolist\"):\n",
    "            tensor = tensor.tolist()\n",
    "        if isinstance(tensor[0], list):\n",
    "            return [self.decode(t) for t in tensor]\n",
    "        else:\n",
    "            return self._map_and_pad(tensor, self.mapping[1])\n",
    "\n",
    "\n",
    "coder = Coder(labels)\n",
    "encode = coder.encode\n",
    "decode = coder.decode\n",
    "\n",
    "\n",
    "def process_datapoint(item):\n",
    "    waveform = item[0]\n",
    "    target = item[2]\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    specgram = mfcc(waveform)[0, ...].transpose(0, -1)\n",
    "    target = encode(target, device=specgram.device)\n",
    "    return specgram, target\n",
    "\n",
    "\n",
    "class PROCESSED_SPEECHCOMMANDS(SPEECHCOMMANDS):\n",
    "    def __getitem__(self, n):\n",
    "        return process_datapoint(super().__getitem__(n))\n",
    "\n",
    "    def __next__(self):\n",
    "        return process_datapoint(super().__next__())\n",
    "\n",
    "\n",
    "class MemoryCache(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a dataset so that, whenever a new item is returned, it is saved to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self._id = id(self)\n",
    "        self._cache = [None] * len(dataset)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        if self._cache[n]:\n",
    "            return self._cache[n]\n",
    "\n",
    "        item = self.dataset[n]\n",
    "        self._cache[n] = item\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def datasets():\n",
    "    root = \"./\"\n",
    "\n",
    "    dataset = PROCESSED_SPEECHCOMMANDS(root, download=True)\n",
    "    dataset = MemoryCache(dataset)\n",
    "    # dataset = SPEECHCOMMANDS(root, download=download)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    tensors = [b[0] for b in batch if b]\n",
    "    targets = [b[1] for b in batch if b]\n",
    "\n",
    "    # tensors = [process_waveform(b[0]) for b in batch if b]\n",
    "    # targets = [process_target(b[2]) for b in batch if b]\n",
    "\n",
    "    # truncate tensor list\n",
    "    # length = 2**10\n",
    "    # a = max(0, min([tensor.shape[-1] for tensor in tensors]) - length)\n",
    "    # m = randint(0, a)\n",
    "    # n = m + length\n",
    "    # tensors = [t[..., m:n] for t in tensors]\n",
    "\n",
    "    input_lengths = [t.shape[0] for t in tensors]\n",
    "    target_lengths = [len(t) for t in targets]\n",
    "\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "\n",
    "    return tensors, targets, input_lengths, target_lengths\n",
    "\n",
    "\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Letter(nn.Module):\n",
    "    \"\"\"Wav2Letter Speech Recognition model\n",
    "        https://arxiv.org/pdf/1609.03193.pdf\n",
    "        This specific architecture accepts mfcc or power spectrums speech signals\n",
    "\n",
    "        Args:\n",
    "            num_features (int): number of mfcc features\n",
    "            num_classes (int): number of unique grapheme class labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Wav2Letter, self).__init__()\n",
    "\n",
    "        # Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.layers = nn.Sequential(\n",
    "            # PrintLayer(),\n",
    "            nn.Conv1d(num_features, 250, 48, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 2000, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, 2000, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass through Wav2Letter network than\n",
    "            takes log probability of output\n",
    "        Args:\n",
    "            batch (int): mini batch of data\n",
    "            shape (batch, num_features, frame_len)\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, num_classes, output_len)\n",
    "        \"\"\"\n",
    "        # y_pred shape (batch_size, num_classes, output_len)\n",
    "        y_pred = self.layers(batch)\n",
    "\n",
    "        # compute log softmax probability on graphemes\n",
    "        log_probs = nn.functional.log_softmax(y_pred, dim=1)\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "def greedy_decoder(outputs):\n",
    "    \"\"\"Greedy Decoder. Returns highest probability of class labels for each timestep\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): shape (1, num_classes, output_len)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: class labels per time step.\n",
    "    \"\"\"\n",
    "    _, indices = topk(outputs, k=1, dim=1)\n",
    "    return indices[:, 0, :]\n",
    "\n",
    "\n",
    "train = datasets()\n",
    "loader_train = DataLoader(\n",
    "    train, batch_size=batch_size, collate_fn=collate_fn, shuffle=True,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "model = Wav2Letter(n_mfcc, vocab_size)\n",
    "model = torch.jit.script(model)\n",
    "model = model.to(device, non_blocking=non_blocking)\n",
    "\n",
    "optimizer = Adadelta(model.parameters(), **optimizer_params)\n",
    "criterion = torch.nn.CTCLoss()\n",
    "\n",
    "# Profiling performance\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "best_loss = 1.\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "\n",
    "    for inputs, targets, _, _ in tqdm(loader_train):\n",
    "\n",
    "        inputs = inputs.to(device, non_blocking=non_blocking)\n",
    "        targets = targets.to(device, non_blocking=non_blocking)\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.transpose(1, 2).transpose(0, 1)\n",
    "\n",
    "        this_batch_size = len(inputs)\n",
    "\n",
    "        input_lengths = torch.full(\n",
    "            (this_batch_size,), outputs.shape[0], dtype=torch.long, device=outputs.device\n",
    "        )\n",
    "        target_lengths = torch.tensor(\n",
    "            [target.shape[0] for target in targets], dtype=torch.long, device=targets.device\n",
    "        )\n",
    "\n",
    "        # CTC\n",
    "        # https://pytorch.org/docs/master/nn.html#torch.nn.CTCLoss\n",
    "        # https://discuss.pytorch.org/t/ctcloss-with-warp-ctc-help/8788/3\n",
    "        # outputs: input length, batch size, number of classes (including blank)\n",
    "        # targets: batch size, max target length\n",
    "        # input_lengths: batch size\n",
    "        # target_lengths: batch size\n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if clip_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(epoch, loss)\n",
    "\n",
    "    if (loss < best_loss).all():\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), f\"./model.{dtstamp}.{epoch}.ph\")\n",
    "        best_loss = loss\n",
    "\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), f\"./model.{dtstamp}.{epoch}.ph\")\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "sample = inputs[0].unsqueeze(0).to(device, non_blocking=non_blocking)\n",
    "target = targets[0].to(device, non_blocking=non_blocking)\n",
    "\n",
    "print(targets[0])\n",
    "print(decode(targets[0]))\n",
    "\n",
    "output = model(sample)\n",
    "output = greedy_decoder(output)\n",
    "\n",
    "print(output)\n",
    "print(decode(output[0]))\n",
    "\n",
    "# Print performance\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).strip_dirs().sort_stats(\"cumtime\").print_stats(20)\n",
    "print(s.getvalue())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
