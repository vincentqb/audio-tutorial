{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import cProfile\n",
    "import hashlib\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pstats\n",
    "import string\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn, topk\n",
    "from torch.optim import Adadelta, Adam, SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import SPEECHCOMMANDS, LIBRISPEECH\n",
    "from torchaudio.transforms import MFCC, Resample\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Empty CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Profiling performance\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_backend = \"soundfile\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "non_blocking = pin_memory\n",
    "\n",
    "labels = [\n",
    "        \"-\",\n",
    "        \"*\",\n",
    "        \"backward\",\n",
    "        \"bed\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"dog\",\n",
    "        \"down\",\n",
    "        \"eight\",\n",
    "        \"five\",\n",
    "        \"follow\",\n",
    "        \"forward\",\n",
    "        \"four\",\n",
    "        \"go\",\n",
    "        \"happy\",\n",
    "        \"house\",\n",
    "        \"learn\",\n",
    "        \"left\",\n",
    "        \"marvin\",\n",
    "        \"nine\",\n",
    "        \"no\",\n",
    "        \"off\",\n",
    "        \"on\",\n",
    "        \"one\",\n",
    "        \"right\",\n",
    "        \"seven\",\n",
    "        \"sheila\",\n",
    "        \"six\",\n",
    "        \"stop\",\n",
    "        \"three\",\n",
    "        \"tree\",\n",
    "        \"two\",\n",
    "        \"up\",\n",
    "        \"visual\",\n",
    "        \"wow\",\n",
    "        \"yes\",\n",
    "        \"zero\",\n",
    "]\n",
    "vocab_size = len(string.ascii_lowercase) + 2\n",
    "\n",
    "\n",
    "# audio, self.sr, window_stride=(160, 80), fft_size=512, num_filt=20, num_coeffs=13\n",
    "n_mfcc = 13\n",
    "melkwargs = {\n",
    "    'n_fft': 512,\n",
    "    'n_mels': 20,\n",
    "    'hop_length': 80,\n",
    "}\n",
    "sample_rate = 16000\n",
    "\n",
    "batch_size = 512  # max number of sentences per batch\n",
    "optimizer_params = {\n",
    "    \"lr\": 1.0,\n",
    "    \"eps\": 1e-8,\n",
    "    \"rho\": 0.95,\n",
    "}\n",
    "\n",
    "max_epoch = 80\n",
    "clip_norm = 0.\n",
    "\n",
    "dtstamp = datetime.now().strftime(\"%y%m%d.%H%M%S\")\n",
    "\n",
    "torchaudio.set_audio_backend(audio_backend)\n",
    "mfcc = MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc, melkwargs=melkwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coder:\n",
    "    def __init__(self, labels):\n",
    "        self.max_length = max(map(len, labels))\n",
    "\n",
    "        labels = list(collections.OrderedDict.fromkeys(list(\"\".join(labels))))\n",
    "        enumerated = list(enumerate(labels))\n",
    "        flipped = [(sub[1], sub[0]) for sub in enumerated]\n",
    "\n",
    "        d1 = collections.OrderedDict(enumerated)\n",
    "        d2 = collections.OrderedDict(flipped)\n",
    "        self.mapping = {**d1, **d2}\n",
    "\n",
    "    def _map_and_pad(self, iterable, fillwith):\n",
    "        iterable = [self.mapping[i] for i in iterable]  # map with dict\n",
    "        iterable += [fillwith] * (self.max_length-len(iterable))  # add padding\n",
    "        return iterable\n",
    "\n",
    "    def encode(self, iterable, device):\n",
    "        if isinstance(iterable[0], list):\n",
    "            return torch.stack([self.encode(i) for i in iterable])\n",
    "        else:\n",
    "            iterable = self._map_and_pad(iterable, self.mapping[\"*\"])\n",
    "            return torch.tensor(iterable, dtype=torch.long, device=device)\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        if hasattr(tensor, \"tolist\"):\n",
    "            tensor = tensor.tolist()\n",
    "        if isinstance(tensor[0], list):\n",
    "            return [self.decode(t) for t in tensor]\n",
    "        else:\n",
    "            return self._map_and_pad(tensor, self.mapping[1])\n",
    "\n",
    "\n",
    "coder = Coder(labels)\n",
    "encode = coder.encode\n",
    "decode = coder.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datapoint(item):\n",
    "    waveform = item[0]\n",
    "    target = item[2]\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    specgram = mfcc(waveform)[0, ...].transpose(0, -1)\n",
    "    target = encode(target, device=specgram.device)\n",
    "    return specgram, target\n",
    "\n",
    "\n",
    "class PROCESSED_SPEECHCOMMANDS(SPEECHCOMMANDS):\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        return process_datapoint(super().__getitem__(n))\n",
    "\n",
    "    def __next__(self):\n",
    "        return process_datapoint(super().__next__())\n",
    "\n",
    "\n",
    "class MemoryCache(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a dataset so that, whenever a new item is returned, it is saved to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self._id = id(self)\n",
    "        self._cache = [None] * len(dataset)\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        if self._cache[n]:\n",
    "            return self._cache[n]\n",
    "\n",
    "        item = self.dataset[n]\n",
    "        self._cache[n] = item\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def datasets():\n",
    "    root = \"./\"\n",
    "\n",
    "    dataset = PROCESSED_SPEECHCOMMANDS(root, download=True)\n",
    "    dataset = MemoryCache(dataset)\n",
    "    # dataset = SPEECHCOMMANDS(root, download=download)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    tensors = [b[0] for b in batch if b]\n",
    "    targets = [b[1] for b in batch if b]\n",
    "\n",
    "    # tensors = [process_waveform(b[0]) for b in batch if b]\n",
    "    # targets = [process_target(b[2]) for b in batch if b]\n",
    "\n",
    "    # truncate tensor list\n",
    "    # length = 2**10\n",
    "    # a = max(0, min([tensor.shape[-1] for tensor in tensors]) - length)\n",
    "    # m = randint(0, a)\n",
    "    # n = m + length\n",
    "    # tensors = [t[..., m:n] for t in tensors]\n",
    "\n",
    "    input_lengths = [t.shape[0] for t in tensors]\n",
    "    target_lengths = [len(t) for t in targets]\n",
    "\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "\n",
    "    return tensors, targets, input_lengths, target_lengths\n",
    "\n",
    "\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Letter(nn.Module):\n",
    "    \"\"\"Wav2Letter Speech Recognition model\n",
    "        https://arxiv.org/pdf/1609.03193.pdf\n",
    "        This specific architecture accepts mfcc or power spectrums speech signals\n",
    "\n",
    "        Args:\n",
    "            num_features (int): number of mfcc features\n",
    "            num_classes (int): number of unique grapheme class labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Wav2Letter, self).__init__()\n",
    "\n",
    "        # Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.layers = nn.Sequential(\n",
    "            # PrintLayer(),\n",
    "            nn.Conv1d(num_features, 250, 48, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 2000, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, 2000, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass through Wav2Letter network than\n",
    "            takes log probability of output\n",
    "        Args:\n",
    "            batch (int): mini batch of data\n",
    "            shape (batch, num_features, frame_len)\n",
    "        Returns:\n",
    "            Tensor with shape (batch_size, num_classes, output_len)\n",
    "        \"\"\"\n",
    "        # y_pred shape (batch_size, num_classes, output_len)\n",
    "        y_pred = self.layers(batch)\n",
    "\n",
    "        # compute log softmax probability on graphemes\n",
    "        log_probs = nn.functional.log_softmax(y_pred, dim=1)\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(outputs):\n",
    "    \"\"\"Greedy Decoder. Returns highest probability of class labels for each timestep\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): shape (1, num_classes, output_len)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: class labels per time step.\n",
    "    \"\"\"\n",
    "    _, indices = topk(outputs, k=1, dim=1)\n",
    "    return indices[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422adfb35fe747f7b2c29df3c1fce02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.5844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1 tensor(2.4498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2 tensor(2.3997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3 tensor(2.1675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4 tensor(1.9909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5 tensor(1.8520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6 tensor(1.7610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7 tensor(1.7455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-65ad25a49c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = datasets()\n",
    "loader_train = DataLoader(\n",
    "    train, batch_size=batch_size, collate_fn=collate_fn, shuffle=True,\n",
    "    num_workers=num_workers, pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "model = Wav2Letter(n_mfcc, vocab_size)\n",
    "model = torch.jit.script(model)\n",
    "model = model.to(device, non_blocking=non_blocking)\n",
    "\n",
    "optimizer = Adadelta(model.parameters(), **optimizer_params)\n",
    "criterion = torch.nn.CTCLoss()\n",
    "\n",
    "# Profiling performance\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "best_loss = 1.\n",
    "\n",
    "with tqdm(total=max_epoch, unit_scale=1) as pbar:\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "\n",
    "        for inputs, targets, _, _ in loader_train:\n",
    "\n",
    "            inputs = inputs.to(device, non_blocking=non_blocking)\n",
    "            targets = targets.to(device, non_blocking=non_blocking)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.transpose(1, 2).transpose(0, 1)\n",
    "\n",
    "            this_batch_size = len(inputs)\n",
    "\n",
    "            input_lengths = torch.full(\n",
    "                (this_batch_size,), outputs.shape[0], dtype=torch.long, device=outputs.device\n",
    "            )\n",
    "            target_lengths = torch.tensor(\n",
    "                [target.shape[0] for target in targets], dtype=torch.long, device=targets.device\n",
    "            )\n",
    "\n",
    "            # CTC\n",
    "            # https://pytorch.org/docs/master/nn.html#torch.nn.CTCLoss\n",
    "            # https://discuss.pytorch.org/t/ctcloss-with-warp-ctc-help/8788/3\n",
    "            # outputs: input length, batch size, number of classes (including blank)\n",
    "            # targets: batch size, max target length\n",
    "            # input_lengths: batch size\n",
    "            # target_lengths: batch size\n",
    "            loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "            optimizer.step()\n",
    "            pbar.update(1/len(loader_train))\n",
    "\n",
    "        print(epoch, loss)\n",
    "\n",
    "    if (loss < best_loss).all():\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), f\"./model.{dtstamp}.{epoch}.ph\")\n",
    "        best_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12, 14,  1,  1,  1,  1,  1,  1])\n",
      "['o', 'n', '*', '*', '*', '*', '*', '*']\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "['*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*']\n",
      "         45960178 function calls (45500977 primitive calls) in 2438.092 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1112 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "       12    0.000    0.000 2435.701  202.975 interactiveshell.py:3293(run_code)\n",
      "    16/12    0.000    0.000 2435.073  202.923 {built-in method builtins.exec}\n",
      "        1    0.241    0.241 2435.068 2435.068 <ipython-input-7-65ad25a49c28>:20(<module>)\n",
      "     1696    0.007    0.000 1805.129    1.064 dataloader.py:344(__next__)\n",
      "     1696    0.015    0.000 1805.122    1.064 dataloader.py:383(_next_data)\n",
      "     1688    0.007    0.000 1804.528    1.069 fetch.py:42(fetch)\n",
      "     1688    0.423    0.000 1770.201    1.049 fetch.py:44(<listcomp>)\n",
      "   863016    0.749    0.000 1769.778    0.002 <ipython-input-4-2815b0e6875b>:28(__getitem__)\n",
      "   105829    0.782    0.000 1769.029    0.017 <ipython-input-4-2815b0e6875b>:11(__getitem__)\n",
      "   105829    1.446    0.000 1724.536    0.016 <ipython-input-4-2815b0e6875b>:1(process_datapoint)\n",
      "532520/109204    7.447    0.000 1724.498    0.016 module.py:522(__call__)\n",
      "   105829    2.466    0.000 1718.665    0.016 transforms.py:426(forward)\n",
      "   105829    0.902    0.000 1690.290    0.016 transforms.py:367(forward)\n",
      "   105829    0.450    0.000 1661.641    0.016 transforms.py:63(forward)\n",
      "   105829    1.240    0.000 1661.098    0.016 functional.py:226(spectrogram)\n",
      "   105829    0.605    0.000 1625.488    0.015 functional.py:517(complex_norm)\n",
      "   105829    0.299    0.000 1617.289    0.015 functional.py:680(norm)\n",
      "   105829 1616.955    0.015 1616.955    0.015 {built-in method norm}\n",
      "     3377  461.232    0.137  461.232    0.137 {method 'to' of 'torch._C._TensorBase' objects}\n",
      "   107516  143.401    0.001  143.401    0.001 {built-in method tensor}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "# torch.save(model.state_dict(), f\"./model.{dtstamp}.{epoch}.ph\")\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "sample = inputs[0].unsqueeze(0).to(device, non_blocking=non_blocking)\n",
    "target = targets[0].to(device, non_blocking=non_blocking)\n",
    "\n",
    "print(targets[0])\n",
    "print(decode(targets[0]))\n",
    "\n",
    "output = model(sample)\n",
    "output = greedy_decoder(output)\n",
    "\n",
    "print(output)\n",
    "print(decode(output[0]))\n",
    "\n",
    "# Print performance\n",
    "pr.disable()\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).strip_dirs().sort_stats(\"cumtime\").print_stats(20)\n",
    "print(s.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
