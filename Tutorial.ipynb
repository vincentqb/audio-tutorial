{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchaudio Tutorial\n",
    "\n",
    "PyTorch is an open source deep learning platform that provides a seamless path from research prototyping to production deployment with GPU support.\n",
    "\n",
    "Significant effort in solving machine learning problems goes into data preparation. Torchaudio leverages PyTorch’s GPU support, and provides many tools to make data loading easy and more readable. In this tutorial, we will see how to load and preprocess data from a simple dataset.\n",
    "\n",
    "For this tutorial, please make sure the `matplotlib` package is installed for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchaudio supports loading sound files in the wav and mp3 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 276858])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"assets/steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n",
    "waveform, frequency = torchaudio.load(filename)\n",
    "\n",
    "waveform.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(waveform.transpose(0,1).numpy())\n",
    "plt.savefig('tutorial_original.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_original.png){ width=350 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "Torchaudio supports a growing list of [transformations](https://pytorch.org/audio/transforms.html). \n",
    "\n",
    "* **Scale**: Scale audio tensor from a 16-bit integer (represented as a FloatTensor) to a floating point number between -1.0 and 1.0.  Note the 16-bit number is called the \"bit depth\" or \"precision\", not to be confused with \"bit rate\".\n",
    "* **PadTrim**: PadTrim a 2d-Tensor\n",
    "* **Downmix**: Downmix any stereo signals to mono.\n",
    "* **LC2CL**: Permute a 2d tensor from samples (n x c) to (c x n).\n",
    "* **Resample**: Resample the signal to a different frequency.\n",
    "* **Spectrogram**: Create a spectrogram from a raw audio signal\n",
    "* **MelScale**: This turns a normal STFT into a mel frequency STFT, using a conversion matrix.  This uses triangular filter banks.\n",
    "* **SpectrogramToDB**: This turns a spectrogram from the power/amplitude scale to the decibel scale.\n",
    "* **MFCC**: Create the Mel-frequency cepstrum coefficients from an audio signal\n",
    "* **MelSpectrogram**: Create MEL Spectrograms from a raw audio signal using the STFT function in PyTorch.\n",
    "* **BLC2CBL**: Permute a 3d tensor from Bands x Sample length x Channels to Channels x Bands x Samples length.\n",
    "* **MuLawEncoding**: Encode signal based on mu-law companding. \n",
    "* **MuLawExpanding**: Decode mu-law encoded signal.\n",
    "\n",
    "Since all transforms are nn.Modules or jit.ScriptModules, they can be used as part of a neural network at any point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we can look at the log of the spectrogram on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1385, 201])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specgram = torchaudio.transforms.Spectrogram()(waveform)\n",
    "specgram.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(specgram.log2().transpose(1,2)[0,:,:].numpy(), cmap='gray')\n",
    "plt.savefig('tutorial_spectrogram.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_spectrogram.png){ width=350 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can look at the Mel Spectrogram on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1385, 128])\n"
     ]
    }
   ],
   "source": [
    "mel_specgram = torchaudio.transforms.MelSpectrogram()(waveform)\n",
    "print(mel_specgram.size())\n",
    "p = plt.imshow(mel_specgram.log2().transpose(1,2)[0,:,:].detach().numpy(), cmap='gray')\n",
    "plt.savefig('tutorial_melspectrogram.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_melspectrogram.png){ width=350 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can resample the signal, one channel at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27686])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_frequency = frequency/10\n",
    "\n",
    "# Since Resample applies to a single channel, we resample first channel here\n",
    "resampled = torchaudio.transforms.Resample(frequency, new_frequency)(waveform[0,:].view(1,-1))\n",
    "resampled.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(resampled[0,:].numpy())\n",
    "plt.savefig('tutorial_resample.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_resample.png){ width=350 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can first convert the stereo to mono, and resample, using composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27686])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled = torchaudio.transforms.Compose([\n",
    "    torchaudio.transforms.LC2CL(),\n",
    "    torchaudio.transforms.DownmixMono(),\n",
    "    torchaudio.transforms.LC2CL(),\n",
    "    torchaudio.transforms.Resample(frequency, new_frequency)\n",
    "])(waveform)\n",
    "\n",
    "resampled.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resampled[0,:].numpy())\n",
    "plt.savefig('tutorial_resample_mono.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_resample_mono.png){ width=350 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example of transformations, we can encode the signal based on the Mu-Law companding. But to do so, we need the signal to be between -1 and 1. Since the tensor is just a regular PyTorch tensor, we can apply standard operators on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.5728), tensor(0.5760), tensor(9.2938e-05))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check if the tensor is in the interval [-1,1]\n",
    "waveform.min(), waveform.max(), waveform.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tensor):\n",
    "    # Subtract the mean, and scale to the interval [-1,1]\n",
    "    tensor_minusmean = tensor - tensor.mean()\n",
    "    return tensor_minusmean/tensor_minusmean.abs().max()\n",
    "\n",
    "normalized = normalize(waveform)  # Let's normalize to the full interval [-1,1]\n",
    "\n",
    "plt.plot(normalized[0,:].numpy())\n",
    "plt.savefig('tutorial_normalize.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_normalize.png){ width=350 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 276858])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = torchaudio.transforms.MuLawEncoding()(normalized)\n",
    "transformed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(transformed[0,:].numpy())\n",
    "plt.savefig('tutorial_mulawenc.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_mulawenc.png){ width=350 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 276858])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered = torchaudio.transforms.MuLawExpanding()(transformed)\n",
    "recovered.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recovered[0,:].numpy())\n",
    "plt.savefig('tutorial_mulawdec.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_mulawdec.png){ width=350 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0122)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered = torchaudio.transforms.MuLawExpanding()(transformed)\n",
    "\n",
    "def compute_median_relative_difference(normalized, recovered):\n",
    "    diff = (normalized-recovered)\n",
    "    return (diff.abs()/normalized.abs()).median()\n",
    "\n",
    "# Median relative difference between original and MuLaw reconstucted signals\n",
    "compute_median_relative_difference(normalized, recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrating to Torchaudio from Kaldi\n",
    "\n",
    "Users may be familiar with [Kaldi](http://github.com/kaldi-asr/kaldi), a toolkit for speech recognition. Torchaudio offers compatibility with it in `torchaudio.kaldi_io`. It can indeed read from kaldi scp, or ark file or streams with:\n",
    "\n",
    "* read_vec_int_ark\n",
    "* read_vec_flt_scp\n",
    "* read_vec_flt_arkfile/stream\n",
    "* read_mat_scp\n",
    "* read_mat_ark\n",
    "\n",
    "Torchaudio provides Kaldi-compatible transforms for `spectrogram` and `fbank` with the benefit of GPU support, see [here](compliance.kaldi.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1383, 201])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_fft = 400.0\n",
    "frame_length = n_fft / frequency * 1000.0\n",
    "frame_shift = frame_length / 2.0\n",
    "\n",
    "params = {\n",
    "    \"channel\": 0,\n",
    "    \"dither\": 0.0,\n",
    "    \"window_type\": \"hanning\",\n",
    "    \"frame_length\": frame_length,\n",
    "    \"frame_shift\": frame_shift,\n",
    "    \"remove_dc_offset\": False,\n",
    "    \"round_to_power_of_two\": False,\n",
    "    \"sample_frequency\": frequency,\n",
    "}\n",
    "\n",
    "specgram = torchaudio.compliance.kaldi.spectrogram(waveform, **params)\n",
    "\n",
    "specgram.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(specgram.transpose(0,1).numpy(), cmap='gray')\n",
    "plt.savefig('tutorial_kaldi_spectrogram.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_kaldi_spectrogram.png){ width=350 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also support computing the filterbank features from raw audio signal, matching Kaldi’s implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1383, 23])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbank = torchaudio.compliance.kaldi.fbank(waveform, **params)\n",
    "fbank.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,100))\n",
    "plt.imshow(fbank.transpose(0,1).numpy(), cmap='gray')\n",
    "plt.savefig('tutorial_kaldi_fbank.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tutorial_kaldi_fbank.png){ width=350 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We used an example sound signal to illustrate how to open an audio file or using Torchaudio, and how to pre-process and transform an audio signal. Given that Torchaudio is built on PyTorch, these techniques can be used as building blocks for more advanced audio applications, such as speech recognition, while leveraging GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
