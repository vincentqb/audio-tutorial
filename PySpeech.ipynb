{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example](https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py\n",
    "# $DIR_FOR_PREPROCESSED_DATA\n",
    "# --save-dir $MODEL_PATH\n",
    "# --max-epoch 80\n",
    "# --task speech_recognition\n",
    "# --arch vggtransformer_2\n",
    "# --optimizer adadelta\n",
    "# --lr 1.0\n",
    "# --adadelta-eps 1e-8\n",
    "# --adadelta-rho 0.95\n",
    "# --clip-norm 10.0\n",
    "# --max-tokens 5000\n",
    "# --log-format json\n",
    "# --log-interval 1\n",
    "# --criterion cross_entropy_acc\n",
    "# --user-dir examples/speech_recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.datasets import LIBRISPEECH, SPEECHCOMMANDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.datasets import LIBRISPEECH\n",
    "# waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id\n",
    "\n",
    "class SAFE_LIBRISPEECH(LIBRISPEECH):\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        try:\n",
    "            return super().__getitem__(n)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return None\n",
    "        \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            return super().__next__()\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return self.__next__()\n",
    "\n",
    "        \n",
    "def datasets():\n",
    "\n",
    "    folder_in_archive = 'LibriSpeech'\n",
    "    download = True\n",
    "    root = \"./\"\n",
    "\n",
    "    print(\"train\")\n",
    "    dataset1 = SAFE_LIBRISPEECH(root, url='train-clean-100', folder_in_archive=folder_in_archive, download=download)\n",
    "    # print(dataset1[0])\n",
    "    dataset2 = SAFE_LIBRISPEECH(root, url='train-clean-360', folder_in_archive=folder_in_archive, download=download)\n",
    "    # dataset3 = SAFE_LIBRISPEECH(root, url='train-other-500', folder_in_archive=folder_in_archive, download=download)\n",
    "    # train = torch.utils.data.ConcatDataset([dataset1, dataset2, dataset3])\n",
    "    train = torch.utils.data.ConcatDataset([dataset1, dataset2])\n",
    "    # print(train[0])\n",
    "\n",
    "    print(\"valid\")\n",
    "    dataset1 = SAFE_LIBRISPEECH(root, url='dev-clean', folder_in_archive=folder_in_archive, download=download)\n",
    "    dataset2 = SAFE_LIBRISPEECH(root, url='dev-other', folder_in_archive=folder_in_archive, download=download)\n",
    "    valid = torch.utils.data.ConcatDataset([dataset1, dataset2])\n",
    "\n",
    "    print(\"test\")\n",
    "    dataset1 = SAFE_LIBRISPEECH(root, url='test-other', folder_in_archive=folder_in_archive, download=download)\n",
    "    dataset2 = SAFE_LIBRISPEECH(root, url='test-clean', folder_in_archive=folder_in_archive, download=download)\n",
    "    test = torch.utils.data.ConcatDataset([dataset1, dataset2])\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PROCESSED_SPEECHCOMMANDS(SPEECHCOMMANDS):\n",
    "    def __getitem__(self, n):\n",
    "        return super().__getitem__(n)\n",
    "\n",
    "    def __next__(self):\n",
    "        return super().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryCache(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap a dataset so that, whenever a new item is returned, it is saved to disk.\n",
    "    \"\"\"\n",
    "                                                      \n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset                                                   \n",
    "        self.location = location\n",
    "                                            \n",
    "        self._id = id(self)                         \n",
    "        self._cache = [None] * len(dataset)\n",
    "                                  \n",
    "    def __getitem__(self, n):      \n",
    "        if self._cache[n]:                             \n",
    "            return self._cache[n]          \n",
    "                               \n",
    "        item = self.dataset[n]          \n",
    "        self._cache[n] = item             \n",
    "                                          \n",
    "        return item                       \n",
    "                                  \n",
    "    def __len__(self):                 \n",
    "        return len(self.dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "def datasets():\n",
    "\n",
    "    download = True\n",
    "    root = \"./\"\n",
    "\n",
    "    print(\"train\")\n",
    "    dataset1 = SPEECHCOMMANDS(root, download=download)\n",
    "    dataset1 = MemoryCache(dataset1)\n",
    "\n",
    "    return dataset1, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm_train \n",
    "# --input=data/lang_char/input.txt\n",
    "# --vocab_size=${nbpe}\n",
    "# --model_type=${bpemode}\n",
    "# --model_prefix=${bpemodel}\n",
    "# --input_sentence_size=100000000\n",
    "# --unk_id  =3\n",
    "# --eos_id=2\n",
    "# --pad_id=1\n",
    "# --bos_id=-1\n",
    "# --character_coverage=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "vocab_size = 5000\n",
    "nbpe = vocab_size\n",
    "bpemode = \"unigram\"\n",
    "input_sentence_size = 100000000\n",
    "fname = 'data_lang_char_input.txt'\n",
    "bpemodel = \"data_lang_char_train_\" + bpemode + \"_\" + str(nbpe)\n",
    "\n",
    "with open(fname, 'a') as f:\n",
    "    # FIXME Load audio along with text\n",
    "    for u in train:\n",
    "        if u:\n",
    "            f.write(u[2] + \"\\n\")\n",
    "    f.write(\"<unk> 3\")\n",
    "    f.write(\"</s> 2\")\n",
    "    f.write(\"<pad> 1\")\n",
    "\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={fname} \"\n",
    "    f\"--vocab_size={nbpe} \"\n",
    "    f\"--model_type={bpemode} \"\n",
    "    f\"--model_prefix={bpemodel} \"\n",
    "    f\"--input_sentence_size={input_sentence_size} \"\n",
    "    f\"--unk_id=3 \"\n",
    "    f\"--eos_id=2 \"\n",
    "    f\"--pad_id=1 \"\n",
    "    f\"--bos_id=-1 \"\n",
    "    f\"--character_coverage=1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁T his ▁ is ▁ a ▁ test\n",
      "[640, 3, 394, 3, 394, 3, 394, 3]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "# sp.Load(bpemodel + \".model\")\n",
    "sp.Load(\"/Users/vincentqb/spm.model\")\n",
    "\n",
    "token = sp.encode_as_pieces(\"This is a test\")\n",
    "token = \" \".join(token)\n",
    "\n",
    "print(token)\n",
    "\n",
    "token = sp.encode_as_ids(\"This is a test\")\n",
    "# token = \" \".join(str(token))\n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'o', 'u', 's', 'e', '*', '*', '*']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\n",
    "    '-', '*', 'right', 'eight', 'cat', 'tree', 'bed', 'happy', 'go', 'dog', 'no', \n",
    "    'wow', 'nine', 'left', 'stop', 'three', 'sheila', 'one', 'bird', 'zero',\n",
    "    'seven', 'up', 'marvin', 'two', 'house', 'down', 'six', 'yes', 'on', \n",
    "    'five', 'off', 'four',\n",
    "]\n",
    "\n",
    "labels = [\n",
    "        '-', '*',\n",
    "        \"backward\",\n",
    "        \"bed\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"dog\",\n",
    "        \"down\",\n",
    "        \"eight\",\n",
    "        \"five\",\n",
    "        \"follow\",\n",
    "        \"forward\",\n",
    "        \"four\",\n",
    "        \"go\",\n",
    "        \"happy\",\n",
    "        \"house\",\n",
    "        \"learn\",\n",
    "        \"left\",\n",
    "        \"marvin\",\n",
    "        \"nine\",\n",
    "        \"no\",\n",
    "        \"off\",\n",
    "        \"on\",\n",
    "        \"one\",\n",
    "        \"right\",\n",
    "        \"seven\",\n",
    "        \"sheila\",\n",
    "        \"six\",\n",
    "        \"stop\",\n",
    "        \"three\",\n",
    "        \"tree\",\n",
    "        \"two\",\n",
    "        \"up\",\n",
    "        \"visual\",\n",
    "        \"wow\",\n",
    "        \"yes\",\n",
    "        \"zero\",\n",
    "]\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "def build_mapping(labels):\n",
    "    labels = list(collections.OrderedDict.fromkeys(list(\"\".join(labels))))\n",
    "    enumerated = list(enumerate(labels))\n",
    "    flipped = [(sub[1], sub[0]) for sub in enumerated]\n",
    "\n",
    "    d1 = collections.OrderedDict(enumerated)\n",
    "    d2 = collections.OrderedDict(flipped)\n",
    "    return {**d1, **d2}\n",
    "\n",
    "def padding(l, max_length, fillwith):\n",
    "    return l  + [fillwith] * (max_length-len(l))\n",
    "\n",
    "def map_with_dict(mapping, l):\n",
    "    return [mapping[t] for t in l]\n",
    "\n",
    "def apply_with_padding(l, mapping, max_length, fillwith):\n",
    "    l = map_with_dict(mapping, l)\n",
    "    l = padding(l, max_length, mapping[\"*\"])\n",
    "    return l\n",
    "\n",
    "\n",
    "test = \"house\"\n",
    "max_length = max(map(len, labels))\n",
    "vocab_size = len(labels) + 2\n",
    "\n",
    "mapping = build_mapping(labels)\n",
    "\n",
    "# test = apply(mapping, test)\n",
    "# test = padding(test, max_length, mapping[\"*\"])\n",
    "\n",
    "encode = lambda l: apply_with_padding(l, mapping, max_length, mapping[\"*\"])\n",
    "decode = lambda l: apply_with_padding(l, mapping, max_length, mapping[1])\n",
    "\n",
    "decode(encode(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "num_features = 13\n",
    "\n",
    "melkwargs = {\n",
    "    'n_fft': 512,\n",
    "    'n_mels': 20,\n",
    "    'hop_length': 80,\n",
    "}\n",
    "\n",
    "mfcc = MFCC(sample_rate=16000, n_mfcc=num_features, melkwargs=melkwargs)\n",
    "\n",
    "# audio, self.sr, window_stride=(160, 80),\n",
    "# fft_size=512, num_filt=20, num_coeffs=13\n",
    "\n",
    "def process_waveform(waveform):\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    return mfcc(waveform)[0, ...].transpose(0, -1)\n",
    "\n",
    "def process_target(target):\n",
    "\n",
    "    # targets = []\n",
    "    # for b in batch:\n",
    "    #     if b:\n",
    "    #         token = sp.encode_as_pieces(b[2])\n",
    "    #         print(len(token))\n",
    "    #         token = \" \".join(token)\n",
    "    #         targets.append(token)\n",
    "\n",
    "    # return \" \".join(sp.encode_as_ids(target))\n",
    "    \n",
    "    # return torch.IntTensor(sp.encode_as_ids(target))\n",
    "    # print(target)\n",
    "    return torch.IntTensor(encode(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from random import randint\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    tensors = [process_waveform(b[0]) for b in batch if b]\n",
    "    targets = [process_target(b[2]) for b in batch if b]\n",
    "\n",
    "    # truncate tensor list\n",
    "    # length = 2**10\n",
    "    # a = max(0, min([tensor.shape[-1] for tensor in tensors]) - length)\n",
    "    # m = randint(0, a)\n",
    "    # n = m + length\n",
    "    # tensors = [t[..., m:n] for t in tensors]\n",
    "    \n",
    "    input_lengths = [t.shape[0] for t in tensors]\n",
    "    target_lengths = [len(t) for t in targets]\n",
    "\n",
    "    if tensors:    \n",
    "        targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "        tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "        tensors = tensors.transpose(1, -1)\n",
    "        return tensors, targets, input_lengths, target_lengths\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "\n",
    "max_tokens = 5000  # max number of tokens per batch\n",
    "# vocab_size = max_tokens\n",
    "batch_size = 32  # max number of sentences per batch\n",
    "loader_train = DataLoader(train, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torchaudio.datasets.ljspeech import LJSPEECH\n",
    "train = LJSPEECH(\"./\", download=True)\n",
    "\n",
    "class SAFE_LJSPEECH(LIBRISPEECH):\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        try:\n",
    "            waveform, _, _, utterrance = super().__getitem__(n)\n",
    "            return waveform, utterance\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return None\n",
    "        \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            waveform, _, _, utterrance = super().__next__()\n",
    "            return waveform, utterance\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            waveform, _, _, utterrance = self.__next__()\n",
    "            return waveform, utterance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 13, 201])\n",
      "torch.Size([32, 8])\n",
      "[201, 201, 201, 201, 201, 201, 201, 201, 201, 180, 201, 201, 171, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 188, 201, 182, 201, 201, 201, 201, 201, 201]\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "for a, b, c, d in loader_train:\n",
    "    print(a.shape)\n",
    "    print(b.shape)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feat_per_channel = 80\n",
    "vggblock_enc_config = \"[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]\"\n",
    "transformer_enc_config = \"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 16\"\n",
    "enc_output_dim = 1024\n",
    "tgt_embed_dim = 512\n",
    "conv_dec_config = \"((256, 3, True),) * 4\"\n",
    "transformer_dec_config = \"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class PrintLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintLayer, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        print(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class Wav2Letter(nn.Module):\n",
    "    \"\"\"Wav2Letter Speech Recognition model\n",
    "        Architecture is based off of Facebooks AI Research paper\n",
    "        https://arxiv.org/pdf/1609.03193.pdf\n",
    "        This specific architecture accepts mfcc or\n",
    "        power spectrums speech signals\n",
    "        TODO: use cuda if available\n",
    "        Args:\n",
    "            num_features (int): number of mfcc features\n",
    "            num_classes (int): number of unique grapheme class labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Wav2Letter, self).__init__()\n",
    "\n",
    "        # Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.layers = nn.Sequential(\n",
    "            # PrintLayer(),\n",
    "            nn.Conv1d(num_features, 250, 48, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv1d(250, 250, 7),\n",
    "            # nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 2000, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, 2000, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass through Wav2Letter network than \n",
    "            takes log probability of output\n",
    "        Args:\n",
    "            batch (int): mini batch of data\n",
    "             shape (batch, num_features, frame_len)\n",
    "        Returns:\n",
    "            log_probs (torch.Tensor):\n",
    "                shape  (batch_size, num_classes, output_len)\n",
    "        \"\"\"\n",
    "        # y_pred shape (batch_size, num_classes, output_len)\n",
    "        y_pred = self.layers(batch)\n",
    "\n",
    "        # compute log softmax probability on graphemes\n",
    "        log_probs = nn.functional.log_softmax(y_pred, dim=1)\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "model = Wav2Letter(num_features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-1fa295f6518f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# print(\"stepping\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;31m# torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspeech/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyspeech/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torch.optim import Adadelta\n",
    "\n",
    "model = Wav2Letter(num_features, vocab_size)\n",
    "\n",
    "optimizer_params = {\n",
    "    \"lr\": 1.0,\n",
    "    \"eps\": 1e-8,\n",
    "    \"rho\": 0.95,\n",
    "}\n",
    "optimizer = Adadelta(model.parameters(), **optimizer_params)\n",
    "\n",
    "max_epoch = 2 # 80\n",
    "clip_norm = 10.\n",
    "\n",
    "criterion = torch.nn.CTCLoss()\n",
    "\n",
    "# max_files = 10\n",
    "for epoch in range(max_epoch):\n",
    "    # print(epoch)\n",
    "    \n",
    "    i_files = 0\n",
    "    for inputs, targets, _, target_lengths in loader_train:\n",
    "        # if i_files > max_files:\n",
    "        #     break\n",
    "\n",
    "        # print(i_files, max_files)\n",
    "\n",
    "        if inputs is None or targets is None:\n",
    "            continue\n",
    "\n",
    "        # print(\"input\", inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        # (input length, batch size, number of classes)\n",
    "        # input_lengths = [len(o) for o in outputs]\n",
    "\n",
    "        outputs = outputs.transpose(1, 2).transpose(0, 1)\n",
    "        # print(\"output\", outputs.shape)\n",
    "        # print(\"target\", targets.shape)\n",
    "        \n",
    "        # print(inputs.shape)\n",
    "        # print(outputs.shape)\n",
    "        # print(targets.shape)\n",
    "        # print(len(targets))\n",
    "        # print(targets.shape)\n",
    "        # print(input_lengths)\n",
    "        # input_lengths = [len(o) for o in outputs]\n",
    "        # print(len(input_lengths))\n",
    "        # target_lengths = [len(t) for t in targets]\n",
    "        # print(target_lengths)\n",
    "        # ctc_loss(input, target, input_lengths, target_lengths)\n",
    "\n",
    "        # input_lengths = [outputs.shape[0]] * outputs.shape[1]\n",
    "        \n",
    "        # CTC arguments\n",
    "        # https://pytorch.org/docs/master/nn.html#torch.nn.CTCLoss\n",
    "        # better definitions for ctc arguments\n",
    "        # https://discuss.pytorch.org/t/ctcloss-with-warp-ctc-help/8788/3\n",
    "        mini_batch_size = len(inputs)\n",
    "        \n",
    "        input_lengths = torch.full((mini_batch_size,), outputs.shape[0], dtype=torch.long)\n",
    "        target_lengths = torch.IntTensor([target.shape[0] for target in targets])\n",
    "        \n",
    "        # print(torch.isnan(outputs).any())\n",
    "        # print(torch.isnan(targets).any())\n",
    "        # print(torch.isnan(input_lengths).any())\n",
    "        # print(torch.isnan(target_lengths).any())\n",
    "        # print(outputs.shape)\n",
    "        # print(targets.shape)\n",
    "        # print(input_lengths.shape)\n",
    "        # print(target_lengths.shape)\n",
    "\n",
    "        # outputs: input length, batch size, number of classes (including blank) \n",
    "        # targets: batch size, max target length\n",
    "        # input_lengths: batch size\n",
    "        # target_lengths: batch size\n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "        # print(\"stepping\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        i_files += 1\n",
    "    \n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import topk\n",
    "\n",
    "def GreedyDecoder(ctc_matrix, blank_label=0):\n",
    "    \"\"\"Greedy Decoder. Returns highest probability of\n",
    "        class labels for each timestep\n",
    "        # TODO: collapse blank labels\n",
    "    Args:\n",
    "        ctc_matrix (torch.Tensor): \n",
    "            shape (1, num_classes, output_len)\n",
    "        blank_label (int): blank labels to collapse\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: class labels per time step.\n",
    "         shape (ctc timesteps)\n",
    "    \"\"\"\n",
    "    _, indices = topk(ctc_matrix, k=1, dim=1)\n",
    "    return indices[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan],\n",
      "         [nan, nan, nan, nan]]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "sample = inputs[0].unsqueeze(0)\n",
    "target = targets[0]\n",
    "\n",
    "# decode(targets[0].tolist())\n",
    "\n",
    "output = model(sample)\n",
    "print(output)\n",
    "\n",
    "# output = GreedyDecoder(output)\n",
    "\n",
    "# print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
