{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example](https://github.com/LearnedVector/Wav2Letter/blob/master/Google%20Speech%20Command%20Example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py\n",
    "# $DIR_FOR_PREPROCESSED_DATA\n",
    "# --save-dir $MODEL_PATH\n",
    "# --max-epoch 80\n",
    "# --task speech_recognition\n",
    "# --arch vggtransformer_2\n",
    "# --optimizer adadelta\n",
    "# --lr 1.0\n",
    "# --adadelta-eps 1e-8\n",
    "# --adadelta-rho 0.95\n",
    "# --clip-norm 10.0\n",
    "# --max-tokens 5000\n",
    "# --log-format json\n",
    "# --log-interval 1\n",
    "# --criterion cross_entropy_acc\n",
    "# --user-dir examples/speech_recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.datasets import LIBRISPEECH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.datasets import LIBRISPEECH\n",
    "# waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id\n",
    "\n",
    "class SAFE_LIBRISPEECH(LIBRISPEECH):\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        try:\n",
    "            return super().__getitem__(n)\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return None\n",
    "        \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            return super().__next__()\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return self.__next__()\n",
    "\n",
    "        \n",
    "def datasets():\n",
    "\n",
    "    folder_in_archive = 'LibriSpeech'\n",
    "    download = True\n",
    "    root = \"./\"\n",
    "\n",
    "    print(\"train\")\n",
    "    dataset1 = SAFE_LIBRISPEECH(root, url='train-clean-100', folder_in_archive=folder_in_archive, download=download)\n",
    "    # print(dataset1[0])\n",
    "    dataset2 = SAFE_LIBRISPEECH(root, url='train-clean-360', folder_in_archive=folder_in_archive, download=download)\n",
    "    # dataset3 = SAFE_LIBRISPEECH(root, url='train-other-500', folder_in_archive=folder_in_archive, download=download)\n",
    "    # train = torch.utils.data.ConcatDataset([dataset1, dataset2, dataset3])\n",
    "    train = torch.utils.data.ConcatDataset([dataset1, dataset2])\n",
    "    # print(train[0])\n",
    "\n",
    "    print(\"valid\")\n",
    "    dataset1 = SAFE_LIBRISPEECH(root, url='dev-clean', folder_in_archive=folder_in_archive, download=download)\n",
    "    dataset2 = SAFE_LIBRISPEECH(root, url='dev-other', folder_in_archive=folder_in_archive, download=download)\n",
    "    valid = torch.utils.data.ConcatDataset([dataset1, dataset2])\n",
    "\n",
    "    print(\"test\")\n",
    "    dataset1 = SAFE_LIBRISPEECH(root, url='test-other', folder_in_archive=folder_in_archive, download=download)\n",
    "    dataset2 = SAFE_LIBRISPEECH(root, url='test-clean', folder_in_archive=folder_in_archive, download=download)\n",
    "    test = torch.utils.data.ConcatDataset([dataset1, dataset2])\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "valid\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm_train \n",
    "# --input=data/lang_char/input.txt\n",
    "# --vocab_size=${nbpe}\n",
    "# --model_type=${bpemode}\n",
    "# --model_prefix=${bpemodel}\n",
    "# --input_sentence_size=100000000\n",
    "# --unk_id  =3\n",
    "# --eos_id=2\n",
    "# --pad_id=1\n",
    "# --bos_id=-1\n",
    "# --character_coverage=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "vocab_size = 5000\n",
    "nbpe = vocab_size\n",
    "bpemode = \"unigram\"\n",
    "input_sentence_size = 100000000\n",
    "fname = 'data_lang_char_input.txt'\n",
    "bpemodel = \"data_lang_char_train_\" + bpemode + \"_\" + str(nbpe)\n",
    "\n",
    "with open(fname, 'a') as f:\n",
    "    # FIXME Load audio along with text\n",
    "    for u in train:\n",
    "        if u:\n",
    "            f.write(u[2] + \"\\n\")\n",
    "    f.write(\"<unk> 3\")\n",
    "    f.write(\"</s> 2\")\n",
    "    f.write(\"<pad> 1\")\n",
    "\n",
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={fname} \"\n",
    "    f\"--vocab_size={nbpe} \"\n",
    "    f\"--model_type={bpemode} \"\n",
    "    f\"--model_prefix={bpemodel} \"\n",
    "    f\"--input_sentence_size={input_sentence_size} \"\n",
    "    f\"--unk_id=3 \"\n",
    "    f\"--eos_id=2 \"\n",
    "    f\"--pad_id=1 \"\n",
    "    f\"--bos_id=-1 \"\n",
    "    f\"--character_coverage=1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁T his ▁ is ▁ a ▁ test\n",
      "[640, 3, 394, 3, 394, 3, 394, 3]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "# sp.Load(bpemodel + \".model\")\n",
    "sp.Load(\"/Users/vincentqb/spm.model\")\n",
    "\n",
    "token = sp.encode_as_pieces(\"This is a test\")\n",
    "token = \" \".join(token)\n",
    "\n",
    "print(token)\n",
    "\n",
    "token = sp.encode_as_ids(\"This is a test\")\n",
    "# token = \" \".join(str(token))\n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.9346e-04,  0.0000e+00, -6.4087e-04,  ...,  6.4087e-04,\n",
       "           2.1362e-04,  6.1035e-05]]),\n",
       " 16000,\n",
       " \"YES SIR SAID THE SERVANT SUCH AN INSTRUCTION WAS REMARKABLY PLEASING TO HIM THERE WAS MUCH THAT HE HAD TO DO AND THAT NIGHT'S FREEDOM WOULD ASSIST HIM MATERIALLY PERHAPS KARA HESITATED PERHAPS YOU HAD BETTER WAIT UNTIL ELEVEN O'CLOCK\",\n",
       " 1088,\n",
       " 134318,\n",
       " 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from random import randint\n",
    "\n",
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "num_features = 13\n",
    "mfcc = MFCC(sample_rate=16000, n_mfcc=num_features)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    tensors = [mfcc(b[0])[0, ...].transpose(0, -1) for b in batch if b]\n",
    "    # targets = [\" \".join(sp.encode_as_ids(b[2])) for b in batch if b]\n",
    "    targets = [torch.IntTensor(sp.encode_as_ids(b[2])) for b in batch if b]\n",
    "\n",
    "    # targets = []\n",
    "    # for b in batch:\n",
    "    #     if b:\n",
    "    #         token = sp.encode_as_pieces(b[2])\n",
    "    #         print(len(token))\n",
    "    #         token = \" \".join(token)\n",
    "    #         targets.append(token)\n",
    "\n",
    "    # truncate tensor list\n",
    "    # length = 2**10\n",
    "    # a = max(0, min([tensor.shape[-1] for tensor in tensors]) - length)\n",
    "    # m = randint(0, a)\n",
    "    # n = m + length\n",
    "    # tensors = [t[..., m:n] for t in tensors]\n",
    "    \n",
    "    input_lengths = [t.shape[0] for t in tensors]\n",
    "    target_lengths = [len(t) for t in targets]\n",
    "\n",
    "    if tensors:    \n",
    "        tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "        tensors = tensors.transpose(1, -1)\n",
    "        \n",
    "        targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "        return tensors, targets, input_lengths, target_lengths\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "\n",
    "# max_tokens = 5000  # max number of tokens per batch\n",
    "batch_size = 3  # max number of sentences per batch\n",
    "loader_train = DataLoader(train, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.9346e-04,  0.0000e+00, -6.4087e-04,  ...,  6.4087e-04,\n",
       "           2.1362e-04,  6.1035e-05]]),\n",
       " 16000,\n",
       " \"YES SIR SAID THE SERVANT SUCH AN INSTRUCTION WAS REMARKABLY PLEASING TO HIM THERE WAS MUCH THAT HE HAD TO DO AND THAT NIGHT'S FREEDOM WOULD ASSIST HIM MATERIALLY PERHAPS KARA HESITATED PERHAPS YOU HAD BETTER WAIT UNTIL ELEVEN O'CLOCK\",\n",
       " 1088,\n",
       " 134318,\n",
       " 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torchaudio.datasets.ljspeech import LJSPEECH\n",
    "train = LJSPEECH(\"./\", download=True)\n",
    "\n",
    "class SAFE_LJSPEECH(LIBRISPEECH):\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        try:\n",
    "            waveform, _, _, utterrance = super().__getitem__(n)\n",
    "            return waveform, utterance\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            return None\n",
    "        \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            waveform, _, _, utterrance = super().__next__()\n",
    "            return waveform, utterance\n",
    "        except (FileNotFoundError, RuntimeError):\n",
    "            waveform, _, _, utterrance = self.__next__()\n",
    "            return waveform, utterance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from random import randint\n",
    "\n",
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "num_features = 13\n",
    "mfcc = MFCC(sample_rate=16000, n_mfcc=num_features)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # pick first channel, apply mfcc, tranpose for pad_sequence\n",
    "    tensors = [mfcc(b[0])[0, ...].transpose(0, -1) for b in batch if b]\n",
    "    # targets = [\" \".join(sp.encode_as_ids(b[2])) for b in batch if b]\n",
    "    targets = [torch.IntTensor(sp.encode_as_ids(b[2])) for b in batch if b]\n",
    "\n",
    "    # targets = []\n",
    "    # for b in batch:\n",
    "    #     if b:\n",
    "    #         token = sp.encode_as_pieces(b[2])\n",
    "    #         print(len(token))\n",
    "    #         token = \" \".join(token)\n",
    "    #         targets.append(token)\n",
    "\n",
    "    # truncate tensor list\n",
    "    # length = 2**10\n",
    "    # a = max(0, min([tensor.shape[-1] for tensor in tensors]) - length)\n",
    "    # m = randint(0, a)\n",
    "    # n = m + length\n",
    "    # tensors = [t[..., m:n] for t in tensors]\n",
    "    \n",
    "    input_lengths = [t.shape[0] for t in tensors]\n",
    "    target_lengths = [len(t) for t in targets]\n",
    "\n",
    "    if tensors:    \n",
    "        tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "        tensors = tensors.transpose(1, -1)\n",
    "        \n",
    "        targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "\n",
    "        return tensors, targets, input_lengths, target_lengths\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "\n",
    "# max_tokens = 5000  # max number of tokens per batch\n",
    "batch_size = 3  # max number of sentences per batch\n",
    "loader_train = DataLoader(train, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.9346e-04,  0.0000e+00, -6.4087e-04,  ...,  6.4087e-04,\n",
       "           2.1362e-04,  6.1035e-05]]),\n",
       " 16000,\n",
       " \"YES SIR SAID THE SERVANT SUCH AN INSTRUCTION WAS REMARKABLY PLEASING TO HIM THERE WAS MUCH THAT HE HAD TO DO AND THAT NIGHT'S FREEDOM WOULD ASSIST HIM MATERIALLY PERHAPS KARA HESITATED PERHAPS YOU HAD BETTER WAIT UNTIL ELEVEN O'CLOCK\",\n",
       " 1088,\n",
       " 134318,\n",
       " 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 13, 1227])\n",
      "torch.Size([3, 55])\n",
      "[1203, 627, 1227]\n",
      "[50, 27, 55]\n"
     ]
    }
   ],
   "source": [
    "for a, b, c, d in loader_train:\n",
    "    print(a.shape)\n",
    "    print(b.shape)\n",
    "    print(c)\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feat_per_channel = 80\n",
    "vggblock_enc_config = \"[(64, 3, 2, 2, True), (128, 3, 2, 2, True)]\"\n",
    "transformer_enc_config = \"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 16\"\n",
    "enc_output_dim = 1024\n",
    "tgt_embed_dim = 512\n",
    "conv_dec_config = \"((256, 3, True),) * 4\"\n",
    "transformer_dec_config = \"((1024, 16, 4096, True, 0.15, 0.15, 0.15),) * 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Wav2Letter(nn.Module):\n",
    "    \"\"\"Wav2Letter Speech Recognition model\n",
    "        Architecture is based off of Facebooks AI Research paper\n",
    "        https://arxiv.org/pdf/1609.03193.pdf\n",
    "        This specific architecture accepts mfcc or\n",
    "        power spectrums speech signals\n",
    "        TODO: use cuda if available\n",
    "        Args:\n",
    "            num_features (int): number of mfcc features\n",
    "            num_classes (int): number of unique grapheme class labels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Wav2Letter, self).__init__()\n",
    "\n",
    "        # Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(num_features, 250, 48, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 250, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(250, 2000, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, 2000, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(2000, num_classes, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Forward pass through Wav2Letter network than \n",
    "            takes log probability of output\n",
    "        Args:\n",
    "            batch (int): mini batch of data\n",
    "             shape (batch, num_features, frame_len)\n",
    "        Returns:\n",
    "            log_probs (torch.Tensor):\n",
    "                shape  (batch_size, num_classes, output_len)\n",
    "        \"\"\"\n",
    "        # y_pred shape (batch_size, num_classes, output_len)\n",
    "        y_pred = self.layers(batch)\n",
    "\n",
    "        # compute log softmax probability on graphemes\n",
    "        log_probs = nn.functional.log_softmax(y_pred, dim=1)\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "vocab_size = 5000\n",
    "model = Wav2Letter(num_features, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "1 10\n",
      "2 10\n",
      "3 10\n",
      "4 10\n",
      "5 10\n",
      "6 10\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torch.optim import Adadelta\n",
    "\n",
    "optimizer_params = {\n",
    "    \"lr\": 1.0,\n",
    "    \"eps\": 1e-8,\n",
    "    \"rho\": 0.95,\n",
    "}\n",
    "optimizer = Adadelta(model.parameters(), **optimizer_params)\n",
    "\n",
    "max_epoch = 2 # 80\n",
    "clip_norm = 10.\n",
    "\n",
    "criterion = torch.nn.CTCLoss()\n",
    "\n",
    "max_files = 10\n",
    "for epoch in range(max_epoch):\n",
    "    # print(epoch)\n",
    "    \n",
    "    i_files = 0\n",
    "    for inputs, targets, _, target_lengths in loader_train:\n",
    "        print(i_files, max_files)\n",
    "\n",
    "        if inputs is None or targets is None:\n",
    "            continue\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        # (input length, batch size, number of classes)\n",
    "        # input_lengths = [len(o) for o in outputs]\n",
    "\n",
    "        outputs = outputs.transpose(1, 2).transpose(0, 1)\n",
    "\n",
    "        # print(inputs.shape)\n",
    "        # print(outputs.shape)\n",
    "        # print(targets.shape)\n",
    "        # print(len(targets))\n",
    "        # print(targets.shape)\n",
    "        # print(input_lengths)\n",
    "        # input_lengths = [len(o) for o in outputs]\n",
    "        # print(len(input_lengths))\n",
    "        # target_lengths = [len(t) for t in targets]\n",
    "        # print(target_lengths)\n",
    "        # ctc_loss(input, target, input_lengths, target_lengths)\n",
    "\n",
    "        input_lengths = [outputs.shape[0]] * outputs.shape[1]\n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "\n",
    "        # print(\"stepping\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_files > max_files:\n",
    "            break\n",
    "        i_files += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import topk\n",
    "\n",
    "def GreedyDecoder(ctc_matrix, blank_label=0):\n",
    "    \"\"\"Greedy Decoder. Returns highest probability of\n",
    "        class labels for each timestep\n",
    "        # TODO: collapse blank labels\n",
    "    Args:\n",
    "        ctc_matrix (torch.Tensor): \n",
    "            shape (1, num_classes, output_len)\n",
    "        blank_label (int): blank labels to collapse\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: class labels per time step.\n",
    "         shape (ctc timesteps)\n",
    "    \"\"\"\n",
    "    top = topk(ctc_matrix, k=1, dim=1)\n",
    "    return top[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = inputs[0]\n",
    "target = targets[0]\n",
    "\n",
    "print(target)\n",
    "\n",
    "output = model.eval(sample)\n",
    "output = GreedyDecoder(output)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
