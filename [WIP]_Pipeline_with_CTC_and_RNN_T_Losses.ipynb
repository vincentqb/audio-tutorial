{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[WIP] Pipeline with CTC and RNN-T Losses.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9e1ce96d01444eea0ff98ee0c064d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3d6b489e3419449b90fee63867190c24",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33bd32ed294a41a1b5d62be17cbffd5c",
              "IPY_MODEL_aba4b77e247f4aa6866781662afb54c9"
            ]
          }
        },
        "3d6b489e3419449b90fee63867190c24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33bd32ed294a41a1b5d62be17cbffd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_96473a9b381340978fe1c22f4398aa56",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2428923189,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2428923189,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8872b4d903ef46b1a1f86827fd44b808"
          }
        },
        "aba4b77e247f4aa6866781662afb54c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_773794be8bb74627b9ac8211071b279d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.26G/2.26G [00:47&lt;00:00, 50.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5eac6c6102a455788baceef24b0be30"
          }
        },
        "96473a9b381340978fe1c22f4398aa56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8872b4d903ef46b1a1f86827fd44b808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "773794be8bb74627b9ac8211071b279d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5eac6c6102a455788baceef24b0be30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYJByTxeZFAV"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHodDynXZFAZ"
      },
      "source": [
        "\n",
        "Moving from CTC to RNN Transducer Loss with torchaudio\n",
        "------------------------------------------------------\n",
        "\n",
        "This tutorial will discuss how to work with CTC and the RNN Transducer loss in torchaudio.\n",
        "\n",
        "Colab has GPU option available. In the menu tabs, select “Runtime” then\n",
        "“Change runtime type”. In the pop-up that follows, you can choose GPU.\n",
        "After the change, your runtime should automatically restart (which means\n",
        "information from executed cells disappear).\n",
        "\n",
        "First, let’s import the common torch packages such as\n",
        "`torchaudio <https://github.com/pytorch/audio>`__ that can be installed\n",
        "by following the instructions on the website.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9jU9XJCIlzO",
        "outputId": "bc98263b-49ff-41bd-87a8-4f9e239e5111"
      },
      "source": [
        "!pip install --pre torch==1.9.0.dev20210416+cpu torchaudio==0.9.0.dev20210416 -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
            "Collecting torch==1.9.0.dev20210416+cpu\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.9.0.dev20210416%2Bcpu-cp37-cp37m-linux_x86_64.whl (178.1MB)\n",
            "\u001b[K     |████████████████████████████████| 178.1MB 55kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.9.0.dev20210416\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/nightly/torchaudio-0.9.0.dev20210416-cp37-cp37m-linux_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 55.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0.dev20210416+cpu) (3.7.4.3)\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.9.0.dev20210416+cpu which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.9.0.dev20210416+cpu which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed torch-1.9.0.dev20210416+cpu torchaudio-0.9.0.dev20210416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPr-MZ9VLcta"
      },
      "source": [
        "\"\"\"\n",
        "!pip install torch==1.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Reload packages available in google colab\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# Compile from source\n",
        "!rm -rf audio/\n",
        "!git clone --recurse-submodules --single-branch https://github.com/pytorch/audio.git\n",
        "!BUILD_TRANSDUCER=1 BUILD_SOX=1 pip install -v audio/\n",
        "\n",
        "site.main()\n",
        "\"\"\";"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o79W0pQOJgod"
      },
      "source": [
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Update environment variable before running `python setup.py install`\n",
        "# https://github.com/HawkAaron/warp-transducer/issues/15\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    print(\"cuda available\")\n",
        "\n",
        "    CUDA_HOME = \"/usr/local/cuda\"\n",
        "\n",
        "    os.environ['CUDA_HOME'] = CUDA_HOME\n",
        "    os.environ['CUDA_TOOLKIT_ROOT_DIR'] = CUDA_HOME\n",
        "\n",
        "    os.environ['LIBRARY_PATH'] = \":\".join([\n",
        "        f\"{CUDA_HOME}/lib64\",\n",
        "        os.environ.get(\"LIBRARY_PATH\", \"\"),\n",
        "    ])\n",
        "\n",
        "    os.environ['LD_LIBRARY_PATH'] = \":\".join([\n",
        "        f\"{CUDA_HOME}/lib64\",\n",
        "        f\"{CUDA_HOME}/extras/CUPTI/lib64\",\n",
        "        os.environ.get(\"LD_LIBRARY_PATH\", \"\"),\n",
        "    ])\n",
        "\n",
        "    os.environ['CFLAGS'] = \" \".join([\n",
        "       f\"-I{CUDA_HOME}/include\",\n",
        "       os.environ.get(\"CFLAGS\", \"\"),\n",
        "    ])\n",
        "\n",
        "!pwd\n",
        "%cd /content/\n",
        "\n",
        "![ ! -d 'warp-transducer' ] && git clone https://github.com/HawkAaron/warp-transducer\n",
        "%cd warp-transducer\n",
        "\n",
        "!mkdir -p build\n",
        "%cd build\n",
        "!rm -f CMakeCache.txt\n",
        "!cmake ..\n",
        "!make\n",
        "\n",
        "%cd ../pytorch_binding\n",
        "\n",
        "!touch files.txt\n",
        "!xargs rm -rf < files.txt\n",
        "!python setup.py clean\n",
        "!python setup.py install --record files.txt --old-and-unmanageable\n",
        "\n",
        "%cd /content/\n",
        "!pwd\n",
        "\n",
        "# Reload packages available in google colab\n",
        "site.main()\n",
        "\n",
        "from warprnnt_pytorch import RNNTLoss, rnnt_loss\n",
        "\n",
        "!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "\"\"\";"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hezaOGu8Itl7"
      },
      "source": [
        "from torchaudio.prototype.transducer import RNNTLoss, rnnt_loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE8O1B5nZFAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26fbd12-8f05-4025-9ad3-706ce947ccf7"
      },
      "source": [
        "# Uncomment the following line to run in Google Colab\n",
        "\n",
        "# CPU:\n",
        "# !pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# GPU:\n",
        "# !pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# For interactive demo at the end:\n",
        "!pip install pydub\n",
        "\n",
        "import math\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torchaudio.models.wav2letter import Wav2Letter"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeCv4zU0ZFAa"
      },
      "source": [
        "Let’s check if a CUDA GPU is available and select our device. Running\n",
        "the network on a GPU will greatly decrease the training/testing runtime.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEf-sM_yZFAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a34d23-a103-476a-a51a-f84d926dcf77"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l-eA_MoZFAb"
      },
      "source": [
        "Importing the Dataset\n",
        "---------------------\n",
        "\n",
        "To load the data, we follow the `Speech Command Recognition <https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio.html>`__ tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "a9e1ce96d01444eea0ff98ee0c064d33",
            "3d6b489e3419449b90fee63867190c24",
            "33bd32ed294a41a1b5d62be17cbffd5c",
            "aba4b77e247f4aa6866781662afb54c9",
            "96473a9b381340978fe1c22f4398aa56",
            "8872b4d903ef46b1a1f86827fd44b808",
            "773794be8bb74627b9ac8211071b279d",
            "a5eac6c6102a455788baceef24b0be30"
          ]
        },
        "id": "mnDfrLxSZFAb",
        "outputId": "97a03fab-a828-4848-cfe7-3c5af39b3679"
      },
      "source": [
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "\n",
        "\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(\"./\", download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                return [os.path.join(self._path, line.strip()) for line in fileobj]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "\n",
        "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
        "train_set = SubsetSC(\"training\")\n",
        "\n",
        "print(\"Length: \", len(train_set))\n",
        "\n",
        "# A data point in the SPEECHCOMMANDS dataset is a tuple made of a waveform (the audio signal), the sample rate, the utterance (label), the ID of the speaker, the number of the utterance.\n",
        "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]\n",
        "\n",
        "# Let’s find the list of labels available in the dataset.\n",
        "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
        "print(labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9e1ce96d01444eea0ff98ee0c064d33",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2428923189.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Length:  105829\n",
            "['backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow', 'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "upIpYszL_bg6",
        "outputId": "778d0799-fbb0-42c2-b74f-9f96702abeef"
      },
      "source": [
        "\"\"\"\n",
        "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
        "train_set = torchaudio.datasets.LIBRISPEECH(\"./\", \"train-clean-100\", download=True)\n",
        "\n",
        "print(\"Length: \", len(train_set))\n",
        "\n",
        "# A data point in the LIBRISPEECH dataset is a tuple made of:\n",
        "waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = train_set[0]\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Create training and testing split of the data. We do not use validation in this tutorial.\\ntrain_set = torchaudio.datasets.LIBRISPEECH(\"./\", \"train-clean-100\", download=True)\\n\\nprint(\"Length: \", len(train_set))\\n\\n# A data point in the LIBRISPEECH dataset is a tuple made of:\\nwaveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = train_set[0]\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7oaq2_VZFAe"
      },
      "source": [
        "We are encoding each word character-by-character. We add a special blank character that we represent by an asterisk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OxHHzNCZFAe",
        "outputId": "bb59328a-5094-421d-a26a-5e0cf9f45977"
      },
      "source": [
        "# We need to define a blank character to be used by the loss function. We'll represent it by an asterisk here.\n",
        "char_blank = \"*\"\n",
        "chars = sorted(set(char_blank + \"\".join(string.ascii_lowercase)))\n",
        "\n",
        "\n",
        "def label_to_indices(word):\n",
        "    # Convert word into indices\n",
        "    return torch.tensor([chars.index(w) for w in word], dtype=torch.int)\n",
        "\n",
        "def indices_to_label(indices):\n",
        "    # Return the word corresponding to indices\n",
        "    # This is a near inverse of label_to_indices\n",
        "    return \"\".join(chars[ind] for ind in indices).replace(char_blank, \"\")\n",
        "    \n",
        "\n",
        "word_start = \"yes\"\n",
        "indices = label_to_indices(word_start)\n",
        "word_recovered = indices_to_label(indices)\n",
        "\n",
        "print(word_start, \"-->\", indices, \"-->\", word_recovered)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yes --> tensor([25,  5, 19], dtype=torch.int32) --> yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul595AI4ZFAd"
      },
      "source": [
        "Let's take a look at a waveform in the time domain, and then move the waveform to the frequency domain by computing its mel spectrogram. It is the spectrogram that will be used in the models here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "NZGGCYN1D806",
        "outputId": "23819f32-86f1-4f99-cb7f-1fe1734fdd73"
      },
      "source": [
        "plt.plot(waveform.t().numpy());"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "TcWiU0x_ZFAd",
        "outputId": "6817a972-64ce-473a-d33e-0c97a13d4231"
      },
      "source": [
        "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=400, win_length=None, hop_length=None, f_min=0.0, f_max=None, pad=0, n_mels=64, normalized=False)\n",
        "\n",
        "transformed = transform(waveform)\n",
        "print(transformed.shape)\n",
        "\n",
        "plt.imshow(transformed.log2()[0,:,:].numpy(), cmap='gray');"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64, 81])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWaOdqYqZFAe"
      },
      "source": [
        "# Data Loader\n",
        "\n",
        "In the collate function, we also apply the resampling, and the text\n",
        "encoding. The data loader uses the collate function to form the batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2B_A44g-eYg"
      },
      "source": [
        "batch_size = 64"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpivMCgwZFAe"
      },
      "source": [
        "def pad_sequence(batch):\n",
        "    \n",
        "    # Move last to first\n",
        "    dims = list(range(len(batch[0].shape)))\n",
        "    dims.insert(0, dims.pop(-1))\n",
        "    batch = [item.permute(*dims) for item in batch]\n",
        "    \n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    \n",
        "    # Move second to last\n",
        "    dims = list(range(len(batch.shape)))\n",
        "    dims.append(dims.pop(1))\n",
        "    batch = batch.permute(*dims)\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    # preprend tensor with blank character for RNN-T\n",
        "    # prepend = char_blank\n",
        "    prepend = \"\"\n",
        "    # or pad in model\n",
        "\n",
        "    # A data tuple has the form:\n",
        "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
        "\n",
        "    tensors, targets_tensor, targets_original, tensors_length, targets_length = [], [], [], [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for waveform, _, label, *_ in batch:\n",
        "        \n",
        "        waveform = transform(waveform)\n",
        "        tensors += [waveform]\n",
        "        tensors_length += torch.tensor([waveform.shape[-1]], dtype=torch.int)\n",
        "        \n",
        "        targets_tensor += [label_to_indices(prepend + label)]\n",
        "        targets_original += [label]\n",
        "        targets_length += torch.tensor([len(label)], dtype=torch.int)\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    tensors = pad_sequence(tensors)\n",
        "    targets_tensor = pad_sequence(targets_tensor)\n",
        "    tensors_length = torch.stack(tensors_length)\n",
        "    targets_length = torch.stack(targets_length)\n",
        "\n",
        "    return tensors, targets_tensor, targets_original, tensors_length, targets_length\n",
        "\n",
        "\n",
        "if device == \"cuda\":\n",
        "    num_workers = 1\n",
        "    pin_memory = True\n",
        "else:\n",
        "    num_workers = 0\n",
        "    pin_memory = False\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVjCCA2dZFAf"
      },
      "source": [
        "The CTC Setting\n",
        "---------------\n",
        "\n",
        "Suppose we have a model that takes an input spectrogram and returns a sequence of log-probabilities for each character at each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZEwpfpuTQpt",
        "outputId": "b51daf26-e71e-472a-a546-e9375bf05125"
      },
      "source": [
        "class CTCModel(torch.nn.Module):\n",
        "    def __init__(self, n_audio_features=transformed.shape[1], n_hidden=512, n_chars=len(chars)):\n",
        "        super().__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size=n_audio_features, hidden_size=n_hidden//2, bidirectional=True)\n",
        "        self.linear_first = torch.nn.Linear((n_hidden//2) * 2, n_hidden//4)\n",
        "        self.linear_last = torch.nn.Linear(n_hidden//4, n_chars)\n",
        "        \n",
        "    def forward(self, x, y):\n",
        "        # Apply model to x. Pass y.\n",
        "        # Return both the output of the model and the target y.\n",
        "\n",
        "        x = x.squeeze().permute(0, 2, 1)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.linear_first(x)\n",
        "        x = self.linear_last(x)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "ctc_model = CTCModel()\n",
        "# print([str(t.device) for t in ctc_model.parameters()])\n",
        "ctc_model.to(device)\n",
        "# print([str(t.device) for t in ctc_model.parameters()])\n",
        "print(ctc_model)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"Number of parameters: {count_parameters(ctc_model):,}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CTCModel(\n",
            "  (rnn): RNN(64, 256, bidirectional=True)\n",
            "  (linear_first): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (linear_last): Linear(in_features=128, out_features=27, bias=True)\n",
            ")\n",
            "Number of parameters: 234,011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QElXwdgxS6eZ"
      },
      "source": [
        "In such a setup, we may run into an alignment issue between the input and the output sequences, where some sounds are longer than others, and so some characters might be repeated across many time steps. For instance, a possible alignment for the word `tree` is `ttr**eee*ee`. Indeed, starting with the alignment `ttr**eee*ee`, we first remove all the repeated characters to get `tr*e*e` and then remove the blank characters to get the word this corresponds to: `tree`. There are many such alignments for a given word, e.g. `ttr**eee*ee`, `trrrreee****e`, or `tre*e`. Note that `tree` is not an alignment of the word `tree` since the repeated `e` would collapse into a single `e`.\n",
        "\n",
        "The CTC loss (Connectionist Temporal Classification, Graves et al., 2006) overcomes the alignment issue by summing over all alignments between the input and the output. More precisely, the CTC conditional probability marginalizes over the set of alignments that are no longer than the input sequence where it computes the probability for a single alignment step-by-step.\n",
        "\n",
        "Here is an example showing how to use the CTC loss in PyTorch with a corresponding model. The loss function defined below takes as input a tensor of size\n",
        "```\n",
        "(batch size) x (input length) x (number of classes including blank)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ghftxM2E2rK"
      },
      "source": [
        "def ctc_criterion(outputs, targets, output_lengths, target_lengths):\n",
        "        \"\"\"\n",
        "        Apply the log softmax, and the CTC loss.\n",
        "\n",
        "        Input:\n",
        "        outputs: tensor of size (batch size, input length, number of classes including blank) containing output from network\n",
        "        targets: tensor of size (batch size, max target length) containing the targets with zero padded\n",
        "        output_lengths: tensor of size (batch size) containing the length of each output sequence\n",
        "        target_lengths: tensor of size (batch size) containing the length of each target\n",
        "        \"\"\"\n",
        "        \n",
        "        # Apply log_softmax as part of criterion as done for rnnt_loss\n",
        "        outputs = nn.functional.log_softmax(outputs, dim=-1)\n",
        "\n",
        "        # Transpose into format consumed by PyTorch\n",
        "        outputs = outputs.permute(1, 0, 2)\n",
        "\n",
        "        # CTC\n",
        "        # outputs: input length, batch size, number of classes (including blank)\n",
        "        # targets: batch size, max target length\n",
        "        # input_lengths: batch size\n",
        "        # target_lengths: batch size\n",
        "        return torch.nn.functional.ctc_loss(\n",
        "            outputs,\n",
        "            targets,\n",
        "            output_lengths,\n",
        "            target_lengths,\n",
        "            blank=0,\n",
        "            reduction='mean',\n",
        "            zero_infinity=False\n",
        "        )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLwFj99rTD2O"
      },
      "source": [
        "Let's compute the error with an untrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9uPd3q-Ascq",
        "outputId": "cabfeca8-22cb-400e-f888-279d693d71af"
      },
      "source": [
        "data, target, _, data_length, target_length = next(iter(train_loader))\n",
        "\n",
        "print(data.shape, target.shape, data_length.shape, target_length.shape)\n",
        "\n",
        "data = data.to(device)\n",
        "target = target.to(device)\n",
        "\n",
        "data, target = ctc_model(data, target)\n",
        "\n",
        "loss = ctc_criterion(\n",
        "    data,\n",
        "    target,\n",
        "    data_length,\n",
        "    target_length,\n",
        ")\n",
        "\n",
        "# This number will be large given that we have not trained the model :)\n",
        "print(loss.item())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 64, 81]) torch.Size([64, 8]) torch.Size([64]) torch.Size([64])\n",
            "27.63673210144043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmipjqekTOI7"
      },
      "source": [
        "And let's see what word the model assigns to the data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCHcBHGvHdu2",
        "outputId": "fcd1af3c-9115-4e73-a42b-3a4006e12072"
      },
      "source": [
        "# Replace by https://gist.github.com/awni/56369a90d03953e370f3964c826ed4b0\n",
        "\n",
        "def ctc_decode(tensor):\n",
        "    # greedy decoder for CTC\n",
        "\n",
        "    # find most likely label index for each element in the batch\n",
        "    tensor = tensor.argmax(dim=-1)\n",
        "    return torch.unique_consecutive(tensor, dim=-1)\n",
        "\n",
        "\n",
        "target = indices_to_label(target[0])\n",
        "\n",
        "indices = data[0, ...]\n",
        "decoded = ctc_decode(indices)\n",
        "word_recovered = indices_to_label(decoded)\n",
        "\n",
        "# The result will be far from the target since we have not trained the model :)\n",
        "print(target, \"-->\", decoded, \"-->\", word_recovered)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "backward --> tensor([ 0,  6,  0,  6, 19,  6,  0,  6,  0,  2,  0, 15,  6,  0,  6,  0, 17,  3,\n",
            "         8,  3,  5,  3, 11,  5, 23, 17,  6, 17,  6,  0,  6,  0]) --> ffsffboffqchceckewqfqff\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N463e54dSSvV"
      },
      "source": [
        "The RNN Transducer Setting\n",
        "--------------------------\n",
        "\n",
        "As mentioned before, the CTC loss defines a distribution over all alignments with all output sequences no longer than the input sequence. However, as well as precluding tasks (e.g. text-to-speech) where the output sequence is longer than the input sequence, CTC does not model the interdependencies between the outputs.\n",
        "\n",
        "The RNN Transducer loss (`Graves 2012 <https://arxiv.org/pdf/1211.3711.pdf>`__) extends the CTC loss by defining a distribution over output sequences of all lengths, and by jointly modelling both input-output and output-output dependencies.\n",
        "\n",
        "To do this, an RNN Transducer model combines a transcription model taking the input spectrogram (as done for the CTC case) and a prediction model taking the target sequence. The prediction network attempts to model each element of the target sequence given the previous ones, starting with a blank prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOPUPjYV1dZM",
        "outputId": "729b91f8-225e-47e8-b756-cde606e5e497"
      },
      "source": [
        "class RNNTBlock(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, bidirectional):\n",
        "        super().__init__()\n",
        "        self.rnn = torch.nn.GRU(input_size=input_size, hidden_size=hidden_size, bidirectional=bidirectional)\n",
        "        if bidirectional:\n",
        "          hidden_size *= 2\n",
        "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = self.linear(x)\n",
        "        return x, hidden\n",
        "\n",
        "class RNNTModel(torch.nn.Module):\n",
        "    # https://arxiv.org/pdf/1211.3711.pdf\n",
        "    # https://github.com/HawkAaron/RNN-Transducer/blob/graves2013/model2012.py\n",
        "    # https://github.com/lorenlugosch/transducer-tutorial/blob/main/transducer_tutorial_example.ipynb\n",
        "    # https://arxiv.org/pdf/1811.06621.pdf\n",
        "\n",
        "    def __init__(self, n_audio_features=transformed.shape[1], n_chars=len(chars)):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_size = 2048\n",
        "        self.transcription_model = RNNTBlock(n_audio_features, hidden_size, n_chars, bidirectional=True)\n",
        "        self.prediction_model = RNNTBlock(1, hidden_size, n_chars, bidirectional=False)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Apply model to x. Preprend blank character to y.\n",
        "        # Return both the output of the model and the target y.\n",
        "\n",
        "        x = x.squeeze().permute(0, 2, 1)\n",
        "        x, _ = self.transcription_model(x)\n",
        "        \n",
        "        # print(\"first\", y.shape)\n",
        "        # y_pad = y.clone()\n",
        "        # y = y_pad = torch.nn.functional.pad(y, (1,0,0,0), mode='constant', value=0)\n",
        "        # print(\"second\", y.shape)\n",
        "        \n",
        "        y = y.unsqueeze(-1).to(torch.float32)\n",
        "        # print(\"third\", y.shape)\n",
        "        y, _ = self.prediction_model(y)\n",
        "        # print(\"fourth\", y.shape)\n",
        "\n",
        "        x = x.unsqueeze(2)\n",
        "        y = y.unsqueeze(1)\n",
        "\n",
        "        # Fewer exponential by doing exp(x)*exp(y) instead of exp(x+y)\n",
        "        # x = torch.exp(x)\n",
        "        # y = torch.exp(y)\n",
        "        # out = x * y\n",
        "        # out = torch.nn.functional.log_softmax(x + y, dim=-1)\n",
        "        # NOTE log_softmax done as part of loss\n",
        "\n",
        "        return x + y # , y_pad\n",
        "\n",
        "        # blank as last token hardcoded? (or more efficient)\n",
        "        # emdedding layer to transcription?\n",
        "\n",
        "        # too simple, try:\n",
        "        # activation(x + y)\n",
        "        # linear\n",
        "        # (fused in loss) log_softmax\n",
        "\n",
        "        # 9 million is too small --> 37 million worked okay --> 80 million\n",
        "        # 100k samples at least would be better\n",
        "\n",
        "\n",
        "rnnt_model = RNNTModel().to(device)\n",
        "print(rnnt_model)\n",
        "print(f\"Number of parameters: {count_parameters(rnnt_model):,}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNNTModel(\n",
            "  (transcription_model): RNNTBlock(\n",
            "    (rnn): GRU(64, 2048, bidirectional=True)\n",
            "    (linear): Linear(in_features=4096, out_features=27, bias=True)\n",
            "  )\n",
            "  (prediction_model): RNNTBlock(\n",
            "    (rnn): GRU(1, 2048)\n",
            "    (linear): Linear(in_features=2048, out_features=27, bias=True)\n",
            "  )\n",
            ")\n",
            "Number of parameters: 38,744,118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctUcSbkZT0Qg"
      },
      "source": [
        "Here is an example showing how to use the RNN Transducer loss in PyTorch with a corresponding model. The loss function defined below takes as input a tensor of size\n",
        "```\n",
        "(batch size) x (input length) x (target length) x (number of classes including blank)\n",
        "```\n",
        "Note the difference in expected shape when compared to the previous case with CTC. The tensor given to the RNN-T loss also carries information about what has been outputed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWIgDQtobx__"
      },
      "source": [
        "def rnnt_criterion(outputs, targets, output_lengths, target_lengths):\n",
        "    \"\"\"\n",
        "    Apply the log softmax, and the RNN-T loss.\n",
        "\n",
        "    Input:\n",
        "    outputs: tensor of size (batch size, input length, output length, number of classes including blank) containing output from network\n",
        "    targets: tensor of size (batch size, max target length) containing the targets with zero padded\n",
        "    output_lengths: tensor of size (batch size) containing the length of each output sequence\n",
        "    target_lengths: tensor of size (batch size) containing the length of each target\n",
        "    \"\"\"\n",
        "\n",
        "    # acts: Tensor of (batch x seqLength x labelLength x outputDim) containing output from network\n",
        "    # labels: 2 dimensional Tensor containing all the targets of the batch with zero padded\n",
        "    # act_lens: Tensor of size (batch) containing size of each output sequence from the network\n",
        "    # label_lens: Tensor of (batch) containing label length of each example\n",
        "    return rnnt_loss(outputs, targets, output_lengths, target_lengths)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQcJcn0UUBim"
      },
      "source": [
        "Let's compute the error with an untrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoHFgfpYArSU",
        "outputId": "4124f5b9-c0cc-43b9-a8ff-f8688855f20e"
      },
      "source": [
        "data, target, _, data_length, target_length = next(iter(train_loader))\n",
        "print(data.shape, target.shape, data_length.shape, target_length.shape)\n",
        "\n",
        "data = data.to(device)\n",
        "target = target.to(device)\n",
        "\n",
        "# start with zero\n",
        "target = torch.nn.functional.pad(target, (1,0,0,0), mode='constant', value=0)\n",
        "print(\"target shape\", target.shape)\n",
        "data = rnnt_model(data, target)\n",
        "print(\"output\", data.shape, target.shape, data_length.shape, target_length.shape)\n",
        "\n",
        "loss = rnnt_criterion(\n",
        "    data,\n",
        "    target,\n",
        "    data_length,\n",
        "    target_length,\n",
        ")\n",
        "\n",
        "# This number will be large given that we have not trained the model :)\n",
        "print(loss.item())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 64, 81]) torch.Size([64, 8]) torch.Size([64]) torch.Size([64])\n",
            "target shape torch.Size([64, 9])\n",
            "output torch.Size([64, 81, 9, 27]) torch.Size([64, 9]) torch.Size([64]) torch.Size([64])\n",
            "276.99346923828125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPD9E54tCILq",
        "outputId": "cf7424c3-42e0-4ed1-ebd5-2ffe612f5390"
      },
      "source": [
        "data, target, _, data_length, target_length = next(iter(train_loader))\n",
        "print(data.shape, target.shape, data_length.shape, target_length.shape)\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "data = data.squeeze().permute(0, 2, 1)\n",
        "data, _ = rnnt_model.transcription_model(data)\n",
        "print(data.shape)\n",
        "\n",
        "print(\"target\", target.shape)\n",
        "target = target.to(device)\n",
        "target = torch.nn.functional.pad(target, (1,0,0,0), mode='constant', value=0)\n",
        "print(\"target\", target.shape)\n",
        "target = target.unsqueeze(-1).to(torch.float32)\n",
        "print(\"target\", target.shape)\n",
        "target, _ = rnnt_model.prediction_model(target)\n",
        "print(target.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 64, 81]) torch.Size([64, 8]) torch.Size([64]) torch.Size([64])\n",
            "torch.Size([64, 81, 27])\n",
            "target torch.Size([64, 8])\n",
            "target torch.Size([64, 9])\n",
            "target torch.Size([64, 9, 1])\n",
            "torch.Size([64, 9, 27])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_QGVzMiUFtW"
      },
      "source": [
        "And let's see what word the model assigns to the data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "2xpenS3oFWb5",
        "outputId": "2ce1d276-a4b1-4e01-eae6-84ba342bf6aa"
      },
      "source": [
        "def log_sum_exp(a, b):\n",
        "    \"\"\"\n",
        "    Stable log sum exp.\n",
        "    \"\"\"\n",
        "    return max(a, b) + math.log1p(math.exp(-abs(a-b)))\n",
        "\n",
        "# STATIC RNNT DECODER\n",
        "def rnnt_decode(log_probs, beam_size=1000, blank=0):\n",
        "    \"\"\"\n",
        "    Decode best prefix in the RNN Transducer. This decoder is static, it does\n",
        "    not update the next step distribution based on the previous prediction. As\n",
        "    such it looks for hypotheses which are length U.\n",
        "\n",
        "    https://github.com/awni/transducer/blob/master/decoders.py\n",
        "    \"\"\"\n",
        "    T, U, V = log_probs.shape\n",
        "    beam = [((), 0)];\n",
        "    for i in range(T + U - 2):\n",
        "        new_beam = {}\n",
        "        for hyp, score in beam:\n",
        "            u = len(hyp)\n",
        "            t = i - u\n",
        "            for v in range(V):\n",
        "                if v == blank:\n",
        "                    if t < T - 1:\n",
        "                        new_hyp = hyp\n",
        "                        new_score = score + log_probs[t, u, v]\n",
        "                elif u < U - 1:\n",
        "                    new_hyp = hyp + (v,)\n",
        "                    new_score = score + log_probs[t, u, v]\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                old_score = new_beam.get(new_hyp, None)\n",
        "                if old_score is not None:\n",
        "                    new_beam[new_hyp] = log_sum_exp(old_score, new_score)\n",
        "                else:\n",
        "                    new_beam[new_hyp] = new_score\n",
        "\n",
        "        new_beam = sorted(new_beam.items(), key=lambda x: x[1], reverse=True)\n",
        "        beam = new_beam[:beam_size]\n",
        "\n",
        "    hyp, score = beam[0]\n",
        "    # best_score = score + log_probs[-1, -1, blank]\n",
        "    return hyp\n",
        "\n",
        "\n",
        "word = indices_to_label(target[0])\n",
        "\n",
        "indices = data[0, ...]\n",
        "decoded = rnnt_decode(indices)\n",
        "word_recovered = indices_to_label(decoded)\n",
        "\n",
        "# The result will be far from the target since we have not trained the model :)\n",
        "print(word, \"-->\", decoded, \"-->\", word_recovered)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0e8c599513a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_to_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7558d981404f>\u001b[0m in \u001b[0;36mindices_to_label\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Return the word corresponding to indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# This is a near inverse of label_to_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_blank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7558d981404f>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Return the word corresponding to indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# This is a near inverse of label_to_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_blank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "TzbuvyxIVV2e",
        "outputId": "439dfbd5-a57f-4278-e82b-16b8fe7ab5c5"
      },
      "source": [
        "indices_to_label(target[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-83fc7ad7b559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindices_to_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-7558d981404f>\u001b[0m in \u001b[0;36mindices_to_label\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Return the word corresponding to indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# This is a near inverse of label_to_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_blank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7558d981404f>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Return the word corresponding to indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# This is a near inverse of label_to_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_blank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTEJwty6ZFAf"
      },
      "source": [
        "# Training\n",
        "\n",
        "We will use the same optimization technique used in the paper, an Adam\n",
        "optimizer with weight decay set to 0.0001. At first, we will train with\n",
        "a learning rate of 0.01, but we will use a ``scheduler`` to decrease it\n",
        "to 0.001 during training after 20 epochs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYUQ-t7GJsAi"
      },
      "source": [
        "# model, criterion = ctc_model, ctc_criterion\n",
        "model, criterion = rnnt_model, rnnt_criterion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wENZfNmLZFAg"
      },
      "source": [
        "# optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "\n",
        "\"\"\"\n",
        "optim.Adadelta(\n",
        "    model.parameters(),\n",
        "    lr=0.6,\n",
        "    weight_decay=1e-5,\n",
        "    eps=1e-8,\n",
        "    rho=0.95,\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# https://github.com/silversparro/wav2letter.pytorch/blob/master/train.py\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),\n",
        "    # lr=0.6,\n",
        "    # lr=0.3,\n",
        "    # lr=0.1,\n",
        "    lr=0.0001,\n",
        "    # lr=1e-5,\n",
        "    # weight_decay=1e-5,\n",
        "    momentum=0.90,\n",
        "    nesterov=True,\n",
        ")\n",
        "\n",
        "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n",
        "# scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=.06, max_lr=.6, step_size_up=100)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20*300, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10\n",
        "# scheduler = torch.optim.swa_utils.SWALR(optimizer, anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knfmycU8ZFAg"
      },
      "source": [
        "Now let’s define a training function that will feed our training data\n",
        "into the model and perform the backward pass and optimization steps. For\n",
        "training, the loss we will use is the negative log-likelihood. The\n",
        "network will then be tested after each epoch to see how the accuracy\n",
        "varies during the training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KadJFlH0ZFAg"
      },
      "source": [
        "negative_batches = []\n",
        "def train(model, epoch, log_interval):\n",
        "    model.train()\n",
        "    # for batch_idx, (data, target, _, data_length, target_length) in enumerate(train_loader):\n",
        "    while True:\n",
        "        batch_idx, (data, target, _, data_length, target_length) = next(iter(enumerate(train_loader)))\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        target = torch.nn.functional.pad(target, (1,0,0,0), mode='constant', value=0)\n",
        "        data = model(data, target)\n",
        "\n",
        "        loss = criterion(\n",
        "            data,\n",
        "            target,\n",
        "            data_length,\n",
        "            target_length,\n",
        "        )\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "        \n",
        "        # https://stackoverflow.com/questions/53405934/how-to-print-the-actual-learning-rate-in-adadelta-in-pytorch\n",
        "        # acc_deltas = [optimizer.state[i][\"acc_delta\"] for i in optimizer.state.keys() if \"acc_delta\" in optimizer.state[i].keys()]\n",
        "        # print(acc_deltas)\n",
        "        \n",
        "        # scheduler.step()\n",
        "\n",
        "        # print training stats\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "        # update progress bar\n",
        "        pbar.update(pbar_update)\n",
        "        # record loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if loss < 0:\n",
        "          print(batch_idx)\n",
        "          negative_batches.append(batch_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTl5mTrpZFAg"
      },
      "source": [
        "Now that we have a training function, we need to make one for testing\n",
        "the networks accuracy. We will set the model to ``eval()`` mode and then\n",
        "run inference on the test dataset. Calling ``eval()`` sets the training\n",
        "variable in all modules in the network to false. Certain layers like\n",
        "batch normalization and dropout layers behave differently during\n",
        "training so this step is crucial for getting correct results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0hzu2KYZFAh"
      },
      "source": [
        "Finally, we can train and test the network. We will train the network\n",
        "for ten epochs then reduce the learn rate and train for ten more epochs.\n",
        "The network will be tested after each epoch to see how the accuracy\n",
        "varies during the training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOdMrT9oZFAh"
      },
      "source": [
        "log_interval = 20\n",
        "n_epoch = 100\n",
        "\n",
        "pbar_update = 1 / len(train_loader)\n",
        "losses, lrs = [], []\n",
        "\n",
        "torch.autograd.set_detect_anomaly(False)                                                                                                                                \n",
        "\n",
        "# The transform needs to live on the same device as the model and the data.\n",
        "# transform = transform.to(device)\n",
        "with tqdm(total=n_epoch) as pbar:\n",
        "    for epoch in range(1, n_epoch + 1):\n",
        "        train(model, epoch, log_interval)\n",
        "        # scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PS-8aez7qb4"
      },
      "source": [
        "# Let's plot the training loss versus the number of iteration.\n",
        "plt.plot(losses)\n",
        "plt.yscale(\"log\")\n",
        "plt.title(\"training loss\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyOQE0vKsemw"
      },
      "source": [
        "# Let's plot the training loss versus the number of iteration.\n",
        "plt.plot(lrs)\n",
        "plt.title(\"learning rate\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wyTP-3K_GXS"
      },
      "source": [
        "Conclusion\n",
        "----------\n",
        "\n",
        "In this tutorial, we have used torchaudio to compare the CTC and the RNN Transducer losses with simple models to compare them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPrO5zEh2k4F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzUAfWIg2lia"
      },
      "source": [
        "# WIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0mPw80caaww"
      },
      "source": [
        "# ZT RNNT DECODER, EXPERIMENT\n",
        "\n",
        "\n",
        "def model_encoder(x):\n",
        "    x = x.squeeze().permute(0, 2, 1)\n",
        "    x, _ = rnnt_model.transcription_model(x)\n",
        "    x = x.unsqueeze(2)\n",
        "    # Fewer exponential by doing it on x and y instead of x + y\n",
        "    # x = torch.exp(x)\n",
        "    return x\n",
        "  \n",
        "\n",
        "def model_decoder(y, hidden=None):\n",
        "    # y = torch.nn.functional.pad(y, (1,0,0,0), mode='constant', value=0)\n",
        "        \n",
        "    y = y.unsqueeze(-1).to(torch.float32)\n",
        "    \n",
        "    if hidden is None:\n",
        "        y, hidden = rnnt_model.prediction_model(y)\n",
        "    else:\n",
        "        y, hidden = rnnt_model.prediction_model(y, hidden)\n",
        "\n",
        "    y = y.unsqueeze(1)\n",
        "\n",
        "    # Fewer exponential by doing it on x and y instead of x + y\n",
        "    # y = torch.exp(y)\n",
        "\n",
        "    return y, hidden\n",
        "\n",
        "\n",
        "def model_joint(x, y):\n",
        "    return x + y\n",
        "\n",
        "\n",
        "# def rnnt_decode_zt(model, inputs, input_lengths):\n",
        "def rnnt_decode_zt(inputs):\n",
        "    # https://github.com/ZhengkunTian/rnn-transducer/blob/master/rnnt/search.py\n",
        "\n",
        "    # assert inputs.dim() == 3\n",
        "    # f = [batch_size, time_step, feature_dim]\n",
        "    # f, _ = model.encoder(inputs, input_lengths)\n",
        "    # print(\"begin\", inputs.shape)\n",
        "    f = model_encoder(inputs)\n",
        "\n",
        "    zero_token = torch.LongTensor([[0]], device=inputs.device)\n",
        "    # print(\"zero_token shape\", zero_token.shape)\n",
        "\n",
        "    results = []\n",
        "    batch_size = inputs.size(0)\n",
        "\n",
        "    # def decode(inputs, inputs):\n",
        "    def decode(inputs):\n",
        "        lengths = len(inputs)\n",
        "\n",
        "        log_prob = 0\n",
        "        token_list = []\n",
        "        gu, hidden = model_decoder(zero_token)\n",
        "        # print(\"start gu shape\", gu.shape)\n",
        "        \n",
        "        for t in range(lengths):\n",
        "            # print(\"begin loop\", inputs[t].shape, gu[0].shape)\n",
        "            h = model_joint(inputs[t], gu)\n",
        "            # print(\"after model_joint\", h.shape)\n",
        "            # h = model_joint(inputs[t].view(-1), gu.view(-1))\n",
        "            # out = F.log_softmax(h, dim=0)\n",
        "            out = F.log_softmax(h, dim=-1)\n",
        "            # print(out.shape)\n",
        "            prob, pred = torch.max(out, dim=-1)\n",
        "            # print(prob.shape, pred.shape)\n",
        "            pred = int(pred.item())\n",
        "            log_prob += prob.item()\n",
        "            if pred != 0:\n",
        "                token_list.append(pred)\n",
        "                token = torch.LongTensor([[pred]])\n",
        "                if zero_token.is_cuda:\n",
        "                    token = token.cuda()\n",
        "                gu, hidden = model_decoder(token, hidden=hidden)\n",
        "\n",
        "        return token_list\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # decoded_seq = decode(f[i], input_lengths[i])\n",
        "        decoded_seq = decode(f[i])\n",
        "        results.append(decoded_seq)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "inputs, outputs, words, input_length, output_length = next(iter(train_loader))\n",
        "\n",
        "# word = indices_to_label(outputs[0])\n",
        "# indices = data[0, ...].unsqueeze(0)\n",
        "\n",
        "# print(\"inputs shape\", inputs.shape)\n",
        "decoded = rnnt_decode_zt(inputs)\n",
        "word_recovered = [indices_to_label(d) for d in decoded]\n",
        "\n",
        "# The result will be far from the target since we have not trained the model :)\n",
        "print(words[0], \"-->\", decoded[0], \"-->\", word_recovered[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDSgBB782ouG"
      },
      "source": [
        "\"\"\"\n",
        "# GREEDY RNN-T DECODER, WIP\n",
        "import math\n",
        "\n",
        "\n",
        "def rnnt_decode(data):\n",
        "    # Experimental greedy RNN-T decoder\n",
        "\n",
        "    i, j = 0, 0\n",
        "    output = []\n",
        "\n",
        "    top, ind = torch.max(data[:,i,j,:], dim=-1)\n",
        "    output.append(ind)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        if i+1 < data.shape[1]:\n",
        "            top_i, ind_i = torch.max(data[:,i+1,j,:], dim=-1)\n",
        "        else:\n",
        "            top_i = - math.inf\n",
        "        if j+1 < data.shape[2]:\n",
        "            top_j, ind_j = torch.max(data[:,i,j+1,:], dim=-1)\n",
        "        else:\n",
        "            top_j = - math.inf\n",
        "    \n",
        "        if top_i < -1 and top_j < -1:\n",
        "            break\n",
        "    \n",
        "        if top_i < top_j:\n",
        "            output.append(ind_j)\n",
        "            j += 1\n",
        "        else:\n",
        "            output.append(ind_i)\n",
        "            i += 1\n",
        "    return torch.cat(output)\n",
        "\n",
        "\n",
        "decoded = rnnt_decode(data)\n",
        "word_recovered = indices_to_label(decoded)\n",
        "\n",
        "# The result will be far from the target since we have not trained the model :)\n",
        "print(f'\"{target}\" -->\\n{decoded}\\n--> \"{word_recovered}\"')\n",
        "\"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}